#!/usr/bin/env python3
"""
VectorAI AI - Advanced Penetration Testing Framework Server

Enhanced with AI-Powered Intelligence & Automation
[>] Bug Bounty | CTF | Red Team | Security Research

RECENT ENHANCEMENTS (v6.0):
[OK] Complete color consistency with reddish hacker theme
[OK] Removed duplicate classes (PythonEnvironmentManager, CVEIntelligenceManager)
[OK] Enhanced visual output with ModernVisualEngine
[OK] Organized code structure with proper section headers
[OK] 100+ security tools with intelligent parameter optimization
[OK] AI-driven decision engine for tool selection
[OK] Advanced error handling and recovery systems

Architecture: Two-script system (VectorAI_server.py + VectorAI_mcp.py)
Framework: FastMCP integration for AI agent communication
"""

import argparse
import json
import logging
import os
import subprocess
import sys
import traceback
import threading
import time
import hashlib
import pickle
import base64
import queue
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Set, Tuple
from collections import OrderedDict
import shutil
import venv
import zipfile
from pathlib import Path
from flask import Flask, request, jsonify
import psutil
import signal
import requests
import re
import socket
import urllib.parse
from dataclasses import dataclass, field
from enum import Enum
from vectorai_app.workflows.http_testing import HTTPTestingFramework
from vectorai_app.workflows.browser import BrowserAgent
from vectorai_app.core.payloads import AIPayloadGenerator

# Import refactored modules
from vectorai_app.workflows.manager import CTFWorkflowManager
from vectorai_app.workflows.automator import CTFChallengeAutomator, CTFTeamCoordinator
from vectorai_app.workflows.bug_bounty import BugBountyWorkflowManager
from vectorai_app.workflows.file_upload import FileUploadTestingFramework
from vectorai_app.core.recon import TechnologyDetector
from vectorai_app.core.optimization import RateLimitDetector, FailureRecoverySystem, PerformanceMonitor, ParameterOptimizer
import asyncio
import aiohttp
from urllib.parse import urljoin, urlparse, parse_qs
from bs4 import BeautifulSoup
import selenium
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
import mitmproxy
from mitmproxy import http as mitmhttp
from mitmproxy.tools.dump import DumpMaster
from mitmproxy.options import Options as MitmOptions

from vectorai_app.config.settings import API_PORT, API_HOST, COMMAND_TIMEOUT, COMMAND_TIMEOUT_MAX
from vectorai_app.core.logging import setup_logging
from vectorai_app.core.models import (
    TargetType, TechnologyStack, ErrorType, RecoveryAction, JobStatus,
    TargetProfile, AttackStep, ErrorContext, RecoveryStrategy,
    BugBountyTarget, CTFChallenge, AsyncJob
)
from vectorai_app.core.engine import AttackChain, IntelligentDecisionEngine
from vectorai_app.core.files import FileOperationsManager
from vectorai_app.tools.manager import CTFToolManager

# ============================================================================
# LOGGING CONFIGURATION (MUST BE FIRST)
# ============================================================================

setup_logging()
logger = logging.getLogger(__name__)

# Flask app configuration
app = Flask(__name__)
app.config['JSON_SORT_KEYS'] = False

# API Configuration
# API_PORT and API_HOST are imported from vectorai_app.config.settings
from vectorai_app.core.visual import ModernVisualEngine
from vectorai_app.core.intelligence import CVEIntelligenceManager

# ============================================================================
# MODERN VISUAL ENGINE (v2.0 ENHANCEMENT)
# ============================================================================

# ModernVisualEngine moved to vectorai_app.core.visual

# ============================================================================
# INTELLIGENT DECISION ENGINE (v6.0 ENHANCEMENT)
# ============================================================================

# TargetType, TechnologyStack, TargetProfile, AttackStep moved to vectorai_app.core.models
# AttackChain, IntelligentDecisionEngine moved to vectorai_app.core.engine

# Global decision engine instance
decision_engine = IntelligentDecisionEngine()

# ============================================================================
# INTELLIGENT ERROR HANDLING AND RECOVERY SYSTEM (v11.0 ENHANCEMENT)
# ============================================================================

# IntelligentErrorHandler and GracefulDegradation moved to vectorai_app.core.error_handling
from vectorai_app.core.error_handling import IntelligentErrorHandler, GracefulDegradation
from vectorai_app.core.execution import (
    ProcessManager, 
    VectorAICache, 
    TelemetryCollector, 
    EnhancedCommandExecutor, 
    AsyncJobManager,
    ProcessPool,
    AdvancedCache,
    cache,
    telemetry,
    EnhancedProcessManager
)
from vectorai_app.core.session import SessionManager, session_manager
from vectorai_app.core.exploit import AIExploitGenerator
from vectorai_app.core.correlation import VulnerabilityCorrelator
from vectorai_app.core.intelligence import CVEIntelligenceManager

# Global error handler and degradation manager instances
error_handler = IntelligentErrorHandler()
degradation_manager = GracefulDegradation()

# ============================================================================
# BUG BOUNTY HUNTING SPECIALIZED WORKFLOWS (v6.0 ENHANCEMENT)
# ============================================================================

# BugBountyTarget moved to vectorai_app.core.models

# BugBountyWorkflowManager moved to vectorai_app.workflows.bug_bounty



# FileUploadTestingFramework moved to vectorai_app.workflows.file_upload

# Global bug bounty workflow manager
bugbounty_manager = BugBountyWorkflowManager()
fileupload_framework = FileUploadTestingFramework()

# ============================================================================
# CTF COMPETITION EXCELLENCE FRAMEWORK (v6.0 ENHANCEMENT)
# ============================================================================

# CTFChallenge moved to vectorai_app.core.models

# CTFWorkflowManager moved to vectorai_app.workflows.manager

# CTFToolManager moved to vectorai_app.tools.manager

# ============================================================================
# ADVANCED CTF AUTOMATION AND CHALLENGE SOLVING (v8.0 ENHANCEMENT)
# ============================================================================

# CTFChallengeAutomator moved to vectorai_app.workflows.automator
# CTFTeamCoordinator moved to vectorai_app.workflows.automator

# ============================================================================
# ADVANCED PARAMETER OPTIMIZATION AND INTELLIGENCE (v9.0 ENHANCEMENT)
# ============================================================================

# TechnologyDetector moved to vectorai_app.core.recon

# RateLimitDetector moved to vectorai_app.core.optimization

# FailureRecoverySystem moved to vectorai_app.core.optimization

# PerformanceMonitor moved to vectorai_app.core.optimization

# ParameterOptimizer moved to vectorai_app.core.optimization

# ============================================================================
# ADVANCED PROCESS MANAGEMENT AND MONITORING (v10.0 ENHANCEMENT)
# ============================================================================

# EnhancedProcessManager moved to vectorai_app.core.execution
# ResourceMonitor, PerformanceDashboard moved to vectorai_app.core.monitoring

# Global instances
tech_detector = TechnologyDetector()
rate_limiter = RateLimitDetector()
failure_recovery = FailureRecoverySystem()
performance_monitor = PerformanceMonitor()
parameter_optimizer = ParameterOptimizer()
enhanced_process_manager = EnhancedProcessManager()

# Global CTF framework instances
ctf_manager = CTFWorkflowManager()
ctf_tools = CTFToolManager()
ctf_automator = CTFChallengeAutomator()
ctf_coordinator = CTFTeamCoordinator()

# ============================================================================
# PROCESS MANAGEMENT FOR COMMAND TERMINATION (v5.0 ENHANCEMENT)
# ============================================================================

# ProcessManager moved to vectorai_app.core.execution

# Enhanced color codes and visual elements for modern terminal output
# All color references consolidated to ModernVisualEngine.COLORS for consistency


from vectorai_app.core.environment import PythonEnvironmentManager

# Global environment manager
env_manager = PythonEnvironmentManager()

# ============================================================================
# ADVANCED VULNERABILITY INTELLIGENCE SYSTEM (v6.0 ENHANCEMENT)
# ============================================================================

# CVEIntelligenceManager moved to vectorai_app.core.intelligence



















# Configure enhanced logging with colors
class ColoredFormatter(logging.Formatter):
    """Custom formatter with colors and emojis"""

    COLORS = {
        'DEBUG': ModernVisualEngine.COLORS['DEBUG'],
        'INFO': ModernVisualEngine.COLORS['SUCCESS'],
        'WARNING': ModernVisualEngine.COLORS['WARNING'],
        'ERROR': ModernVisualEngine.COLORS['ERROR'],
        'CRITICAL': ModernVisualEngine.COLORS['CRITICAL']
    }

    EMOJIS = {
        'DEBUG': '[?]',
        'INFO': '[OK]',
        'WARNING': '[WARN]',
        'ERROR': '[X]',
        'CRITICAL': '[!]'
    }

    def format(self, record):
        emoji = self.EMOJIS.get(record.levelname, '[N]')
        color = self.COLORS.get(record.levelname, ModernVisualEngine.COLORS['BRIGHT_WHITE'])

        # Add color and emoji to the message
        record.msg = f"{color}{emoji} {record.msg}{ModernVisualEngine.COLORS['RESET']}"
        return super().format(record)

# Enhanced logging setup
def setup_logging():
    """Setup enhanced logging with colors and formatting"""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    # Clear existing handlers
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    # Console handler with colors
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(ColoredFormatter(
        "[[!] VectorAI AI] %(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    ))
    logger.addHandler(console_handler)

    return logger

# Configuration (using existing API_PORT from top of file)
DEBUG_MODE = os.environ.get("DEBUG_MODE", "0").lower() in ("1", "true", "yes", "y")
COMMAND_TIMEOUT = 300  # 5 minutes default timeout
COMMAND_TIMEOUT_MAX = 1800  # 30 minutes max for long-running tools
CACHE_SIZE = 1000
CACHE_TTL = 3600  # 1 hour

# ============================================================================
# ASYNC JOB QUEUE SYSTEM (v6.1 ENHANCEMENT)
# ============================================================================

# JobStatus, AsyncJob moved to vectorai_app.core.models

# AsyncJobManager moved to vectorai_app.core.execution
job_manager = AsyncJobManager()

# ============================================================================
# SESSION MANAGEMENT SYSTEM (v6.3 - Persistent Storage with SQLite + Files)
# ============================================================================

import sqlite3
from pathlib import Path

# Session storage paths
# SessionManager moved to vectorai_app.core.session

    

        

# """
#         for asset_type, values in assets.items():
#             md += f"| {asset_type} | {len(values)} |\n"
#         
#         md += "\n---\n\n"
#         
#         # Detail findings by severity


# Global session manager
session_manager = SessionManager()

# VectorAICache moved to vectorai_app.core.execution
# cache = VectorAICache()

# TelemetryCollector moved to vectorai_app.core.execution
# telemetry = TelemetryCollector()

# EnhancedCommandExecutor moved to vectorai_app.core.execution

# ============================================================================
# DUPLICATE CLASSES REMOVED - Using the first definitions above
# ============================================================================

# ============================================================================
# AI-POWERED EXPLOIT GENERATION SYSTEM (v6.0 ENHANCEMENT)
# ============================================================================
#
# This section contains advanced AI-powered exploit generation capabilities
# for automated vulnerability exploitation and proof-of-concept development.
#
# Features:
# - Automated exploit template generation from CVE data
# - Multi-architecture support (x86, x64, ARM)
# - Evasion technique integration
# - Custom payload generation
# - Exploit effectiveness scoring
#
# ============================================================================


# AIExploitGenerator moved to vectorai_app.core.exploit





# Global intelligence managers
cve_intelligence = CVEIntelligenceManager()
exploit_generator = AIExploitGenerator()
vulnerability_correlator = VulnerabilityCorrelator()

from vectorai_app.core.execution import execute_command, execute_command_with_recovery

# File Operations Manager
# Global file operations manager
file_manager = FileOperationsManager()

# API Routes

@app.route("/health", methods=["GET"])
def health_check():
    # Health check endpoint with comprehensive tool detection

    essential_tools = [
        "nmap", "gobuster", "dirb", "nikto", "sqlmap", "hydra", "john", "hashcat"
    ]

    network_tools = [
        "rustscan", "masscan", "autorecon", "nbtscan", "arp-scan", "responder",
        "nxc", "enum4linux-ng", "rpcclient", "enum4linux"
    ]

    web_security_tools = [
        "ffuf", "feroxbuster", "dirsearch", "dotdotpwn", "xsser", "wfuzz",
        "gau", "waybackurls", "arjun", "dalfox",
        "httpx", "wafw00f", "burpsuite", "zaproxy", "katana", "hakrawler"
    ]

    vuln_scanning_tools = [
        "nuclei", "wpscan", "graphql-scanner", "jwt-analyzer"
    ]

    password_tools = [
        "medusa", "patator", "hash-identifier", "ophcrack", "hashcat-utils"
    ]

    binary_tools = [
        "gdb", "radare2", "binwalk", "ropgadget", "checksec", "objdump",
        "ghidra", "pwntools", "one-gadget", "ropper", "angr", "libc-database",
        "pwninit"
    ]

    forensics_tools = [
        "volatility3", "vol", "steghide", "hashpump", "foremost", "exiftool",
        "strings", "xxd", "file", "photorec", "testdisk", "scalpel", "bulk-extractor",
        "stegsolve", "zsteg", "outguess"
    ]

    cloud_tools = [
        "prowler", "scout-suite", "trivy", "kube-hunter", "kube-bench",
        "docker-bench-security", "checkov", "terrascan", "falco", "clair"
    ]

    osint_tools = [
        "amass", "subfinder", "fierce", "dnsenum", "theharvester", "sherlock",
        "social-analyzer", "recon-ng", "maltego", "spiderfoot", "shodan-cli",
        "censys-cli", "have-i-been-pwned"
    ]

    exploitation_tools = [
        "metasploit", "exploit-db", "searchsploit"
    ]

    api_tools = [
        "api-schema-analyzer", "postman", "insomnia", "curl", "httpie", "anew", "qsreplace", "uro"
    ]

    wireless_tools = [
        "kismet", "wireshark", "tshark", "tcpdump"
    ]

    additional_tools = [
        "smbmap", "volatility", "sleuthkit", "autopsy", "evil-winrm",
        "paramspider", "airmon-ng", "airodump-ng", "aireplay-ng", "aircrack-ng",
        "msfvenom", "msfconsole", "graphql-scanner", "jwt-analyzer"
    ]

    all_tools = (
        essential_tools + network_tools + web_security_tools + vuln_scanning_tools +
        password_tools + binary_tools + forensics_tools + cloud_tools +
        osint_tools + exploitation_tools + api_tools + wireless_tools + additional_tools
    )
    tools_status = {}

    for tool in all_tools:
        try:
            # Use shutil.which for cross-platform tool detection (Windows/Linux compatible)
            path = shutil.which(tool)
            tools_status[tool] = path is not None
        except:
            tools_status[tool] = False

    all_essential_tools_available = all(tools_status[tool] for tool in essential_tools)

    category_stats = {
        "essential": {"total": len(essential_tools), "available": sum(1 for tool in essential_tools if tools_status.get(tool, False))},
        "network": {"total": len(network_tools), "available": sum(1 for tool in network_tools if tools_status.get(tool, False))},
        "web_security": {"total": len(web_security_tools), "available": sum(1 for tool in web_security_tools if tools_status.get(tool, False))},
        "vuln_scanning": {"total": len(vuln_scanning_tools), "available": sum(1 for tool in vuln_scanning_tools if tools_status.get(tool, False))},
        "password": {"total": len(password_tools), "available": sum(1 for tool in password_tools if tools_status.get(tool, False))},
        "binary": {"total": len(binary_tools), "available": sum(1 for tool in binary_tools if tools_status.get(tool, False))},
        "forensics": {"total": len(forensics_tools), "available": sum(1 for tool in forensics_tools if tools_status.get(tool, False))},
        "cloud": {"total": len(cloud_tools), "available": sum(1 for tool in cloud_tools if tools_status.get(tool, False))},
        "osint": {"total": len(osint_tools), "available": sum(1 for tool in osint_tools if tools_status.get(tool, False))},
        "exploitation": {"total": len(exploitation_tools), "available": sum(1 for tool in exploitation_tools if tools_status.get(tool, False))},
        "api": {"total": len(api_tools), "available": sum(1 for tool in api_tools if tools_status.get(tool, False))},
        "wireless": {"total": len(wireless_tools), "available": sum(1 for tool in wireless_tools if tools_status.get(tool, False))},
        "additional": {"total": len(additional_tools), "available": sum(1 for tool in additional_tools if tools_status.get(tool, False))}
    }

    return jsonify({
        "status": "healthy",
        "message": "VectorAI AI Tools API Server is operational",
        "version": "6.1.0",
        "tools_status": tools_status,
        "all_essential_tools_available": all_essential_tools_available,
        "total_tools_available": sum(1 for tool, available in tools_status.items() if available),
        "total_tools_count": len(all_tools),
        "category_stats": category_stats,
        "cache_stats": cache.get_stats(),
        "telemetry": telemetry.get_stats(),
        "uptime": time.time() - telemetry.stats["start_time"],
        "features": {
            "async_jobs": True,
            "custom_timeout": True,
            "max_timeout_seconds": COMMAND_TIMEOUT_MAX,
            "default_timeout_seconds": COMMAND_TIMEOUT
        }
    })

@app.route("/api/command/quick", methods=["POST"])
def quick_command():
    # Execute a quick command with short timeout (30s max) for fast responses
    try:
        params = request.json
        command = params.get("command", "")
        timeout = min(int(params.get("timeout", 30)), 30)  # Max 30 seconds

        if not command:
            return jsonify({"error": "Command parameter is required"}), 400

        # Use subprocess directly for speed
        try:
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            return jsonify({
                "success": result.returncode == 0,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "return_code": result.returncode,
                "execution_time": 0  # Not tracked for quick commands
            })
        except subprocess.TimeoutExpired:
            return jsonify({"error": "Command timed out"}), 408
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500


        return jsonify({"error": str(e)}), 500

@app.route("/api/command", methods=["POST"])
def generic_command():
    # Execute any command provided in the request with enhanced logging

    try:
        params = request.json
        command = params.get("command", "")
        use_cache = params.get("use_cache", True)
        timeout = min(int(params.get("timeout", COMMAND_TIMEOUT)), COMMAND_TIMEOUT_MAX)
        async_mode = params.get("async", False)

        if not command:
            logger.warning("[WARN]  Command endpoint called without command parameter")
            return jsonify({
                "error": "Command parameter is required"
            }), 400

        # Async mode - submit job and return job_id immediately
        if async_mode:
            job_id = job_manager.create_job(command, timeout=timeout)
            return jsonify({
                "job_id": job_id,
                "status": "pending",
                "message": "Job submitted. Poll /api/jobs/<job_id> for status.",
                "poll_url": f"/api/jobs/{job_id}"
            }), 202

        # Sync mode - execute with custom timeout
        executor = EnhancedCommandExecutor(command, timeout=timeout)
        result = executor.execute()
        
        # Cache successful results
        if use_cache and result.get("success", False):
            cache.set(command, {}, result)
        
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in command endpoint: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

# ============================================================================
# ASYNC JOB API ENDPOINTS (v6.1 ENHANCEMENT)
# ============================================================================

@app.route("/api/jobs", methods=["GET"])
def list_jobs():
    # List recent async jobs
    try:
        limit = int(request.args.get("limit", 20))
        jobs = job_manager.list_jobs(limit=limit)
        return jsonify({
            "jobs": jobs,
            "count": len(jobs)
        })
    except Exception as e:
        logger.error(f"[!!] Error listing jobs: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/jobs/<job_id>", methods=["GET"])
def get_job_status(job_id: str):
    # Get status of an async job
    try:
        status = job_manager.get_job_status(job_id)
        if "error" in status and status["error"] == "Job not found":
            return jsonify(status), 404
        return jsonify(status)
    except Exception as e:
        logger.error(f"[!!] Error getting job status: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/jobs/<job_id>/cancel", methods=["POST"])
def cancel_job(job_id: str):
    # Cancel a pending or running job
    try:
        success = job_manager.cancel_job(job_id)
        if success:
            return jsonify({"success": True, "message": "Job cancelled"})
        return jsonify({"success": False, "message": "Job cannot be cancelled (not found or already completed)"}), 400
    except Exception as e:
        logger.error(f"[!!] Error cancelling job: {str(e)}")
        return jsonify({"error": str(e)}), 500

# ============================================================================
# BATCH EXECUTION API (v6.2 - Parallel Command Execution)
# ============================================================================

@app.route("/api/batch", methods=["POST"])
def batch_execute():
    # Execute multiple commands in parallel

    try:
        params = request.json
        commands = params.get("commands", [])
        max_parallel = min(int(params.get("max_parallel", 5)), 10)  # Cap at 10
        
        if not commands:
            return jsonify({"error": "commands array is required"}), 400
        
        if len(commands) > 50:
            return jsonify({"error": "Maximum 50 commands per batch"}), 400
        
        job_ids = []
        for cmd in commands:
            if isinstance(cmd, str):
                command = cmd
                timeout = COMMAND_TIMEOUT
            else:
                command = cmd.get("command", "")
                timeout = min(int(cmd.get("timeout", COMMAND_TIMEOUT)), COMMAND_TIMEOUT_MAX)
            
            if command:
                job_id = job_manager.create_job(command, timeout=timeout)
                job_ids.append({"command": command[:80], "job_id": job_id})
        
        return jsonify({
            "success": True,
            "batch_size": len(job_ids),
            "jobs": job_ids,
            "message": f"Submitted {len(job_ids)} jobs. Poll /api/jobs/<job_id> for each."
        }), 202
        
    except Exception as e:
        logger.error(f"[!!] Error in batch execute: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/batch/status", methods=["POST"])
def batch_status():
    # Get status of multiple jobs at once

    try:
        params = request.json
        job_ids = params.get("job_ids", [])
        
        results = []
        completed = 0
        running = 0
        failed = 0
        
        for job_id in job_ids:
            status = job_manager.get_job_status(job_id)
            results.append(status)
            if status.get("status") == "completed":
                completed += 1
            elif status.get("status") == "running":
                running += 1
            elif status.get("status") in ("failed", "timeout"):
                failed += 1
        
        return jsonify({
            "jobs": results,
            "summary": {
                "total": len(job_ids),
                "completed": completed,
                "running": running,
                "failed": failed,
                "pending": len(job_ids) - completed - running - failed
            }
        })
        
    except Exception as e:
        logger.error(f"[!!] Error in batch status: {str(e)}")
        return jsonify({"error": str(e)}), 500

# ============================================================================
# SMART TOOL PRESETS (v6.2 - Optimized Scan Configurations)
# ============================================================================

# ============================================================================
# SOTA SCAN PRESETS (v6.2 - Research-Backed Comprehensive Coverage)
# Based on: ProjectDiscovery docs, Nuclei 11k+ templates, Nmap techniques
# ============================================================================

SCAN_PRESETS = {
    # -------------------------------------------------------------------------
    # RECONNAISSANCE PRESETS
    # -------------------------------------------------------------------------
    "recon_quick": {
        "description": "Quick reconnaissance - fast subdomain enumeration and live host detection (2-3 min)",
        "coverage": "Subdomains, live hosts, basic tech stack",
        "tools": [
            {"name": "subfinder", "command": "subfinder -d {target} -all -silent", "timeout": 120},
            {"name": "httpx", "command": "echo '{target}' | httpx -silent -sc -title -td -ip -cdn -asn -server -hash sha256", "timeout": 90},
            {"name": "dig_records", "command": "dig {target} A AAAA MX NS TXT SOA +short 2>/dev/null", "timeout": 30},
        ]
    },
    "recon_full": {
        "description": "Full reconnaissance - comprehensive subdomain, port, and technology discovery (15-20 min)",
        "coverage": "Subdomains (multi-source), ports, services, technologies, CDN/WAF, OSINT, crawling",
        "tools": [
            # Multi-source subdomain enumeration (expert level: 5 sources)
            {"name": "subfinder", "command": "subfinder -d {target} -all -recursive -silent", "timeout": 300},
            {"name": "amass_passive", "command": "amass enum -passive -d {target} -timeout 5 2>/dev/null | head -200", "timeout": 360},
            {"name": "fierce", "command": "fierce --domain {target} 2>/dev/null | head -100", "timeout": 180},
            {"name": "dnsenum", "command": "dnsenum --noreverse {target} 2>/dev/null | head -100", "timeout": 180},
            # Live host detection with full probing
            {"name": "httpx_full", "command": "echo '{target}' | httpx -silent -sc -cl -title -td -ip -cname -cdn -asn -server -hash sha256 -jarm -rt -method -websocket -probe", "timeout": 120},
            # Port scanning - top 1000 with service/version detection
            {"name": "nmap_services", "command": "nmap -sV -sC -T4 --top-ports 1000 --open -oG - {target} 2>/dev/null | grep -v '^#'", "timeout": 600},
            {"name": "masscan", "command": "masscan {target} -p1-1000 --rate=1000 2>/dev/null | head -50", "timeout": 180},
            # Technology fingerprinting
            {"name": "whatweb", "command": "whatweb -a 3 --color=never https://{target} 2>/dev/null", "timeout": 120},
            {"name": "wafw00f", "command": "wafw00f https://{target} -a 2>/dev/null", "timeout": 90},
            # Active crawling and URL discovery
            {"name": "hakrawler", "command": "echo 'https://{target}' | hakrawler -d 2 -insecure 2>/dev/null | head -100", "timeout": 120},
            {"name": "katana", "command": "katana -u https://{target} -d 3 -jc -kf -silent -nc 2>/dev/null | head -150", "timeout": 180},
            # OSINT gathering
            {"name": "recon-ng", "command": "echo 'marketplace install recon/domains-hosts/hackertarget\nworkspaces create temp\ndb insert domains domain={target}\nmodules load recon/domains-hosts/hackertarget\nrun\nshow hosts' | recon-ng -r - 2>/dev/null | tail -50", "timeout": 180},
            {"name": "spiderfoot", "command": "spiderfoot -s {target} -t IP_ADDRESS,DOMAIN_NAME -q 2>/dev/null | head -100", "timeout": 300},
        ]
    },
    "recon_stealth": {
        "description": "Stealthy reconnaissance - slower but less detectable (15-20 min)",
        "coverage": "Passive-only subdomain enum, slow port scan, minimal fingerprinting",
        "tools": [
            {"name": "subfinder_passive", "command": "subfinder -d {target} -silent", "timeout": 180},
            {"name": "amass_passive", "command": "amass enum -passive -d {target} -timeout 10 2>/dev/null | head -100", "timeout": 600},
            # Slow scan with randomization to evade IDS
            {"name": "nmap_stealth", "command": "nmap -sS -T2 --top-ports 100 --randomize-hosts --open {target} 2>/dev/null", "timeout": 600},
            {"name": "curl_headers", "command": "curl -sI https://{target} 2>/dev/null | head -20", "timeout": 30},
        ]
    },
    
    # -------------------------------------------------------------------------
    # VULNERABILITY SCANNING PRESETS
    # -------------------------------------------------------------------------
    "vuln_quick": {
        "description": "Quick vulnerability scan - critical/high CVEs and known exploited vulnerabilities (3-5 min)",
        "coverage": "Critical+High CVEs, CISA KEV vulns, common misconfigs",
        "tools": [
            # Nuclei with KEV (Known Exploited Vulnerabilities) - most dangerous first
            {"name": "nuclei_kev", "command": "nuclei -u https://{target} -tags kev,vkev -silent -c 25 -rl 100", "timeout": 300},
            {"name": "nuclei_critical", "command": "nuclei -u https://{target} -severity critical,high -silent -c 25 -rl 100", "timeout": 300},
            {"name": "nikto_quick", "command": "nikto -h https://{target} -Tuning 1234 -maxtime 180s -no404 2>&1 | tail -100", "timeout": 240},
        ]
    },
    "vuln_full": {
        "description": "Full vulnerability assessment - comprehensive security testing (20-25 min)",
        "coverage": "All severity CVEs, web vulns, misconfigs, exposed panels, default creds, OWASP Top 10",
        "tools": [
            # Full nuclei scan with all important tags
            {"name": "nuclei_all_sev", "command": "nuclei -u https://{target} -severity critical,high,medium -silent -c 50 -rl 150", "timeout": 600},
            {"name": "nuclei_cve", "command": "nuclei -u https://{target} -tags cve -silent -c 25 -rl 100", "timeout": 600},
            {"name": "nuclei_exposure", "command": "nuclei -u https://{target} -tags exposure,misconfig,default-login -silent -c 25 -rl 100", "timeout": 300},
            {"name": "nuclei_owasp", "command": "nuclei -u https://{target} -tags owasp,owasp-top-10 -silent -c 25 -rl 100", "timeout": 300},
            # Web server analysis
            {"name": "nikto_full", "command": "nikto -h https://{target} -Tuning 123456789 -no404 2>&1 | tail -150", "timeout": 600},
            # OWASP ZAP baseline scan
            {"name": "zaproxy", "command": "zap-baseline.py -t https://{target} -J - 2>/dev/null | head -100", "timeout": 600},
            # Nmap vuln scripts
            {"name": "nmap_vuln", "command": "nmap -sV --script vuln,exploit --top-ports 100 {target} 2>/dev/null | tail -100", "timeout": 600},
            # Known exploit search
            {"name": "searchsploit", "command": "searchsploit --nmap nmap_output.xml 2>/dev/null || searchsploit {target} 2>/dev/null | head -50", "timeout": 120},
            # Container/image vulnerability scanning
            {"name": "trivy", "command": "trivy repo https://{target} --scanners vuln 2>/dev/null | head -80 || echo 'Trivy: Target not a repo'", "timeout": 300},
            # SQL injection detection
            {"name": "sqlmap_detect", "command": "sqlmap -u 'https://{target}' --batch --level=2 --risk=2 --crawl=2 --forms --random-agent 2>/dev/null | tail -60", "timeout": 480},
        ]
    },
    "vuln_kev": {
        "description": "CISA KEV scan - Known Exploited Vulnerabilities actively used in the wild",
        "coverage": "1496 unique KEV templates (CISA + VulnCheck catalogs)",
        "tools": [
            {"name": "nuclei_kev_cisa", "command": "nuclei -u https://{target} -tags kev -silent -c 50 -rl 150", "timeout": 600},
            {"name": "nuclei_kev_vulncheck", "command": "nuclei -u https://{target} -tags vkev -silent -c 50 -rl 150", "timeout": 600},
        ]
    },
    
    # -------------------------------------------------------------------------
    # WEB APPLICATION PRESETS
    # -------------------------------------------------------------------------
    "web_dirs": {
        "description": "Directory and file discovery - find hidden paths and sensitive files (8-12 min)",
        "coverage": "Common dirs, backup files, config files, admin panels, API endpoints, sensitive data",
        "tools": [
            # Gobuster - fast directory brute-forcing
            {"name": "gobuster_common", "command": "gobuster dir -u https://{target} -w /usr/share/wordlists/dirb/common.txt -q -t 30 -x php,asp,aspx,jsp,html,js,txt,bak,old,conf,config,xml,json 2>/dev/null | head -100", "timeout": 300},
            # Feroxbuster - recursive content discovery
            {"name": "feroxbuster", "command": "feroxbuster -u https://{target} -w /usr/share/wordlists/dirb/common.txt -q -t 30 -x php,asp,html,txt,bak,json --no-state 2>/dev/null | head -100", "timeout": 300},
            # FFuF - flexible fuzzer
            {"name": "ffuf", "command": "ffuf -u https://{target}/FUZZ -w /usr/share/wordlists/dirb/common.txt -mc 200,201,204,301,302,307,401,403,405 -s -t 40 2>/dev/null | head -80", "timeout": 300},
            # Dirb - classic directory scanner
            {"name": "dirb", "command": "dirb https://{target} /usr/share/wordlists/dirb/common.txt -S -r 2>/dev/null | grep -E '^\\+|CODE' | head -80", "timeout": 300},
            # Dirsearch - advanced web path scanner
            {"name": "dirsearch", "command": "dirsearch -u https://{target} -e php,asp,aspx,jsp,html,js,txt,bak -t 30 --format plain --quiet 2>/dev/null | head -80", "timeout": 300},
            # Nuclei for exposed files/panels
            {"name": "nuclei_exposure", "command": "nuclei -u https://{target} -tags exposure,backup,config,admin -silent -c 25", "timeout": 180},
            {"name": "nuclei_files", "command": "nuclei -u https://{target} -tags files,logs,git -silent -c 25", "timeout": 180},
        ]
    },
    "web_params": {
        "description": "Parameter and endpoint discovery - find API endpoints and parameters (8-12 min)",
        "coverage": "URLs from crawling, archives, JS parsing, API endpoints, hidden parameters, query strings",
        "tools": [
            # Active crawling
            {"name": "katana", "command": "katana -u https://{target} -d 3 -jc -kf -silent -nc 2>/dev/null | head -200", "timeout": 240},
            {"name": "hakrawler", "command": "echo 'https://{target}' | hakrawler -d 3 -insecure -subs 2>/dev/null | head -150", "timeout": 180},
            # Passive sources - archives and databases
            {"name": "gau", "command": "gau --subs {target} --blacklist ttf,woff,svg,png,jpg,gif,ico,css 2>/dev/null | head -200", "timeout": 180},
            {"name": "waybackurls", "command": "waybackurls {target} 2>/dev/null | head -200", "timeout": 120},
            # Hidden parameter discovery
            {"name": "arjun", "command": "arjun -u https://{target} -t 10 --stable 2>/dev/null | head -50", "timeout": 300},
            # URL deduplication and parameter extraction
            {"name": "grep_params", "command": "gau {target} 2>/dev/null | grep -E '\\?|=' | head -100", "timeout": 120},
            {"name": "qsreplace_test", "command": "gau {target} 2>/dev/null | grep '=' | qsreplace FUZZ 2>/dev/null | head -50", "timeout": 120},
            # Nuclei for interesting endpoints
            {"name": "nuclei_api", "command": "nuclei -u https://{target} -tags api,graphql,swagger -silent -c 25", "timeout": 180},
        ]
    },
    "web_xss": {
        "description": "XSS vulnerability scan - Cross-Site Scripting detection",
        "coverage": "Reflected XSS, DOM XSS, stored XSS patterns",
        "tools": [
            {"name": "nuclei_xss", "command": "nuclei -u https://{target} -tags xss -silent -c 25 -rl 100", "timeout": 300},
            {"name": "dalfox", "command": "echo 'https://{target}' | dalfox pipe --silence --no-color 2>/dev/null | head -50", "timeout": 300},
        ]
    },
    "web_sqli": {
        "description": "SQL Injection scan - database injection detection",
        "coverage": "Error-based, blind, time-based SQLi",
        "tools": [
            {"name": "nuclei_sqli", "command": "nuclei -u https://{target} -tags sqli -silent -c 25 -rl 100", "timeout": 300},
            {"name": "sqlmap_crawl", "command": "sqlmap -u 'https://{target}' --batch --level=2 --risk=2 --crawl=3 --forms --random-agent 2>/dev/null | tail -80", "timeout": 600},
        ]
    },
    
    # -------------------------------------------------------------------------
    # OSINT & INFORMATION GATHERING
    # -------------------------------------------------------------------------
    "osint": {
        "description": "OSINT and information gathering - emails, hosts, metadata (3-5 min)",
        "coverage": "Emails, subdomains, employee names, DNS records, WHOIS",
        "tools": [
            {"name": "theHarvester_multi", "command": "theHarvester -d {target} -b crtsh,dnsdumpster,hackertarget,rapiddns,urlscan -l 100 2>&1 | tail -100", "timeout": 300},
            {"name": "whois", "command": "whois {target} 2>/dev/null", "timeout": 45},
            {"name": "dig_all", "command": "dig {target} ANY +noall +answer 2>/dev/null; dig {target} A AAAA MX NS TXT SOA CNAME +short 2>/dev/null", "timeout": 30},
            {"name": "host_lookup", "command": "host -a {target} 2>/dev/null", "timeout": 30},
            {"name": "dnsrecon", "command": "dnsrecon -d {target} -t std 2>/dev/null | head -50", "timeout": 120},
        ]
    },
    "osint_deep": {
        "description": "Deep OSINT - extended information gathering like a senior expert (15-20 min)",
        "coverage": "All OSINT sources, DNS zone transfer, reverse lookups, social media, employee data, metadata",
        "tools": [
            # theHarvester - comprehensive email/host/name harvesting
            {"name": "theHarvester_all", "command": "theHarvester -d {target} -b all -l 200 2>&1 | tail -150", "timeout": 600},
            # Amass - advanced DNS enumeration and intel
            {"name": "amass_intel", "command": "amass intel -d {target} -timeout 5 2>/dev/null | head -100", "timeout": 360},
            {"name": "amass_enum", "command": "amass enum -passive -d {target} -timeout 5 2>/dev/null | head -150", "timeout": 360},
            # DNS reconnaissance
            {"name": "dnsrecon_full", "command": "dnsrecon -d {target} -t std,brt,axfr 2>/dev/null | head -100", "timeout": 300},
            {"name": "dnsenum", "command": "dnsenum --noreverse {target} 2>/dev/null | head -80", "timeout": 180},
            {"name": "fierce", "command": "fierce --domain {target} 2>/dev/null | head -80", "timeout": 180},
            # Recon-ng framework
            {"name": "recon-ng", "command": "echo 'workspaces create temp\ndb insert domains domain={target}\nmodules load recon/domains-hosts/hackertarget\nrun\nshow hosts' | recon-ng -r - 2>/dev/null | tail -50", "timeout": 180},
            # SpiderFoot - automated OSINT
            {"name": "spiderfoot", "command": "spiderfoot -s {target} -t IP_ADDRESS,DOMAIN_NAME,EMAILADDR,HUMAN_NAME -q 2>/dev/null | head -100", "timeout": 360},
            # Sherlock - social media username search (if target is username)
            {"name": "sherlock", "command": "sherlock {target} --print-found 2>/dev/null | head -50 || echo 'Sherlock: Use with usernames'", "timeout": 180},
            # WHOIS and DNS basics
            {"name": "whois", "command": "whois {target} 2>/dev/null", "timeout": 45},
            {"name": "nmap_dns", "command": "nmap --script dns-brute,dns-zone-transfer -p 53 {target} 2>/dev/null | head -80", "timeout": 300},
            # Certificate transparency
            {"name": "curl_crtsh", "command": "curl -s 'https://crt.sh/?q=%25.{target}&output=json' 2>/dev/null | jq -r '.[].name_value' 2>/dev/null | sort -u | head -100", "timeout": 120},
        ]
    },
    
    # -------------------------------------------------------------------------
    # SECURITY INFRASTRUCTURE
    # -------------------------------------------------------------------------
    "waf_detect": {
        "description": "WAF and security detection - identify protection layers",
        "coverage": "WAF vendors, CDN detection, security headers",
        "tools": [
            {"name": "wafw00f_all", "command": "wafw00f https://{target} -a 2>/dev/null", "timeout": 90},
            {"name": "whatweb", "command": "whatweb -a 3 --color=never https://{target} 2>/dev/null", "timeout": 90},
            {"name": "httpx_security", "command": "echo '{target}' | httpx -silent -cdn -waf -td -server", "timeout": 60},
            # Check security headers
            {"name": "curl_headers", "command": "curl -sI https://{target} 2>/dev/null | grep -iE '(x-frame|x-xss|x-content|strict-transport|content-security|referrer-policy|permissions-policy)'", "timeout": 30},
        ]
    },
    "ssl_check": {
        "description": "SSL/TLS security analysis - certificate and cipher analysis",
        "coverage": "Certificate validity, weak ciphers, TLS versions, HSTS, known SSL vulns",
        "tools": [
            # Nmap SSL scripts - comprehensive
            {"name": "nmap_ssl_full", "command": "nmap --script ssl-enum-ciphers,ssl-cert,ssl-date,ssl-known-key,ssl-dh-params,ssl-heartbleed,ssl-poodle,ssl-ccs-injection -p 443 {target} 2>/dev/null", "timeout": 180},
            # Certificate details
            {"name": "openssl_cert", "command": "echo | openssl s_client -connect {target}:443 -servername {target} 2>/dev/null | openssl x509 -noout -dates -subject -issuer -ext subjectAltName 2>/dev/null", "timeout": 30},
            # TLS version check
            {"name": "curl_tls", "command": "curl -vI --tlsv1.2 https://{target} 2>&1 | grep -E '(SSL|TLS|subject|issuer|expire|protocol)'", "timeout": 30},
            # Nuclei SSL templates
            {"name": "nuclei_ssl", "command": "nuclei -u https://{target} -tags ssl -silent", "timeout": 120},
        ]
    },
    
    # -------------------------------------------------------------------------
    # SPECIALIZED SCANS
    # -------------------------------------------------------------------------
    "api_security": {
        "description": "API security scan - REST/GraphQL endpoint testing",
        "coverage": "API misconfigs, exposed docs, auth bypass, BOLA/IDOR patterns",
        "tools": [
            {"name": "nuclei_api", "command": "nuclei -u https://{target} -tags api,graphql,swagger,openapi -silent -c 25", "timeout": 300},
            # Check common API paths
            {"name": "ffuf_api", "command": "ffuf -u https://{target}/FUZZ -w /usr/share/wordlists/dirb/common.txt -mc 200,201,204,301,302,401,403 -s 2>/dev/null | head -50", "timeout": 180},
            # API documentation exposure
            {"name": "curl_swagger", "command": "for p in swagger.json openapi.json api-docs v1/swagger.json v2/swagger.json docs/api; do curl -s -o /dev/null -w '%{http_code} https://{target}/'$p'\n' https://{target}/$p 2>/dev/null; done", "timeout": 60},
        ]
    },
    "cloud_security": {
        "description": "Cloud security scan - AWS/Azure/GCP misconfigurations",
        "coverage": "S3 buckets, Azure blobs, exposed cloud metadata, IAM issues",
        "tools": [
            {"name": "nuclei_cloud", "command": "nuclei -u https://{target} -tags cloud,aws,azure,gcp,s3,bucket -silent -c 25", "timeout": 300},
            {"name": "nuclei_takeover", "command": "nuclei -u https://{target} -tags takeover -silent", "timeout": 180},
            # Check cloud metadata
            {"name": "curl_metadata", "command": "curl -s --connect-timeout 2 http://169.254.169.254/latest/meta-data/ 2>/dev/null | head -20", "timeout": 10},
        ]
    },
    "wordpress": {
        "description": "WordPress security scan - WP-specific vulnerabilities",
        "coverage": "WP core, plugins, themes, user enumeration, xmlrpc",
        "tools": [
            {"name": "nuclei_wp", "command": "nuclei -u https://{target} -tags wordpress,wp-plugin,wp-theme -silent -c 50 -rl 150", "timeout": 600},
            {"name": "wpscan", "command": "wpscan --url https://{target} --enumerate vp,vt,u --random-user-agent --no-update 2>/dev/null | tail -100", "timeout": 600},
        ]
    },
    "network_scan": {
        "description": "Network infrastructure scan - comprehensive port/service/vuln discovery (15-20 min)",
        "coverage": "All 65535 ports, service versions, network vulns, SNMP, NetBIOS, ARP discovery",
        "tools": [
            # Fast top ports with service detection
            {"name": "nmap_fast", "command": "nmap -sV -sC -T4 --top-ports 1000 --open {target} 2>/dev/null | tail -100", "timeout": 600},
            # Masscan - ultra-fast port scanning
            {"name": "masscan", "command": "masscan {target} -p1-65535 --rate=1000 2>/dev/null | head -100", "timeout": 600},
            # Full port scan (slower but complete)
            {"name": "nmap_full", "command": "nmap -p- -T4 --open {target} 2>/dev/null | grep -E '^[0-9]+'", "timeout": 900},
            # UDP top ports
            {"name": "nmap_udp", "command": "nmap -sU -T4 --top-ports 100 --open {target} 2>/dev/null | grep -E '^[0-9]+'", "timeout": 400},
            # ARP scan for local network discovery
            {"name": "arp-scan", "command": "arp-scan -l 2>/dev/null | head -50 || echo 'arp-scan: Requires local network'", "timeout": 60},
            # NBTScan - NetBIOS scanning
            {"name": "nbtscan", "command": "nbtscan {target}/24 2>/dev/null | head -50 || nbtscan {target} 2>/dev/null", "timeout": 120},
            # Vuln scripts
            {"name": "nmap_vuln", "command": "nmap -sV --script vuln,exploit --top-ports 100 {target} 2>/dev/null | tail -100", "timeout": 600},
            # Network service enumeration
            {"name": "nmap_services", "command": "nmap --script banner,version -sV --top-ports 200 {target} 2>/dev/null | tail -80", "timeout": 300},
            # SNMP enumeration
            {"name": "nmap_snmp", "command": "nmap -sU -p 161,162 --script snmp-info,snmp-interfaces,snmp-processes {target} 2>/dev/null | tail -50", "timeout": 180},
            # Packet capture (brief)
            {"name": "tcpdump", "command": "timeout 10 tcpdump -i any -c 100 host {target} -nn 2>/dev/null | tail -50 || echo 'tcpdump: Requires privileges'", "timeout": 15},
        ]
    },
    "cve_2024": {
        "description": "2024 CVE scan - latest vulnerabilities from 2024",
        "coverage": "Recent CVEs with working exploits",
        "tools": [
            {"name": "nuclei_2024", "command": "nuclei -u https://{target} -tags cve2024 -silent -c 25 -rl 100", "timeout": 300},
            {"name": "nuclei_2023", "command": "nuclei -u https://{target} -tags cve2023 -silent -c 25 -rl 100", "timeout": 300},
        ]
    },
    
    # -------------------------------------------------------------------------
    # EXPERT-LEVEL SPECIALIZED PRESETS (Senior Bug Bounty Hunter)
    # -------------------------------------------------------------------------
    "content_discovery": {
        "description": "Deep content discovery - find every hidden file, directory, and endpoint (10-15 min)",
        "coverage": "Hidden files, backups, configs, source code, git repos, API docs, admin panels",
        "tools": [
            # Multi-fuzzer approach for maximum coverage
            {"name": "gobuster", "command": "gobuster dir -u https://{target} -w /usr/share/wordlists/dirb/common.txt -q -t 40 -x php,asp,aspx,jsp,html,js,txt,bak,old,conf,xml,json,sql,log,zip,tar,gz 2>/dev/null | head -100", "timeout": 360},
            {"name": "feroxbuster", "command": "feroxbuster -u https://{target} -w /usr/share/wordlists/dirb/common.txt -q -t 40 -x php,asp,html,txt,bak,json,xml -d 3 --no-state 2>/dev/null | head -100", "timeout": 360},
            {"name": "ffuf", "command": "ffuf -u https://{target}/FUZZ -w /usr/share/wordlists/dirb/common.txt -mc 200,201,204,301,302,307,401,403,405,500 -s -t 50 2>/dev/null | head -100", "timeout": 300},
            {"name": "dirb", "command": "dirb https://{target} /usr/share/wordlists/dirb/common.txt -S -r 2>/dev/null | grep -E '^\\+|CODE' | head -80", "timeout": 300},
            {"name": "dirsearch", "command": "dirsearch -u https://{target} -e php,asp,aspx,jsp,html,js,txt,bak,sql,xml,json -t 40 --format plain --quiet 2>/dev/null | head -80", "timeout": 300},
            # Git/SVN exposure
            {"name": "nuclei_git", "command": "nuclei -u https://{target} -tags git,svn,hg,exposure -silent -c 25", "timeout": 180},
            # Backup/config files
            {"name": "nuclei_backup", "command": "nuclei -u https://{target} -tags backup,config,database -silent -c 25", "timeout": 180},
            # Sensitive data exposure
            {"name": "nuclei_sensitive", "command": "nuclei -u https://{target} -tags token,secret,password,credential -silent -c 25", "timeout": 180},
            # Archive crawling for historical endpoints
            {"name": "waybackurls", "command": "waybackurls {target} 2>/dev/null | grep -E '\\.(php|asp|aspx|jsp|js|json|xml|txt|sql|bak|old|conf|config|env|log)' | head -100", "timeout": 120},
            {"name": "gau_files", "command": "gau {target} 2>/dev/null | grep -E '\\.(php|asp|aspx|jsp|js|json|xml|txt|bak|sql|conf|env|log|zip|tar|gz)' | head -100", "timeout": 120},
        ]
    },
    "auth_testing": {
        "description": "Authentication testing - brute force, default creds, auth bypass (10-15 min)",
        "coverage": "Default credentials, weak passwords, login bypass, auth misconfiguration",
        "tools": [
            # Nuclei auth-related templates
            {"name": "nuclei_default_login", "command": "nuclei -u https://{target} -tags default-login -silent -c 50 -rl 150", "timeout": 300},
            {"name": "nuclei_auth", "command": "nuclei -u https://{target} -tags auth-bypass,authentication -silent -c 25", "timeout": 180},
            {"name": "nuclei_creds", "command": "nuclei -u https://{target} -tags weak-credentials,default-credentials -silent -c 25", "timeout": 180},
            # Hydra - network login cracker (common services)
            {"name": "hydra_ssh", "command": "hydra -L /usr/share/wordlists/metasploit/unix_users.txt -P /usr/share/wordlists/metasploit/unix_passwords.txt -t 4 -vV {target} ssh 2>/dev/null | tail -30 || echo 'SSH not open'", "timeout": 300},
            {"name": "hydra_ftp", "command": "hydra -L /usr/share/wordlists/metasploit/unix_users.txt -P /usr/share/wordlists/metasploit/unix_passwords.txt -t 4 -vV {target} ftp 2>/dev/null | tail -30 || echo 'FTP not open'", "timeout": 300},
            # Medusa - parallel password testing
            {"name": "medusa_ssh", "command": "medusa -h {target} -U /usr/share/wordlists/metasploit/unix_users.txt -P /usr/share/wordlists/metasploit/unix_passwords.txt -M ssh -t 4 2>/dev/null | tail -30 || echo 'SSH test skipped'", "timeout": 300},
            # Patator - multi-purpose brute-forcer
            {"name": "patator_http", "command": "patator http_fuzz url='https://{target}/login' method=POST body='user=FILE0&pass=FILE1' 0=/usr/share/wordlists/metasploit/unix_users.txt 1=/usr/share/wordlists/metasploit/unix_passwords.txt -x ignore:code=401 2>/dev/null | head -30 || echo 'HTTP auth test'", "timeout": 300},
            # Nmap auth scripts
            {"name": "nmap_auth", "command": "nmap --script auth,brute --top-ports 20 {target} 2>/dev/null | tail -60", "timeout": 300},
            # Default credential check
            {"name": "nmap_default_creds", "command": "nmap --script http-default-accounts,ftp-anon,ssh-auth-methods -p 21,22,80,443,8080 {target} 2>/dev/null | tail -50", "timeout": 180},
        ]
    },
    "smb_enum": {
        "description": "SMB/Windows enumeration - shares, users, policies, AD info (8-12 min)",
        "coverage": "SMB shares, users, groups, policies, null sessions, AD enumeration",
        "tools": [
            # SMBMap - enumerate shares and permissions
            {"name": "smbmap", "command": "smbmap -H {target} 2>/dev/null || smbmap -H {target} -u '' -p '' 2>/dev/null", "timeout": 120},
            {"name": "smbmap_shares", "command": "smbmap -H {target} -r 2>/dev/null | head -50", "timeout": 120},
            # Enum4linux-ng - comprehensive Windows/Samba enumeration
            {"name": "enum4linux-ng", "command": "enum4linux-ng -A {target} 2>/dev/null | head -150", "timeout": 300},
            # NetExec (nxc) - modern replacement for CrackMapExec
            {"name": "nxc_smb", "command": "nxc smb {target} 2>/dev/null | head -30", "timeout": 120},
            {"name": "nxc_smb_shares", "command": "nxc smb {target} --shares 2>/dev/null | head -30", "timeout": 120},
            {"name": "nxc_smb_users", "command": "nxc smb {target} --users 2>/dev/null | head -50", "timeout": 120},
            # RPC client - Windows RPC enumeration
            {"name": "rpcclient", "command": "rpcclient -U '' -N {target} -c 'enumdomusers; enumdomgroups; querydominfo' 2>/dev/null | head -50", "timeout": 120},
            # NBTScan - NetBIOS name scanning
            {"name": "nbtscan", "command": "nbtscan {target} 2>/dev/null", "timeout": 60},
            # Nmap SMB scripts
            {"name": "nmap_smb", "command": "nmap --script smb-enum-shares,smb-enum-users,smb-os-discovery,smb-protocols,smb-security-mode,smb-vuln* -p 139,445 {target} 2>/dev/null | tail -100", "timeout": 300},
            # Responder (passive - just show what it would capture)
            {"name": "responder_analyze", "command": "responder --analyze -I eth0 2>/dev/null | head -20 || echo 'Responder: Run manually for active capture'", "timeout": 30},
        ]
    },
    "container_security": {
        "description": "Container and cloud security - Docker, K8s, cloud misconfigs (10-15 min)",
        "coverage": "Container vulnerabilities, K8s misconfigs, cloud IAM, IaC security",
        "tools": [
            # Trivy - comprehensive vulnerability scanner
            {"name": "trivy_image", "command": "trivy image {target} --scanners vuln 2>/dev/null | head -100 || echo 'Trivy: Provide image name'", "timeout": 300},
            {"name": "trivy_fs", "command": "trivy fs . --scanners vuln,secret,misconfig 2>/dev/null | head -80 || echo 'Trivy: Run in project directory'", "timeout": 300},
            # Kube-bench - Kubernetes CIS benchmark
            {"name": "kube-bench", "command": "kube-bench run --json 2>/dev/null | jq '.Controls[].tests[].results[] | select(.status != \"PASS\")' 2>/dev/null | head -100 || echo 'kube-bench: Requires K8s cluster access'", "timeout": 300},
            # Prowler - AWS/Azure/GCP security assessments
            {"name": "prowler_aws", "command": "prowler aws --list-checks-json 2>/dev/null | head -50 || echo 'Prowler: Requires cloud credentials'", "timeout": 180},
            # Checkov - IaC security scanner
            {"name": "checkov_docker", "command": "checkov -f Dockerfile 2>/dev/null | head -80 || echo 'Checkov: Provide Dockerfile path'", "timeout": 180},
            {"name": "checkov_terraform", "command": "checkov -d . --framework terraform 2>/dev/null | head -80 || echo 'Checkov: Run in IaC directory'", "timeout": 180},
            # Nuclei cloud/container templates
            {"name": "nuclei_cloud", "command": "nuclei -u https://{target} -tags cloud,aws,azure,gcp,kubernetes,docker -silent -c 25", "timeout": 300},
            {"name": "nuclei_takeover", "command": "nuclei -u https://{target} -tags takeover,subdomain-takeover -silent -c 25", "timeout": 180},
            # Cloud metadata checks
            {"name": "curl_aws_meta", "command": "curl -s --connect-timeout 2 http://169.254.169.254/latest/meta-data/ 2>/dev/null | head -20 || echo 'AWS metadata: Not accessible'", "timeout": 10},
            {"name": "curl_azure_meta", "command": "curl -s --connect-timeout 2 -H 'Metadata:true' 'http://169.254.169.254/metadata/instance?api-version=2021-02-01' 2>/dev/null | head -20 || echo 'Azure metadata: Not accessible'", "timeout": 10},
        ]
    },
    "exploit_research": {
        "description": "Exploit research and development - find known exploits (5-10 min)",
        "coverage": "ExploitDB search, Metasploit modules, payload generation",
        "tools": [
            # SearchSploit - Exploit-DB local search
            {"name": "searchsploit_target", "command": "searchsploit {target} 2>/dev/null | head -50", "timeout": 60},
            {"name": "searchsploit_web", "command": "searchsploit apache nginx iis tomcat 2>/dev/null | head -50", "timeout": 60},
            {"name": "searchsploit_cms", "command": "searchsploit wordpress joomla drupal 2>/dev/null | head -40", "timeout": 60},
            # Metasploit search for modules
            {"name": "msfconsole_search", "command": "msfconsole -q -x 'search {target}; exit' 2>/dev/null | tail -50 || echo 'MSF: Run search manually'", "timeout": 120},
            {"name": "msfconsole_vulns", "command": "msfconsole -q -x 'search type:exploit platform:linux; exit' 2>/dev/null | head -30 || echo 'MSF: Exploit search'", "timeout": 120},
            # Nuclei for known exploits
            {"name": "nuclei_rce", "command": "nuclei -u https://{target} -tags rce -silent -c 25", "timeout": 180},
            {"name": "nuclei_lfi", "command": "nuclei -u https://{target} -tags lfi,rfi -silent -c 25", "timeout": 180},
            {"name": "nuclei_ssrf", "command": "nuclei -u https://{target} -tags ssrf -silent -c 25", "timeout": 180},
            # Nmap exploit scripts
            {"name": "nmap_exploit", "command": "nmap --script exploit --top-ports 50 {target} 2>/dev/null | tail -60", "timeout": 300},
        ]
    },
    "forensics_analysis": {
        "description": "Digital forensics and file analysis (5-10 min)",
        "coverage": "File metadata, embedded data, hidden files, memory artifacts, steganography",
        "tools": [
            # Binwalk - firmware/file analysis
            {"name": "binwalk", "command": "binwalk {target} 2>/dev/null | head -50 || echo 'binwalk: Provide file path'", "timeout": 120},
            {"name": "binwalk_extract", "command": "binwalk -e {target} 2>/dev/null | head -30 || echo 'binwalk: Extract embedded files'", "timeout": 180},
            # Exiftool - metadata extraction
            {"name": "exiftool", "command": "exiftool {target} 2>/dev/null | head -80 || echo 'exiftool: Provide file path'", "timeout": 60},
            # Foremost - file carving
            {"name": "foremost", "command": "foremost -t all -i {target} -o /tmp/foremost_out 2>/dev/null && ls /tmp/foremost_out 2>/dev/null || echo 'foremost: Provide image file'", "timeout": 180},
            # Strings - extract readable strings
            {"name": "strings", "command": "strings {target} 2>/dev/null | head -100 || echo 'strings: Provide binary file'", "timeout": 60},
            {"name": "strings_long", "command": "strings -n 10 {target} 2>/dev/null | grep -E '(password|secret|key|token|api|admin|root|user)' | head -50 || echo 'strings: Search sensitive data'", "timeout": 60},
            # File type identification
            {"name": "file", "command": "file {target} 2>/dev/null || echo 'file: Provide file path'", "timeout": 30},
            # Steganography detection
            {"name": "steghide", "command": "steghide info {target} 2>/dev/null || echo 'steghide: Provide image file'", "timeout": 60},
            # Memory forensics with Volatility
            {"name": "vol", "command": "vol -f {target} imageinfo 2>/dev/null | head -30 || echo 'Volatility: Provide memory dump'", "timeout": 120},
            {"name": "vol_pslist", "command": "vol -f {target} --profile=Win10x64 pslist 2>/dev/null | head -50 || echo 'Volatility: Process list'", "timeout": 120},
            # PhotoRec - file recovery
            {"name": "photorec", "command": "echo 'photorec: Interactive tool - run manually with: photorec {target}'", "timeout": 5},
            # TestDisk - partition recovery
            {"name": "testdisk", "command": "echo 'testdisk: Interactive tool - run manually with: testdisk {target}'", "timeout": 5},
        ]
    },
}

@app.route("/api/presets", methods=["GET"])
def list_presets():
    # List all available scan presets with coverage information
    presets = {}
    for name, config in SCAN_PRESETS.items():
        presets[name] = {
            "description": config["description"],
            "coverage": config.get("coverage", ""),
            "tool_count": len(config["tools"]),
            "tools": [t["name"] for t in config["tools"]]
        }
    return jsonify({
        "presets": presets,
        "total_presets": len(presets),
        "categories": {
            "reconnaissance": ["recon_quick", "recon_full", "recon_stealth"],
            "vulnerability": ["vuln_quick", "vuln_full", "vuln_kev", "cve_2024"],
            "web_application": ["web_dirs", "web_params", "web_xss", "web_sqli", "content_discovery"],
            "osint": ["osint", "osint_deep"],
            "infrastructure": ["waf_detect", "ssl_check", "network_scan", "smb_enum"],
            "specialized": ["api_security", "cloud_security", "wordpress", "container_security"],
            "offensive": ["auth_testing", "exploit_research"],
            "forensics": ["forensics_analysis"]
        }
    })

@app.route("/api/scan/<preset>", methods=["POST"])
def run_preset_scan(preset: str):
    # Run a predefined scan preset against a target
    try:
        if preset not in SCAN_PRESETS:
            return jsonify({
                "error": f"Unknown preset: {preset}",
                "available": list(SCAN_PRESETS.keys())
            }), 400
        
        params = request.json
        target = params.get("target", "")
        
        if not target:
            return jsonify({"error": "target is required"}), 400
        
        # Sanitize target (basic validation)
        target = target.strip().replace("'", "").replace('"', '').replace(';', '').replace('|', '')
        
        config = SCAN_PRESETS[preset]
        job_ids = []
        
        for tool in config["tools"]:
            command = tool["command"].replace("{target}", target)
            timeout = tool.get("timeout", COMMAND_TIMEOUT)
            job_id = job_manager.create_job(command, timeout=timeout)
            job_ids.append({
                "tool": tool["name"],
                "job_id": job_id,
                "command": command[:100]
            })
        
        return jsonify({
            "success": True,
            "preset": preset,
            "description": config["description"],
            "target": target,
            "jobs": job_ids,
            "message": f"Started {len(job_ids)} tools. Poll /api/batch/status with job_ids to check progress."
        }), 202
        
    except Exception as e:
        logger.error(f"[!!] Error running preset: {str(e)}")
        return jsonify({"error": str(e)}), 500

# ============================================================================
# WORKFLOW SESSION INTEGRATION (v6.3)
# ============================================================================

def create_workflow_with_session(target: str, workflow_name: str, commands: List[Dict]) -> Dict:
    # Create a session and submit all workflow jobs with proper tracking.
    # Create session for this workflow
    session_id = session_manager.create_session(target, workflow_name)
    
    jobs = []
    for item in commands:
        tool_name = item.get("tool", "unknown")
        cmd = item.get("cmd", "")
        timeout = item.get("timeout", 300)
        
        # Create job through job_manager
        job_id = job_manager.create_job(cmd, timeout=timeout)
        
        # Register with session manager
        session_manager.add_job(session_id, job_id, tool_name, cmd)
        
        jobs.append({
            "tool": tool_name,
            "job_id": job_id
        })
    
    return {
        "session_id": session_id,
        "jobs": jobs,
        "total_jobs": len(jobs),
        "session_endpoints": {
            "status": f"/api/session/{session_id}",
            "findings": f"/api/session/{session_id}/findings",
            "assets": f"/api/session/{session_id}/assets",
            "report": f"/api/session/{session_id}/report",
            "files": f"/api/session/{session_id}/files"
        }
    }

# ============================================================================
# SCAN WORKFLOWS (v6.2 - Automated Multi-Stage Pipelines)
# Research-backed comprehensive security assessment workflows
# ============================================================================

@app.route("/api/workflow/full-recon", methods=["POST"])
def workflow_full_recon():
    # Complete reconnaissance workflow - 99% attack surface coverage
    try:
        params = request.json
        target = params.get("target", "").strip()
        
        if not target:
            return jsonify({"error": "target is required"}), 400
        
        target = target.replace("'", "").replace('"', '').replace(';', '').replace('|', '')
        
        # Create session for this workflow
        session_id = session_manager.create_session(target, "full-recon")
        
        workflow = {
            "workflow_name": "full-recon",
            "target": target,
            "session_id": session_id,
            "estimated_time": "15-20 minutes",
            "stages": {}
        }
        
        def add_job(cmd: str, tool_name: str, timeout: int = 300) -> str:
            job_id = job_manager.create_job(cmd, timeout=timeout)
            session_manager.add_job(session_id, job_id, tool_name, cmd)
            return job_id
        
        # Stage 1: Subdomain Discovery (multi-source)
        stage1_jobs = []
        for cmd, name in [
            (f"subfinder -d {target} -all -recursive -silent", "subfinder"),
            (f"amass enum -passive -d {target} -timeout 5 2>/dev/null | head -200", "amass"),
        ]:
            job_id = add_job(cmd, name, timeout=360)
            stage1_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["1_subdomain_discovery"] = stage1_jobs
        
        # Stage 2: Live Host Detection + Tech Stack
        stage2_jobs = []
        cmd = f"echo '{target}' | httpx -silent -sc -cl -title -td -ip -cname -cdn -asn -server -hash sha256 -jarm -rt -method -websocket -probe"
        job_id = add_job(cmd, "httpx_full", timeout=180)
        stage2_jobs.append({"tool": "httpx_full", "job_id": job_id})
        workflow["stages"]["2_live_host_tech_stack"] = stage2_jobs
        
        # Stage 3: Port Scanning + Service Detection
        stage3_jobs = []
        cmd = f"nmap -sV -sC -T4 --top-ports 1000 --open -oG - {target} 2>/dev/null | grep -v '^#'"
        job_id = add_job(cmd, "nmap_services", timeout=600)
        stage3_jobs.append({"tool": "nmap_services", "job_id": job_id})
        workflow["stages"]["3_port_service_scan"] = stage3_jobs
        
        # Stage 4: WAF/CDN Detection
        stage4_jobs = []
        for cmd, name in [
            (f"wafw00f https://{target} -a 2>/dev/null", "wafw00f"),
            (f"whatweb -a 3 --color=never https://{target} 2>/dev/null", "whatweb"),
        ]:
            job_id = add_job(cmd, name, timeout=120)
            stage4_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["4_waf_cdn_detection"] = stage4_jobs
        
        # Stage 5: URL/Endpoint Discovery
        stage5_jobs = []
        for cmd, name in [
            (f"katana -u https://{target} -d 3 -jc -kf -silent -nc 2>/dev/null | head -200", "katana"),
            (f"gau --subs {target} --blacklist ttf,woff,svg,png,jpg,gif,ico,css 2>/dev/null | head -200", "gau"),
            (f"waybackurls {target} 2>/dev/null | head -200", "waybackurls"),
        ]:
            job_id = add_job(cmd, name, timeout=240)
            stage5_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["5_url_endpoint_discovery"] = stage5_jobs
        
        # Stage 6: OSINT Gathering
        stage6_jobs = []
        for cmd, name in [
            (f"theHarvester -d {target} -b crtsh,dnsdumpster,hackertarget,rapiddns,urlscan -l 100 2>&1 | tail -100", "theHarvester"),
            (f"dig {target} A AAAA MX NS TXT SOA +short 2>/dev/null", "dig_records"),
            (f"whois {target} 2>/dev/null", "whois"),
        ]:
            job_id = add_job(cmd, name, timeout=180)
            stage6_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["6_osint_gathering"] = stage6_jobs
        
        # Collect all job IDs
        all_jobs = []
        for stage_jobs in workflow["stages"].values():
            all_jobs.extend([j["job_id"] for j in stage_jobs])
        
        workflow["total_jobs"] = len(all_jobs)
        workflow["all_job_ids"] = all_jobs
        workflow["message"] = "Full recon workflow started with session tracking."
        workflow["session_endpoints"] = {
            "status": f"/api/session/{session_id}",
            "findings": f"/api/session/{session_id}/findings",
            "assets": f"/api/session/{session_id}/assets",
            "report": f"/api/session/{session_id}/report?format=markdown",
            "files": f"/api/session/{session_id}/files"
        }
        
        return jsonify(workflow), 202
        
    except Exception as e:
        logger.error(f"[!!] Error in full-recon workflow: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/workflow/vuln-assessment", methods=["POST"])
def workflow_vuln_assessment():
    # Comprehensive vulnerability assessment workflow - 99% vuln coverage
    try:
        params = request.json
        target = params.get("target", "").strip()
        
        if not target:
            return jsonify({"error": "target is required"}), 400
        
        target = target.replace("'", "").replace('"', '').replace(';', '').replace('|', '')
        
        # Create session for this workflow
        session_id = session_manager.create_session(target, "vuln-assessment")
        
        workflow = {
            "workflow_name": "vuln-assessment",
            "target": target,
            "session_id": session_id,
            "estimated_time": "20-25 minutes",
            "stages": {}
        }
        
        def add_job(cmd: str, tool_name: str, timeout: int = 300) -> str:
            job_id = job_manager.create_job(cmd, timeout=timeout)
            session_manager.add_job(session_id, job_id, tool_name, cmd)
            return job_id
        
        # Stage 1: Known Exploited Vulnerabilities (Most Critical!)
        stage1_jobs = []
        for cmd, name in [
            (f"nuclei -u https://{target} -tags kev -silent -c 50 -rl 150", "nuclei_cisa_kev"),
            (f"nuclei -u https://{target} -tags vkev -silent -c 50 -rl 150", "nuclei_vulncheck_kev"),
        ]:
            job_id = add_job(cmd, name, timeout=600)
            stage1_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["1_known_exploited_vulns"] = stage1_jobs
        
        # Stage 2: Critical/High CVE Scan
        stage2_jobs = []
        cmd = f"nuclei -u https://{target} -severity critical,high -silent -c 50 -rl 150"
        job_id = add_job(cmd, "nuclei_critical_high", timeout=600)
        stage2_jobs.append({"tool": "nuclei_critical_high", "job_id": job_id})
        workflow["stages"]["2_critical_high_cves"] = stage2_jobs
        
        # Stage 3: Web Server Analysis
        stage3_jobs = []
        cmd = f"nikto -h https://{target} -Tuning 1234567890 -no404 2>&1 | tail -150"
        job_id = add_job(cmd, "nikto_comprehensive", timeout=600)
        stage3_jobs.append({"tool": "nikto_comprehensive", "job_id": job_id})
        workflow["stages"]["3_web_server_analysis"] = stage3_jobs
        
        # Stage 4: Directory/File Discovery
        stage4_jobs = []
        for cmd, name in [
            (f"gobuster dir -u https://{target} -w /usr/share/wordlists/dirb/common.txt -q -t 30 -x php,asp,aspx,jsp,html,js,txt,bak 2>/dev/null | head -100", "gobuster"),
            (f"feroxbuster -u https://{target} -w /usr/share/wordlists/dirb/common.txt -q -t 30 -x php,asp,html,txt --no-state 2>/dev/null | head -100", "feroxbuster"),
        ]:
            job_id = add_job(cmd, name, timeout=360)
            stage4_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["4_directory_file_discovery"] = stage4_jobs
        
        # Stage 5: Full Nuclei Scan (all severity + special tags)
        stage5_jobs = []
        for cmd, name in [
            (f"nuclei -u https://{target} -severity critical,high,medium -silent -c 50 -rl 150", "nuclei_all_severity"),
            (f"nuclei -u https://{target} -tags exposure,misconfig,default-login -silent -c 25 -rl 100", "nuclei_misconfigs"),
            (f"nuclei -u https://{target} -tags cve -silent -c 25 -rl 100", "nuclei_all_cve"),
        ]:
            job_id = add_job(cmd, name, timeout=600)
            stage5_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["5_full_nuclei_scan"] = stage5_jobs
        
        # Stage 6: Nmap Vulnerability Scripts
        stage6_jobs = []
        cmd = f"nmap -sV --script vuln,exploit --top-ports 100 {target} 2>/dev/null | tail -100"
        job_id = add_job(cmd, "nmap_vuln_scripts", timeout=600)
        stage6_jobs.append({"tool": "nmap_vuln_scripts", "job_id": job_id})
        workflow["stages"]["6_nmap_vuln_scripts"] = stage6_jobs
        
        # Collect all job IDs
        all_jobs = []
        for stage_jobs in workflow["stages"].values():
            all_jobs.extend([j["job_id"] for j in stage_jobs])
        
        workflow["total_jobs"] = len(all_jobs)
        workflow["all_job_ids"] = all_jobs
        workflow["message"] = "Vulnerability assessment started with session tracking."
        workflow["session_endpoints"] = {
            "status": f"/api/session/{session_id}",
            "findings": f"/api/session/{session_id}/findings",
            "assets": f"/api/session/{session_id}/assets",
            "report": f"/api/session/{session_id}/report?format=markdown",
            "files": f"/api/session/{session_id}/files"
        }
        
        return jsonify(workflow), 202
        
    except Exception as e:
        logger.error(f"[!!] Error in vuln-assessment workflow: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/workflow/complete-pentest", methods=["POST"])
def workflow_complete_pentest():
    # Complete penetration testing workflow - Full assessment
    try:
        params = request.json
        target = params.get("target", "").strip()
        
        if not target:
            return jsonify({"error": "target is required"}), 400
        
        target = target.replace("'", "").replace('"', '').replace(';', '').replace('|', '')
        
        # Create session for this workflow
        session_id = session_manager.create_session(target, "complete-pentest")
        
        workflow = {
            "workflow_name": "complete-pentest",
            "target": target,
            "session_id": session_id,
            "estimated_time": "30-40 minutes",
            "stages": {}
        }
        
        def add_job(cmd: str, tool_name: str, timeout: int = 300) -> str:
            job_id = job_manager.create_job(cmd, timeout=timeout)
            session_manager.add_job(session_id, job_id, tool_name, cmd)
            return job_id
        
        # Stage 1: Subdomain Enumeration
        stage1_jobs = []
        for cmd, name in [
            (f"subfinder -d {target} -all -recursive -silent", "subfinder"),
            (f"amass enum -passive -d {target} -timeout 5 2>/dev/null | head -200", "amass"),
        ]:
            job_id = add_job(cmd, name, timeout=360)
            stage1_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["1_subdomain_enum"] = stage1_jobs
        
        # Stage 2: Live Host + Tech Stack
        stage2_jobs = []
        cmd = f"echo '{target}' | httpx -silent -sc -cl -title -td -ip -cname -cdn -asn -server -hash sha256 -jarm -rt"
        job_id = add_job(cmd, "httpx_comprehensive", timeout=180)
        stage2_jobs.append({"tool": "httpx_comprehensive", "job_id": job_id})
        workflow["stages"]["2_live_host_tech"] = stage2_jobs
        
        # Stage 3: Port + Service Scanning
        stage3_jobs = []
        for cmd, name in [
            (f"nmap -sV -sC -T4 --top-ports 1000 --open -oG - {target} 2>/dev/null | grep -v '^#'", "nmap_tcp"),
            (f"nmap -sU -T4 --top-ports 50 --open {target} 2>/dev/null | grep -E '^[0-9]+'", "nmap_udp"),
        ]:
            job_id = add_job(cmd, name, timeout=600)
            stage3_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["3_port_service_scan"] = stage3_jobs
        
        # Stage 4: WAF/CDN + Security Headers
        stage4_jobs = []
        for cmd, name in [
            (f"wafw00f https://{target} -a 2>/dev/null", "wafw00f"),
            (f"whatweb -a 3 --color=never https://{target} 2>/dev/null", "whatweb"),
            (f"curl -sI https://{target} 2>/dev/null | grep -iE '(x-frame|x-xss|x-content|strict-transport|content-security|referrer-policy)'", "security_headers"),
        ]:
            job_id = add_job(cmd, name, timeout=120)
            stage4_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["4_waf_security_detection"] = stage4_jobs
        
        # Stage 5: Known Exploited Vulnerabilities
        stage5_jobs = []
        for cmd, name in [
            (f"nuclei -u https://{target} -tags kev,vkev -silent -c 50 -rl 150", "nuclei_kev"),
            (f"nuclei -u https://{target} -severity critical,high -silent -c 50 -rl 150", "nuclei_critical"),
        ]:
            job_id = add_job(cmd, name, timeout=600)
            stage5_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["5_kev_critical_vulns"] = stage5_jobs
        
        # Stage 6: Full Vulnerability Scan
        stage6_jobs = []
        for cmd, name in [
            (f"nuclei -u https://{target} -severity critical,high,medium -silent -c 50 -rl 150", "nuclei_all"),
            (f"nuclei -u https://{target} -tags exposure,misconfig,default-login -silent -c 25", "nuclei_misconfig"),
            (f"nikto -h https://{target} -Tuning 1234567890 -no404 2>&1 | tail -100", "nikto"),
        ]:
            job_id = add_job(cmd, name, timeout=600)
            stage6_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["6_full_vuln_scan"] = stage6_jobs
        
        # Stage 7: Web Application Testing
        stage7_jobs = []
        for cmd, name in [
            (f"gobuster dir -u https://{target} -w /usr/share/wordlists/dirb/common.txt -q -t 30 -x php,asp,html,txt,bak 2>/dev/null | head -80", "gobuster"),
            (f"katana -u https://{target} -d 3 -jc -kf -silent -nc 2>/dev/null | head -150", "katana"),
            (f"gau --subs {target} 2>/dev/null | head -150", "gau"),
        ]:
            job_id = add_job(cmd, name, timeout=360)
            stage7_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["7_web_app_testing"] = stage7_jobs
        
        # Stage 8: OSINT
        stage8_jobs = []
        for cmd, name in [
            (f"theHarvester -d {target} -b crtsh,dnsdumpster,hackertarget,rapiddns -l 100 2>&1 | tail -80", "theHarvester"),
            (f"whois {target} 2>/dev/null", "whois"),
            (f"dig {target} A AAAA MX NS TXT SOA +short 2>/dev/null", "dig"),
        ]:
            job_id = add_job(cmd, name, timeout=180)
            stage8_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["8_osint"] = stage8_jobs
        
        # Collect all job IDs
        all_jobs = []
        for stage_jobs in workflow["stages"].values():
            all_jobs.extend([j["job_id"] for j in stage_jobs])
        
        workflow["total_jobs"] = len(all_jobs)
        workflow["all_job_ids"] = all_jobs
        workflow["message"] = "Complete pentest workflow started with session tracking."
        workflow["session_endpoints"] = {
            "status": f"/api/session/{session_id}",
            "findings": f"/api/session/{session_id}/findings",
            "assets": f"/api/session/{session_id}/assets",
            "report": f"/api/session/{session_id}/report?format=markdown",
            "files": f"/api/session/{session_id}/files"
        }
        
        return jsonify(workflow), 202
        
    except Exception as e:
        logger.error(f"[!!] Error in complete-pentest workflow: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/workflow/bug-bounty-quick", methods=["POST"])
def workflow_bug_bounty_quick():
    # Quick bug bounty workflow - fast high-impact findings (10-12 min)
    try:
        params = request.json
        target = params.get("target", "").strip()
        
        if not target:
            return jsonify({"error": "target is required"}), 400
        
        target = target.replace("'", "").replace('"', '').replace(';', '').replace('|', '')
        
        # Create session for this workflow
        session_id = session_manager.create_session(target, "bug-bounty-quick")
        
        workflow = {
            "workflow_name": "bug-bounty-quick",
            "target": target,
            "session_id": session_id,
            "estimated_time": "10-12 minutes",
            "stages": {}
        }
        
        def add_job(cmd: str, tool_name: str, timeout: int = 300) -> str:
            # Helper to create job and register with session
            job_id = job_manager.create_job(cmd, timeout=timeout)
            session_manager.add_job(session_id, job_id, tool_name, cmd)
            return job_id
        
        # Stage 1: Quick Subdomain Enumeration
        stage1_jobs = []
        for cmd, name in [
            (f"subfinder -d {target} -all -silent", "subfinder"),
            (f"curl -s 'https://crt.sh/?q=%25.{target}&output=json' 2>/dev/null | jq -r '.[].name_value' 2>/dev/null | sort -u | head -100", "crtsh"),
        ]:
            job_id = add_job(cmd, name, timeout=180)
            stage1_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["1_subdomain_enum"] = stage1_jobs
        
        # Stage 2: Live Host + Tech Stack
        stage2_jobs = []
        cmd = f"echo '{target}' | httpx-toolkit -silent -sc -title -td -ip -server"
        job_id = add_job(cmd, "httpx", timeout=120)
        stage2_jobs.append({"tool": "httpx", "job_id": job_id})
        workflow["stages"]["2_live_host_tech"] = stage2_jobs
        
        # Stage 3: Known Exploited Vulnerabilities (High Impact)
        stage3_jobs = []
        for cmd, name in [
            (f"nuclei -u https://{target} -tags kev,vkev -silent -c 50 -rl 150", "nuclei_kev"),
            (f"nuclei -u https://{target} -severity critical,high -silent -c 50 -rl 150", "nuclei_critical"),
        ]:
            job_id = add_job(cmd, name, timeout=300)
            stage3_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["3_kev_critical_vulns"] = stage3_jobs
        
        # Stage 4: Content Discovery (Sensitive Files)
        stage4_jobs = []
        for cmd, name in [
            (f"nuclei -u https://{target} -tags exposure,backup,config,git -silent -c 25", "nuclei_exposure"),
            (f"ffuf -u https://{target}/FUZZ -w /usr/share/wordlists/dirb/common.txt -mc 200,301,302,401,403 -s -t 50 2>/dev/null | head -50", "ffuf"),
        ]:
            job_id = add_job(cmd, name, timeout=240)
            stage4_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["4_content_discovery"] = stage4_jobs
        
        # Stage 5: Parameter Discovery + XSS/SQLi Quick Check
        stage5_jobs = []
        for cmd, name in [
            (f"katana -u https://{target} -d 2 -jc -kf all -silent -nc 2>/dev/null | head -100", "katana"),
            (f"gau {target} 2>/dev/null | grep '=' | head -50", "gau_params"),
            (f"nuclei -u https://{target} -tags xss,sqli -silent -c 25", "nuclei_injection"),
        ]:
            job_id = add_job(cmd, name, timeout=180)
            stage5_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["5_params_injection"] = stage5_jobs
        
        # Collect all job IDs
        all_jobs = []
        for stage_jobs in workflow["stages"].values():
            all_jobs.extend([j["job_id"] for j in stage_jobs])
        
        workflow["total_jobs"] = len(all_jobs)
        workflow["all_job_ids"] = all_jobs
        workflow["message"] = "Bug bounty quick scan started with session tracking."
        workflow["session_endpoints"] = {
            "status": f"/api/session/{session_id}",
            "findings": f"/api/session/{session_id}/findings",
            "assets": f"/api/session/{session_id}/assets",
            "report": f"/api/session/{session_id}/report?format=markdown",
            "files": f"/api/session/{session_id}/files"
        }
        
        return jsonify(workflow), 202
        
    except Exception as e:
        logger.error(f"[!!] Error in bug-bounty-quick workflow: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/workflow/api-pentest", methods=["POST"])
def workflow_api_pentest():
    # API Security Penetration Testing workflow (15-20 min)
    try:
        params = request.json
        target = params.get("target", "").strip()
        
        if not target:
            return jsonify({"error": "target is required"}), 400
        
        target = target.replace("'", "").replace('"', '').replace(';', '').replace('|', '')
        
        # Create session for this workflow
        session_id = session_manager.create_session(target, "api-pentest")
        
        workflow = {
            "workflow_name": "api-pentest",
            "target": target,
            "session_id": session_id,
            "estimated_time": "15-20 minutes",
            "stages": {}
        }
        
        def add_job(cmd: str, tool_name: str, timeout: int = 300) -> str:
            job_id = job_manager.create_job(cmd, timeout=timeout)
            session_manager.add_job(session_id, job_id, tool_name, cmd)
            return job_id
        
        # Stage 1: API Endpoint Discovery
        stage1_jobs = []
        for cmd, name in [
            (f"katana -u https://{target} -d 3 -jc -kf all -silent -nc 2>/dev/null | grep -E '/api/|/v[0-9]/|/graphql' | head -100", "katana_api"),
            (f"gau {target} 2>/dev/null | grep -E '/api/|/v[0-9]/|/rest/|/graphql' | head -100", "gau_api"),
            (f"ffuf -u https://{target}/FUZZ -w /usr/share/wordlists/dirb/common.txt -mc 200,201,204,301,302,401,403,405 -s 2>/dev/null | head -50", "ffuf_api"),
        ]:
            job_id = add_job(cmd, name, timeout=240)
            stage1_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["1_endpoint_discovery"] = stage1_jobs
        
        # Stage 2: API Documentation Exposure
        stage2_jobs = []
        for cmd, name in [
            (f"nuclei -u https://{target} -tags swagger,openapi,graphql,api-docs -silent -c 25", "nuclei_api_docs"),
            (f"for p in swagger.json openapi.json api-docs swagger/v1/swagger.json v1/swagger.json v2/api-docs graphql; do curl -s -o /dev/null -w '%{{http_code}} https://{target}/'$p'\\n' https://{target}/$p 2>/dev/null; done", "curl_api_docs"),
        ]:
            job_id = add_job(cmd, name, timeout=120)
            stage2_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["2_api_docs_exposure"] = stage2_jobs
        
        # Stage 3: Authentication Testing
        stage3_jobs = []
        for cmd, name in [
            (f"nuclei -u https://{target} -tags auth-bypass,jwt,token,api-key -silent -c 25", "nuclei_auth"),
            (f"nuclei -u https://{target} -tags default-login,weak-credentials -silent -c 25", "nuclei_creds"),
        ]:
            job_id = add_job(cmd, name, timeout=180)
            stage3_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["3_auth_testing"] = stage3_jobs
        
        # Stage 4: Injection Vulnerabilities
        stage4_jobs = []
        for cmd, name in [
            (f"nuclei -u https://{target} -tags sqli,nosqli,injection -silent -c 25", "nuclei_sqli"),
            (f"nuclei -u https://{target} -tags ssrf,xxe,ssti -silent -c 25", "nuclei_injection"),
            (f"arjun -u https://{target} -t 10 --stable 2>/dev/null | head -30", "arjun_params"),
        ]:
            job_id = add_job(cmd, name, timeout=240)
            stage4_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["4_injection_vulns"] = stage4_jobs
        
        # Stage 5: Business Logic / BOLA/IDOR
        stage5_jobs = []
        for cmd, name in [
            (f"nuclei -u https://{target} -tags idor,bola,access-control -silent -c 25", "nuclei_bola"),
            (f"nuclei -u https://{target} -tags api -silent -c 25", "nuclei_api_vulns"),
        ]:
            job_id = add_job(cmd, name, timeout=180)
            stage5_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["5_business_logic"] = stage5_jobs
        
        # Stage 6: Rate Limiting / Information Disclosure
        stage6_jobs = []
        for cmd, name in [
            (f"nuclei -u https://{target} -tags ratelimit,dos -silent -c 10", "nuclei_ratelimit"),
            (f"nuclei -u https://{target} -tags disclosure,debug,error -silent -c 25", "nuclei_disclosure"),
        ]:
            job_id = add_job(cmd, name, timeout=120)
            stage6_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["6_ratelimit_disclosure"] = stage6_jobs
        
        # Collect all job IDs
        all_jobs = []
        for stage_jobs in workflow["stages"].values():
            all_jobs.extend([j["job_id"] for j in stage_jobs])
        
        workflow["total_jobs"] = len(all_jobs)
        workflow["all_job_ids"] = all_jobs
        workflow["message"] = "API pentest workflow started with session tracking."
        workflow["session_endpoints"] = {
            "status": f"/api/session/{session_id}",
            "findings": f"/api/session/{session_id}/findings",
            "assets": f"/api/session/{session_id}/assets",
            "report": f"/api/session/{session_id}/report?format=markdown",
            "files": f"/api/session/{session_id}/files"
        }
        
        return jsonify(workflow), 202
        
    except Exception as e:
        logger.error(f"[!!] Error in api-pentest workflow: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/workflow/internal-network", methods=["POST"])
def workflow_internal_network():
    # Internal Network Assessment workflow (20-30 min)
    try:
        params = request.json
        target = params.get("target", "").strip()  # Can be IP, range, or CIDR
        
        if not target:
            return jsonify({"error": "target is required (IP, range, or CIDR)"}), 400
        
        target = target.replace("'", "").replace('"', '').replace(';', '').replace('|', '')
        
        # Create session for this workflow
        session_id = session_manager.create_session(target, "internal-network")
        
        workflow = {
            "workflow_name": "internal-network",
            "target": target,
            "session_id": session_id,
            "estimated_time": "20-30 minutes",
            "stages": {}
        }
        
        def add_job(cmd: str, tool_name: str, timeout: int = 300) -> str:
            job_id = job_manager.create_job(cmd, timeout=timeout)
            session_manager.add_job(session_id, job_id, tool_name, cmd)
            return job_id
        
        # Stage 1: Network Discovery
        stage1_jobs = []
        for cmd, name in [
            (f"arp-scan -l 2>/dev/null | head -50 || echo 'arp-scan: Requires local network'", "arp_scan"),
            (f"nmap -sn {target} 2>/dev/null | grep -E 'Nmap scan|Host is' | head -50", "nmap_ping"),
            (f"nbtscan {target} 2>/dev/null | head -30", "nbtscan"),
        ]:
            job_id = add_job(cmd, name, timeout=180)
            stage1_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["1_network_discovery"] = stage1_jobs
        
        # Stage 2: Port Scanning
        stage2_jobs = []
        for cmd, name in [
            (f"nmap -sV -sC -T4 --top-ports 1000 --open {target} 2>/dev/null | tail -150", "nmap_tcp"),
            (f"nmap -sU -T4 --top-ports 50 --open {target} 2>/dev/null | grep -E '^[0-9]+' | head -30", "nmap_udp"),
            (f"masscan {target} -p1-10000 --rate=1000 2>/dev/null | head -50", "masscan"),
        ]:
            job_id = add_job(cmd, name, timeout=600)
            stage2_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["2_port_scanning"] = stage2_jobs
        
        # Stage 3: Service Enumeration
        stage3_jobs = []
        for cmd, name in [
            (f"nmap --script banner,version -sV --top-ports 200 {target} 2>/dev/null | tail -100", "nmap_banner"),
            (f"nmap --script snmp-info,snmp-interfaces -sU -p 161 {target} 2>/dev/null | tail -50", "nmap_snmp"),
        ]:
            job_id = add_job(cmd, name, timeout=300)
            stage3_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["3_service_enum"] = stage3_jobs
        
        # Stage 4: SMB/Windows Enumeration
        stage4_jobs = []
        for cmd, name in [
            (f"smbmap -H {target} 2>/dev/null || smbmap -H {target} -u '' -p '' 2>/dev/null", "smbmap"),
            (f"enum4linux-ng -A {target} 2>/dev/null | head -150", "enum4linux"),
            (f"nxc smb {target} --shares 2>/dev/null | head -30", "nxc_shares"),
            (f"rpcclient -U '' -N {target} -c 'enumdomusers; enumdomgroups' 2>/dev/null | head -50", "rpcclient"),
            (f"nmap --script smb-enum-shares,smb-enum-users,smb-os-discovery,smb-vuln* -p 139,445 {target} 2>/dev/null | tail -80", "nmap_smb"),
        ]:
            job_id = add_job(cmd, name, timeout=300)
            stage4_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["4_smb_windows_enum"] = stage4_jobs
        
        # Stage 5: Vulnerability Scanning
        stage5_jobs = []
        for cmd, name in [
            (f"nmap --script vuln,exploit --top-ports 100 {target} 2>/dev/null | tail -100", "nmap_vuln"),
            (f"nmap --script smb-vuln* -p 139,445 {target} 2>/dev/null | tail -50", "nmap_smb_vuln"),
        ]:
            job_id = add_job(cmd, name, timeout=600)
            stage5_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["5_vuln_scanning"] = stage5_jobs
        
        # Stage 6: Credential Testing (light)
        stage6_jobs = []
        for cmd, name in [
            (f"nmap --script ftp-anon,ssh-auth-methods -p 21,22 {target} 2>/dev/null | tail -30", "nmap_anon"),
            (f"nxc smb {target} -u '' -p '' 2>/dev/null | head -20", "nxc_null"),
        ]:
            job_id = add_job(cmd, name, timeout=180)
            stage6_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["6_credential_testing"] = stage6_jobs
        
        # Collect all job IDs
        all_jobs = []
        for stage_jobs in workflow["stages"].values():
            all_jobs.extend([j["job_id"] for j in stage_jobs])
        
        workflow["total_jobs"] = len(all_jobs)
        workflow["all_job_ids"] = all_jobs
        workflow["message"] = "Internal network assessment started with session tracking."
        workflow["session_endpoints"] = {
            "status": f"/api/session/{session_id}",
            "findings": f"/api/session/{session_id}/findings",
            "assets": f"/api/session/{session_id}/assets",
            "report": f"/api/session/{session_id}/report?format=markdown",
            "files": f"/api/session/{session_id}/files"
        }
        
        return jsonify(workflow), 202
        
    except Exception as e:
        logger.error(f"[!!] Error in internal-network workflow: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/workflow/red-team-full", methods=["POST"])
def workflow_red_team_full():
    # Full Red Team Assessment workflow (45-60 min)
    try:
        params = request.json
        target = params.get("target", "").strip()
        
        if not target:
            return jsonify({"error": "target is required"}), 400
        
        target = target.replace("'", "").replace('"', '').replace(';', '').replace('|', '')
        
        # Create session for this workflow
        session_id = session_manager.create_session(target, "red-team-full")
        
        workflow = {
            "workflow_name": "red-team-full",
            "target": target,
            "session_id": session_id,
            "estimated_time": "45-60 minutes",
            "stages": {}
        }
        
        def add_job(cmd: str, tool_name: str, timeout: int = 300) -> str:
            job_id = job_manager.create_job(cmd, timeout=timeout)
            session_manager.add_job(session_id, job_id, tool_name, cmd)
            return job_id
        
        # Stage 1: OSINT & Reconnaissance
        stage1_jobs = []
        for cmd, name in [
            (f"theHarvester -d {target} -b all -l 200 2>&1 | tail -100", "theHarvester"),
            (f"subfinder -d {target} -all -recursive -silent", "subfinder"),
            (f"amass enum -passive -d {target} -timeout 5 2>/dev/null | head -150", "amass"),
            (f"whois {target} 2>/dev/null", "whois"),
            (f"curl -s 'https://crt.sh/?q=%25.{target}&output=json' 2>/dev/null | jq -r '.[].name_value' 2>/dev/null | sort -u | head -100", "crtsh"),
        ]:
            job_id = add_job(cmd, name, timeout=360)
            stage1_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["1_osint_recon"] = stage1_jobs
        
        # Stage 2: Infrastructure Mapping
        stage2_jobs = []
        for cmd, name in [
            (f"echo '{target}' | httpx -silent -sc -cl -title -td -ip -cname -cdn -asn -server -hash sha256 -jarm", "httpx"),
            (f"nmap -sV -sC -T4 --top-ports 1000 --open -oG - {target} 2>/dev/null | grep -v '^#'", "nmap_tcp"),
            (f"masscan {target} -p1-10000 --rate=1000 2>/dev/null | head -80", "masscan"),
            (f"wafw00f https://{target} -a 2>/dev/null", "wafw00f"),
        ]:
            job_id = add_job(cmd, name, timeout=600)
            stage2_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["2_infrastructure_mapping"] = stage2_jobs
        
        # Stage 3: Vulnerability Discovery
        stage3_jobs = []
        for cmd, name in [
            (f"nuclei -u https://{target} -tags kev,vkev -silent -c 50 -rl 150", "nuclei_kev"),
            (f"nuclei -u https://{target} -severity critical,high -silent -c 50 -rl 150", "nuclei_critical"),
            (f"nuclei -u https://{target} -tags cve -silent -c 25 -rl 100", "nuclei_cve"),
            (f"nuclei -u https://{target} -tags exposure,misconfig,default-login -silent -c 25", "nuclei_misconfig"),
            (f"nikto -h https://{target} -Tuning 1234567890 -no404 2>&1 | tail -100", "nikto"),
            (f"nmap --script vuln,exploit --top-ports 100 {target} 2>/dev/null | tail -80", "nmap_vuln"),
        ]:
            job_id = add_job(cmd, name, timeout=600)
            stage3_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["3_vuln_discovery"] = stage3_jobs
        
        # Stage 4: Exploitation Research
        stage4_jobs = []
        for cmd, name in [
            (f"searchsploit {target} 2>/dev/null | head -40", "searchsploit"),
            (f"nuclei -u https://{target} -tags rce,lfi,ssrf -silent -c 25", "nuclei_rce"),
        ]:
            job_id = add_job(cmd, name, timeout=180)
            stage4_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["4_exploit_research"] = stage4_jobs
        
        # Stage 5: Web Application Testing
        stage5_jobs = []
        for cmd, name in [
            (f"gobuster dir -u https://{target} -w /usr/share/wordlists/dirb/common.txt -q -t 40 -x php,asp,html,txt,bak,json 2>/dev/null | head -80", "gobuster"),
            (f"katana -u https://{target} -d 3 -jc -kf -silent -nc 2>/dev/null | head -150", "katana"),
            (f"gau --subs {target} 2>/dev/null | head -150", "gau"),
            (f"nuclei -u https://{target} -tags xss,sqli,injection -silent -c 25", "nuclei_injection"),
            (f"sqlmap -u 'https://{target}' --batch --level=2 --risk=2 --crawl=2 --forms --random-agent 2>/dev/null | tail -50", "sqlmap"),
        ]:
            job_id = add_job(cmd, name, timeout=480)
            stage5_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["5_web_app_testing"] = stage5_jobs
        
        # Stage 6: Authentication Attacks
        stage6_jobs = []
        for cmd, name in [
            (f"nuclei -u https://{target} -tags default-login,weak-credentials -silent -c 50", "nuclei_default"),
            (f"nuclei -u https://{target} -tags auth-bypass,jwt -silent -c 25", "nuclei_auth"),
            (f"nmap --script http-default-accounts,ftp-anon,ssh-auth-methods -p 21,22,80,443,8080 {target} 2>/dev/null | tail -40", "nmap_auth"),
        ]:
            job_id = add_job(cmd, name, timeout=300)
            stage6_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["6_auth_attacks"] = stage6_jobs
        
        # Stage 7: Post-Exploitation Prep (info gathering for next steps)
        stage7_jobs = []
        for cmd, name in [
            (f"nmap --script smb-enum-shares,smb-enum-users,smb-os-discovery -p 139,445 {target} 2>/dev/null | tail -60", "nmap_smb"),
            (f"smbmap -H {target} 2>/dev/null | head -30", "smbmap"),
            (f"nxc smb {target} --shares 2>/dev/null | head -20", "nxc_shares"),
        ]:
            job_id = add_job(cmd, name, timeout=180)
            stage7_jobs.append({"tool": name, "job_id": job_id})
        workflow["stages"]["7_post_exploit_prep"] = stage7_jobs
        
        # Collect all job IDs
        all_jobs = []
        for stage_jobs in workflow["stages"].values():
            all_jobs.extend([j["job_id"] for j in stage_jobs])
        
        workflow["total_jobs"] = len(all_jobs)
        workflow["all_job_ids"] = all_jobs
        workflow["message"] = "Red team full assessment started with session tracking."
        workflow["session_endpoints"] = {
            "status": f"/api/session/{session_id}",
            "findings": f"/api/session/{session_id}/findings",
            "assets": f"/api/session/{session_id}/assets",
            "report": f"/api/session/{session_id}/report?format=markdown",
            "files": f"/api/session/{session_id}/files"
        }
        
        return jsonify(workflow), 202
        
    except Exception as e:
        logger.error(f"[!!] Error in red-team-full workflow: {str(e)}")
        return jsonify({"error": str(e)}), 500

# ============================================================================
# RESULT AGGREGATION & REPORTING (v6.2)
# ============================================================================

@app.route("/api/report", methods=["POST"])
def generate_report():
    # Aggregate results from multiple jobs into a unified report
    try:
        params = request.json
        job_ids = params.get("job_ids", [])
        report_format = params.get("format", "json")
        include_raw = params.get("include_raw", False)
        
        if not job_ids:
            return jsonify({"error": "job_ids is required"}), 400
        
        # Collect all job results
        results = []
        completed = 0
        failed = 0
        pending = 0
        
        for job_id in job_ids:
            job_status = job_manager.get_job_status(job_id)
            if "error" not in job_status:
                status = job_status.get("status", "")
                if status == "completed":
                    completed += 1
                    # Get output from result
                    result = job_status.get("result", {})
                    output = result.get("stdout", "") or result.get("stderr", "")
                    results.append({
                        "job_id": job_id,
                        "command": job_status.get("command", "")[:100],
                        "output": output,
                        "exit_code": result.get("return_code", 0)
                    })
                elif status == "failed":
                    failed += 1
                else:
                    pending += 1
        
        # Parse and categorize findings
        findings = {
            "critical": [],
            "high": [],
            "medium": [],
            "low": [],
            "info": []
        }
        
        # Pattern matching for severity
        critical_patterns = ["critical", "rce", "remote code execution", "command injection", "kev", "cve-202"]
        high_patterns = ["high", "sqli", "sql injection", "xss", "ssrf", "lfi", "rfi", "xxe", "auth bypass"]
        medium_patterns = ["medium", "csrf", "open redirect", "information disclosure", "misconfig"]
        low_patterns = ["low", "cookie", "header", "version"]
        
        for result in results:
            output = result.get("output", "").lower()
            finding = {
                "source": result.get("command", "")[:50],
                "job_id": result.get("job_id"),
                "snippet": result.get("output", "")[:500] if not include_raw else result.get("output", "")
            }
            
            # Categorize by severity based on output content
            if any(p in output for p in critical_patterns):
                if "[critical]" in output or "severity:critical" in output:
                    findings["critical"].append(finding)
                elif any(p in output for p in high_patterns):
                    findings["high"].append(finding)
                else:
                    findings["critical"].append(finding)
            elif any(p in output for p in high_patterns):
                findings["high"].append(finding)
            elif any(p in output for p in medium_patterns):
                findings["medium"].append(finding)
            elif any(p in output for p in low_patterns):
                findings["low"].append(finding)
            elif result.get("output", "").strip():
                findings["info"].append(finding)
        
        report = {
            "report_generated": datetime.now().isoformat(),
            "summary": {
                "total_jobs": len(job_ids),
                "completed": completed,
                "failed": failed,
                "pending": pending,
                "findings_by_severity": {
                    "critical": len(findings["critical"]),
                    "high": len(findings["high"]),
                    "medium": len(findings["medium"]),
                    "low": len(findings["low"]),
                    "info": len(findings["info"])
                }
            },
            "findings": findings
        }
        
        if report_format == "markdown":
            # Generate markdown report
            total_jobs = completed + failed + pending
            md = "# Security Assessment Report\n"
            md += f"Generated: {report['report_generated']}\n\n"
            md += "## Summary\n"
            md += f"- **Total Jobs:** {total_jobs}\n"
            md += f"- **Completed:** {completed}\n"
            md += f"- **Failed:** {failed}\n"
            md += f"- **Pending:** {pending}\n\n"
            md += "## Findings by Severity\n"
            md += "| Severity | Count |\n"
            md += "|----------|-------|\n"
            md += f"| Critical | {len(findings['critical'])} |\n"
            md += f"| High | {len(findings['high'])} |\n"
            md += f"| Medium | {len(findings['medium'])} |\n"
            md += f"| Low | {len(findings['low'])} |\n"
            md += f"| Info | {len(findings['info'])} |\n\n"
            for severity in ["critical", "high", "medium", "low"]:
                if findings[severity]:
                    md += f"\n## {severity.upper()} Findings\n\n"
                    for i, f in enumerate(findings[severity][:10], 1):  # Limit to 10 per category
                        md += f"### {i}. {f['source']}\n```\n{f['snippet'][:300]}...\n```\n\n"
            
            return jsonify({"report": md, "format": "markdown"}), 200
        
        return jsonify(report), 200
        
    except Exception as e:
        logger.error(f"[!!] Error generating report: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/report/<job_id>", methods=["GET"])
def get_single_job_report(job_id: str):
    # Get a detailed report for a single job
    try:
        job_status = job_manager.get_job_status(job_id)
        if "error" in job_status:
            return jsonify({"error": f"Job {job_id} not found"}), 404
        
        result = job_status.get("result", {})
        return jsonify({
            "job_id": job_id,
            "status": job_status.get("status"),
            "command": job_status.get("command"),
            "output": result.get("stdout", "") or result.get("stderr", ""),
            "exit_code": result.get("return_code"),
            "created_at": job_status.get("created_at"),
            "completed_at": job_status.get("completed_at"),
            "elapsed_seconds": job_status.get("elapsed_seconds")
        }), 200
        
    except Exception as e:
        logger.error(f"[!!] Error getting job report: {str(e)}")
        return jsonify({"error": str(e)}), 500

# ============================================================================
# SESSION API ENDPOINTS (v6.3 - Persistent Storage)
# ============================================================================

@app.route("/api/session/create", methods=["POST"])
def create_session():
    # Create a new scan session for persistent storage
    try:
        data = request.get_json() or {}
        target = data.get("target", "unknown")
        session_type = data.get("type", "manual")
        notes = data.get("notes", "")
        
        session_id = session_manager.create_session(target, session_type, notes)
        session_dir = session_manager.get_session_dir(session_id)
        
        return jsonify({
            "success": True,
            "session_id": session_id,
            "target": target,
            "type": session_type,
            "directory": str(session_dir) if session_dir else None,
            "message": f"Session created. Use session_id in subsequent requests for persistent storage."
        }), 201
        
    except Exception as e:
        logger.error(f"[!!] Error creating session: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/session/<session_id>", methods=["GET"])
def get_session(session_id: str):
    # Get session details and status
    try:
        session = session_manager.get_session(session_id)
        if not session:
            return jsonify({"error": "Session not found"}), 404
        
        summary = session_manager.get_session_summary(session_id)
        return jsonify({
            "session": session,
            "summary": summary
        }), 200
        
    except Exception as e:
        logger.error(f"[!!] Error getting session: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/session/<session_id>/jobs", methods=["GET"])
def get_session_jobs(session_id: str):
    # Get all jobs for a session
    try:
        jobs = session_manager.get_session_jobs(session_id)
        return jsonify({
            "session_id": session_id,
            "jobs": jobs,
            "count": len(jobs)
        }), 200
        
    except Exception as e:
        logger.error(f"[!!] Error getting session jobs: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/session/<session_id>/findings", methods=["GET"])
def get_session_findings(session_id: str):
    # Get parsed findings for a session
    try:
        severity = request.args.get("severity")
        findings = session_manager.get_session_findings(session_id, severity)
        
        # Group by severity for summary
        by_severity = {}
        for f in findings:
            sev = f.get("severity", "info")
            if sev not in by_severity:
                by_severity[sev] = []
            by_severity[sev].append(f)
        
        return jsonify({
            "session_id": session_id,
            "total_findings": len(findings),
            "by_severity": {sev: len(items) for sev, items in by_severity.items()},
            "findings": findings
        }), 200
        
    except Exception as e:
        logger.error(f"[!!] Error getting session findings: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/session/<session_id>/assets", methods=["GET"])
def get_session_assets(session_id: str):
    # Get discovered assets for a session
    try:
        asset_type = request.args.get("type")
        assets = session_manager.get_session_assets(session_id, asset_type)
        
        # Group by type
        by_type = {}
        for a in assets:
            atype = a.get("asset_type", "unknown")
            if atype not in by_type:
                by_type[atype] = []
            by_type[atype].append(a.get("value"))
        
        return jsonify({
            "session_id": session_id,
            "total_assets": len(assets),
            "by_type": {t: len(items) for t, items in by_type.items()},
            "assets": by_type if not asset_type else {asset_type: by_type.get(asset_type, [])}
        }), 200
        
    except Exception as e:
        logger.error(f"[!!] Error getting session assets: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/session/<session_id>/report", methods=["GET"])
def get_session_report(session_id: str):
    # Generate comprehensive session report
    try:
        format_type = request.args.get("format", "json")
        report = session_manager.generate_session_report(session_id, format_type)
        
        if "error" in report:
            return jsonify(report), 404
        
        return jsonify(report), 200
        
    except Exception as e:
        logger.error(f"[!!] Error generating session report: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/session/<session_id>/files", methods=["GET"])
def list_session_files(session_id: str):
    # List all output files for a session
    try:
        session_dir = session_manager.get_session_dir(session_id)
        if not session_dir or not session_dir.exists():
            return jsonify({"error": "Session directory not found"}), 404
        
        files = {}
        for subdir in ["raw", "parsed"]:
            dir_path = session_dir / subdir
            if dir_path.exists():
                files[subdir] = [f.name for f in dir_path.iterdir() if f.is_file()]
        
        return jsonify({
            "session_id": session_id,
            "directory": str(session_dir),
            "files": files
        }), 200
        
    except Exception as e:
        logger.error(f"[!!] Error listing session files: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/session/<session_id>/file/<path:filename>", methods=["GET"])
def get_session_file(session_id: str, filename: str):
    # Get contents of a specific session file
    try:
        session_dir = session_manager.get_session_dir(session_id)
        if not session_dir:
            return jsonify({"error": "Session not found"}), 404
        
        # Check in raw and parsed directories
        for subdir in ["raw", "parsed", ""]:
            file_path = session_dir / subdir / filename if subdir else session_dir / filename
            if file_path.exists() and file_path.is_file():
                with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                    content = f.read()
                return jsonify({
                    "session_id": session_id,
                    "filename": filename,
                    "path": str(file_path),
                    "content": content,
                    "size": file_path.stat().st_size
                }), 200
        
        return jsonify({"error": f"File {filename} not found"}), 404
        
    except Exception as e:
        logger.error(f"[!!] Error reading session file: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/session/<session_id>/complete", methods=["POST"])
def complete_session(session_id: str):
    # Mark a session as completed
    try:
        session_manager.complete_session(session_id)
        return jsonify({
            "success": True,
            "session_id": session_id,
            "status": "completed"
        }), 200
        
    except Exception as e:
        logger.error(f"[!!] Error completing session: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/sessions", methods=["GET"])
def list_sessions():
    # List all sessions
    try:
        limit = request.args.get("limit", 20, type=int)
        sessions = session_manager.list_sessions(limit)
        return jsonify({
            "sessions": sessions,
            "count": len(sessions)
        }), 200
        
    except Exception as e:
        logger.error(f"[!!] Error listing sessions: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/session/scan", methods=["POST"])
def session_scan():
    # Run a scan with automatic session management
    try:
        data = request.get_json() or {}
        target = data.get("target")
        tools = data.get("tools", [])
        workflow = data.get("workflow")
        
        if not target:
            return jsonify({"error": "Target is required"}), 400
        
        if not tools and not workflow:
            return jsonify({"error": "Either 'tools' or 'workflow' is required"}), 400
        
        # Create session
        session_type = workflow if workflow else "custom"
        session_id = session_manager.create_session(target, session_type)
        
        job_ids = []
        
        if workflow:
            # Use workflow endpoint logic (simplified)
            # This would trigger the appropriate workflow
            pass
        else:
            # Run individual tools
            for tool_config in tools:
                tool_name = tool_config if isinstance(tool_config, str) else tool_config.get("tool")
                extra_args = "" if isinstance(tool_config, str) else tool_config.get("args", "")
                
                # Generate command based on tool
                command = f"{tool_name} {extra_args}".strip() if extra_args else tool_name
                
                # Submit job
                job_id = job_manager.submit_job(command, category=tool_name, timeout=300)
                session_manager.add_job(session_id, job_id, tool_name, command)
                job_ids.append({"job_id": job_id, "tool": tool_name})
        
        return jsonify({
            "success": True,
            "session_id": session_id,
            "target": target,
            "jobs": job_ids,
            "total_jobs": len(job_ids),
            "endpoints": {
                "status": f"/api/session/{session_id}",
                "jobs": f"/api/session/{session_id}/jobs",
                "findings": f"/api/session/{session_id}/findings",
                "assets": f"/api/session/{session_id}/assets",
                "report": f"/api/session/{session_id}/report",
                "files": f"/api/session/{session_id}/files"
            }
        }), 201
        
    except Exception as e:
        logger.error(f"[!!] Error starting session scan: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/api/workflows", methods=["GET"])
def list_workflows():
    # List all available workflows
    return jsonify({
        "workflows": {
            "full-recon": {
                "endpoint": "/api/workflow/full-recon",
                "description": "Complete reconnaissance - 99% attack surface coverage",
                "stages": 6,
                "estimated_time": "15-20 minutes",
                "coverage": "Subdomains, live hosts, ports, services, tech stack, URLs, OSINT"
            },
            "vuln-assessment": {
                "endpoint": "/api/workflow/vuln-assessment", 
                "description": "Comprehensive vulnerability assessment - 99% vuln coverage",
                "stages": 6,
                "estimated_time": "20-25 minutes",
                "coverage": "KEV vulns, CVEs, web vulns, misconfigs, exposed files"
            },
            "complete-pentest": {
                "endpoint": "/api/workflow/complete-pentest",
                "description": "Full penetration testing - combines recon + vuln assessment",
                "stages": 8,
                "estimated_time": "30-40 minutes",
                "coverage": "Full attack surface + vulnerability coverage"
            },
            "bug-bounty-quick": {
                "endpoint": "/api/workflow/bug-bounty-quick",
                "description": "Quick bug bounty - fast high-impact findings",
                "stages": 5,
                "estimated_time": "10-12 minutes",
                "coverage": "Subdomains, KEV, critical vulns, hidden files, injection"
            },
            "api-pentest": {
                "endpoint": "/api/workflow/api-pentest",
                "description": "API security testing - REST/GraphQL comprehensive",
                "stages": 6,
                "estimated_time": "15-20 minutes",
                "coverage": "API endpoints, auth, injection, BOLA/IDOR, rate limiting"
            },
            "internal-network": {
                "endpoint": "/api/workflow/internal-network",
                "description": "Internal network assessment - AD/Windows focused",
                "stages": 6,
                "estimated_time": "20-30 minutes",
                "coverage": "Network discovery, ports, SMB, Windows enum, credentials"
            },
            "red-team-full": {
                "endpoint": "/api/workflow/red-team-full",
                "description": "Full red team simulation - complete offensive assessment",
                "stages": 7,
                "estimated_time": "45-60 minutes",
                "coverage": "OSINT, infrastructure, vulns, exploits, web, auth, post-exploit"
            }
        },
        "total_workflows": 7
    })

# File Operations API Endpoints

@app.route("/api/files/create", methods=["POST"])
def create_file():
    # Create a new file
    try:
        params = request.json
        filename = params.get("filename", "")
        content = params.get("content", "")
        binary = params.get("binary", False)

        if not filename:
            return jsonify({"error": "Filename is required"}), 400

        result = file_manager.create_file(filename, content, binary)
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error creating file: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/files/modify", methods=["POST"])
def modify_file():
    # Modify an existing file
    try:
        params = request.json
        filename = params.get("filename", "")
        content = params.get("content", "")
        append = params.get("append", False)

        if not filename:
            return jsonify({"error": "Filename is required"}), 400

        result = file_manager.modify_file(filename, content, append)
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error modifying file: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/files/delete", methods=["DELETE"])
def delete_file():
    # Delete a file or directory
    try:
        params = request.json
        filename = params.get("filename", "")

        if not filename:
            return jsonify({"error": "Filename is required"}), 400

        result = file_manager.delete_file(filename)
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error deleting file: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/files/list", methods=["GET"])
def list_files():
    # List files in a directory
    try:
        directory = request.args.get("directory", ".")
        result = file_manager.list_files(directory)
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error listing files: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

# Payload Generation Endpoint
@app.route("/api/payloads/generate", methods=["POST"])
def generate_payload():
    # Generate large payloads for testing
    try:
        params = request.json
        payload_type = params.get("type", "buffer")
        size = params.get("size", 1024)
        pattern = params.get("pattern", "A")
        filename = params.get("filename", f"payload_{int(time.time())}")

        if size > 100 * 1024 * 1024:  # 100MB limit
            return jsonify({"error": "Payload size too large (max 100MB)"}), 400

        if payload_type == "buffer":
            content = pattern * (size // len(pattern))
        elif payload_type == "cyclic":
            # Generate cyclic pattern
            alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
            content = ""
            for i in range(size):
                content += alphabet[i % len(alphabet)]
        elif payload_type == "random":
            import random
            import string
            content = ''.join(random.choices(string.ascii_letters + string.digits, k=size))
        else:
            return jsonify({"error": "Invalid payload type"}), 400

        result = file_manager.create_file(filename, content)
        result["payload_info"] = {
            "type": payload_type,
            "size": size,
            "pattern": pattern
        }

        logger.info(f"[>] Generated {payload_type} payload: {filename} ({size} bytes)")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error generating payload: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

# Cache Management Endpoint
@app.route("/api/cache/stats", methods=["GET"])
def cache_stats():
    # Get cache statistics
    return jsonify(cache.get_stats())

@app.route("/api/cache/clear", methods=["POST"])
def clear_cache():
    # Clear the cache
    cache.cache.clear()
    cache.stats = {"hits": 0, "misses": 0, "evictions": 0}
    logger.info("[~] Cache cleared")
    return jsonify({"success": True, "message": "Cache cleared"})

# Telemetry Endpoint
@app.route("/api/telemetry", methods=["GET"])
def get_telemetry():
    # Get system telemetry
    return jsonify(telemetry.get_stats())

# ============================================================================
# PROCESS MANAGEMENT API ENDPOINTS (v5.0 ENHANCEMENT)
# ============================================================================

@app.route("/api/processes/list", methods=["GET"])
def list_processes():
    # List all active processes
    try:
        processes = ProcessManager.list_active_processes()

        # Add calculated fields for each process
        for pid, info in processes.items():
            runtime = time.time() - info["start_time"]
            info["runtime_formatted"] = f"{runtime:.1f}s"

            if info["progress"] > 0:
                eta = (runtime / info["progress"]) * (1.0 - info["progress"])
                info["eta_formatted"] = f"{eta:.1f}s"
            else:
                info["eta_formatted"] = "Unknown"

        return jsonify({
            "success": True,
            "active_processes": processes,
            "total_count": len(processes)
        })
    except Exception as e:
        logger.error(f"[!!] Error listing processes: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/processes/status/<int:pid>", methods=["GET"])
def get_process_status(pid):
    # Get status of a specific process
    try:
        process_info = ProcessManager.get_process_status(pid)

        if process_info:
            # Add calculated fields
            runtime = time.time() - process_info["start_time"]
            process_info["runtime_formatted"] = f"{runtime:.1f}s"

            if process_info["progress"] > 0:
                eta = (runtime / process_info["progress"]) * (1.0 - process_info["progress"])
                process_info["eta_formatted"] = f"{eta:.1f}s"
            else:
                process_info["eta_formatted"] = "Unknown"

            return jsonify({
                "success": True,
                "process": process_info
            })
        else:
            return jsonify({
                "success": False,
                "error": f"Process {pid} not found"
            }), 404

    except Exception as e:
        logger.error(f"[!!] Error getting process status: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/processes/terminate/<int:pid>", methods=["POST"])
def terminate_process(pid):
    # Terminate a specific process
    try:
        success = ProcessManager.terminate_process(pid)

        if success:
            logger.info(f"🛑 Process {pid} terminated successfully")
            return jsonify({
                "success": True,
                "message": f"Process {pid} terminated successfully"
            })
        else:
            return jsonify({
                "success": False,
                "error": f"Failed to terminate process {pid} or process not found"
            }), 404

    except Exception as e:
        logger.error(f"[!!] Error terminating process {pid}: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/processes/pause/<int:pid>", methods=["POST"])
def pause_process(pid):
    # Pause a specific process
    try:
        success = ProcessManager.pause_process(pid)

        if success:
            logger.info(f"⏸️ Process {pid} paused successfully")
            return jsonify({
                "success": True,
                "message": f"Process {pid} paused successfully"
            })
        else:
            return jsonify({
                "success": False,
                "error": f"Failed to pause process {pid} or process not found"
            }), 404

    except Exception as e:
        logger.error(f"[!!] Error pausing process {pid}: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/processes/resume/<int:pid>", methods=["POST"])
def resume_process(pid):
    # Resume a paused process
    try:
        success = ProcessManager.resume_process(pid)

        if success:
            logger.info(f"▶️ Process {pid} resumed successfully")
            return jsonify({
                "success": True,
                "message": f"Process {pid} resumed successfully"
            })
        else:
            return jsonify({
                "success": False,
                "error": f"Failed to resume process {pid} or process not found"
            }), 404

    except Exception as e:
        logger.error(f"[!!] Error resuming process {pid}: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/processes/dashboard", methods=["GET"])
def process_dashboard():
    # Get enhanced process dashboard with visual status using ModernVisualEngine
    try:
        processes = ProcessManager.list_active_processes()
        current_time = time.time()

        # Create beautiful dashboard using ModernVisualEngine
        dashboard_visual = ModernVisualEngine.create_live_dashboard(processes)

        dashboard = {
            "timestamp": datetime.now().isoformat(),
            "total_processes": len(processes),
            "visual_dashboard": dashboard_visual,
            "processes": [],
            "system_load": {
                "cpu_percent": psutil.cpu_percent(interval=1),
                "memory_percent": psutil.virtual_memory().percent,
                "active_connections": len(psutil.net_connections())
            }
        }

        for pid, info in processes.items():
            runtime = current_time - info["start_time"]
            progress_fraction = info.get("progress", 0)

            # Create beautiful progress bar using ModernVisualEngine
            progress_bar = ModernVisualEngine.render_progress_bar(
                progress_fraction,
                width=25,
                style='cyber',
                eta=info.get("eta", 0)
            )

            process_status = {
                "pid": pid,
                "command": info["command"][:60] + "..." if len(info["command"]) > 60 else info["command"],
                "status": info["status"],
                "runtime": f"{runtime:.1f}s",
                "progress_percent": f"{progress_fraction * 100:.1f}%",
                "progress_bar": progress_bar,
                "eta": f"{info.get('eta', 0):.0f}s" if info.get('eta', 0) > 0 else "Calculating...",
                "bytes_processed": info.get("bytes_processed", 0),
                "last_output": info.get("last_output", "")[:100]
            }
            dashboard["processes"].append(process_status)

        return jsonify(dashboard)

    except Exception as e:
        logger.error(f"[!!] Error getting process dashboard: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/visual/vulnerability-card", methods=["POST"])
def create_vulnerability_card():
    # Create a beautiful vulnerability card using ModernVisualEngine
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "No data provided"}), 400

        # Create vulnerability card
        card = ModernVisualEngine.render_vulnerability_card(data)

        return jsonify({
            "success": True,
            "vulnerability_card": card,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error creating vulnerability card: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/visual/summary-report", methods=["POST"])
def create_summary_report():
    # Create a beautiful summary report using ModernVisualEngine
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "No data provided"}), 400

        # Create summary report
        visual_engine = ModernVisualEngine()
        report = visual_engine.create_summary_report(data)

        return jsonify({
            "success": True,
            "summary_report": report,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error creating summary report: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/visual/tool-output", methods=["POST"])
def format_tool_output():
    # Format tool output using ModernVisualEngine
    try:
        data = request.get_json()
        if not data or 'tool' not in data or 'output' not in data:
            return jsonify({"error": "Tool and output data required"}), 400

        tool = data['tool']
        output = data['output']
        success = data.get('success', True)

        # Format tool output
        formatted_output = ModernVisualEngine.format_tool_output(tool, output, success)

        return jsonify({
            "success": True,
            "formatted_output": formatted_output,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error formatting tool output: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

# ============================================================================
# INTELLIGENT DECISION ENGINE API ENDPOINTS
# ============================================================================

@app.route("/api/intelligence/analyze-target", methods=["POST"])
def analyze_target():
    # Analyze target and create comprehensive profile using Intelligent Decision Engine
    try:
        data = request.get_json()
        if not data or 'target' not in data:
            return jsonify({"error": "Target is required"}), 400

        target = data['target']
        logger.info(f"🧠 Analyzing target: {target}")

        # Use the decision engine to analyze the target
        profile = decision_engine.analyze_target(target)

        logger.info(f"[OK] Target analysis completed for {target}")
        logger.info(f"[#] Target type: {profile.target_type.value}, Risk level: {profile.risk_level}")

        return jsonify({
            "success": True,
            "target_profile": profile.to_dict(),
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error analyzing target: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/intelligence/select-tools", methods=["POST"])
def select_optimal_tools():
    # Select optimal tools based on target profile and objective
    try:
        data = request.get_json()
        if not data or 'target' not in data:
            return jsonify({"error": "Target is required"}), 400

        target = data['target']
        objective = data.get('objective', 'comprehensive')  # comprehensive, quick, stealth

        logger.info(f"[>] Selecting optimal tools for {target} with objective: {objective}")

        # Analyze target first
        profile = decision_engine.analyze_target(target)

        # Select optimal tools
        selected_tools = decision_engine.select_optimal_tools(profile, objective)

        logger.info(f"[OK] Selected {len(selected_tools)} tools for {target}")

        return jsonify({
            "success": True,
            "target": target,
            "objective": objective,
            "target_profile": profile.to_dict(),
            "selected_tools": selected_tools,
            "tool_count": len(selected_tools),
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error selecting tools: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/intelligence/optimize-parameters", methods=["POST"])
def optimize_tool_parameters():
    # Optimize tool parameters based on target profile and context
    try:
        data = request.get_json()
        if not data or 'target' not in data or 'tool' not in data:
            return jsonify({"error": "Target and tool are required"}), 400

        target = data['target']
        tool = data['tool']
        context = data.get('context', {})

        logger.info(f"⚙️  Optimizing parameters for {tool} against {target}")

        # Analyze target first
        profile = decision_engine.analyze_target(target)

        # Optimize parameters
        optimized_params = decision_engine.optimize_parameters(tool, profile, context)

        logger.info(f"[OK] Parameters optimized for {tool}")

        return jsonify({
            "success": True,
            "target": target,
            "tool": tool,
            "context": context,
            "target_profile": profile.to_dict(),
            "optimized_parameters": optimized_params,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error optimizing parameters: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/intelligence/create-attack-chain", methods=["POST"])
def create_attack_chain():
    # Create an intelligent attack chain based on target profile
    try:
        data = request.get_json()
        if not data or 'target' not in data:
            return jsonify({"error": "Target is required"}), 400

        target = data['target']
        objective = data.get('objective', 'comprehensive')

        logger.info(f"⚔️  Creating attack chain for {target} with objective: {objective}")

        # Analyze target first
        profile = decision_engine.analyze_target(target)

        # Create attack chain
        attack_chain = decision_engine.create_attack_chain(profile, objective)

        logger.info(f"[OK] Attack chain created with {len(attack_chain.steps)} steps")
        logger.info(f"[#] Success probability: {attack_chain.success_probability:.2f}, Estimated time: {attack_chain.estimated_time}s")

        return jsonify({
            "success": True,
            "target": target,
            "objective": objective,
            "target_profile": profile.to_dict(),
            "attack_chain": attack_chain.to_dict(),
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error creating attack chain: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/intelligence/smart-scan", methods=["POST"])
def intelligent_smart_scan():
    # Execute an intelligent scan using AI-driven tool selection and parameter optimization with parallel execution
    try:
        data = request.get_json()
        if not data or 'target' not in data:
            return jsonify({"error": "Target is required"}), 400

        target = data['target']
        objective = data.get('objective', 'comprehensive')
        max_tools = data.get('max_tools', 5)

        logger.info(f"[>] Starting intelligent smart scan for {target}")

        # Analyze target
        profile = decision_engine.analyze_target(target)

        # Select optimal tools
        selected_tools = decision_engine.select_optimal_tools(profile, objective)[:max_tools]

        # Execute tools in parallel with real tool execution
        scan_results = {
            "target": target,
            "target_profile": profile.to_dict(),
            "tools_executed": [],
            "total_vulnerabilities": 0,
            "execution_summary": {},
            "combined_output": ""
        }

        def execute_single_tool(tool_name, target, profile):
            # Execute a single tool and return results
            try:
                logger.info(f"[+] Executing {tool_name} with optimized parameters")

                # Get optimized parameters for this tool
                optimized_params = decision_engine.optimize_parameters(tool_name, profile)

                # Map tool names to their actual execution functions
                tool_execution_map = {
                    'nmap': lambda: execute_nmap_scan(target, optimized_params),
                    'gobuster': lambda: execute_gobuster_scan(target, optimized_params),
                    'nuclei': lambda: execute_nuclei_scan(target, optimized_params),
                    'nikto': lambda: execute_nikto_scan(target, optimized_params),
                    'sqlmap': lambda: execute_sqlmap_scan(target, optimized_params),
                    'ffuf': lambda: execute_ffuf_scan(target, optimized_params),
                    'feroxbuster': lambda: execute_feroxbuster_scan(target, optimized_params),
                    'katana': lambda: execute_katana_scan(target, optimized_params),
                    'httpx': lambda: execute_httpx_scan(target, optimized_params),
                    'wpscan': lambda: execute_wpscan_scan(target, optimized_params),
                    'dirsearch': lambda: execute_dirsearch_scan(target, optimized_params),
                    'arjun': lambda: execute_arjun_scan(target, optimized_params),
                    'paramspider': lambda: execute_paramspider_scan(target, optimized_params),
                    'dalfox': lambda: execute_dalfox_scan(target, optimized_params),
                    'amass': lambda: execute_amass_scan(target, optimized_params),
                    'subfinder': lambda: execute_subfinder_scan(target, optimized_params)
                }

                # Execute the tool if we have a mapping for it
                if tool_name in tool_execution_map:
                    result = tool_execution_map[tool_name]()

                    # Extract vulnerability count from result
                    vuln_count = 0
                    if result.get('success') and result.get('stdout'):
                        # Simple vulnerability detection based on common patterns
                        output = result.get('stdout', '')
                        vuln_indicators = ['CRITICAL', 'HIGH', 'MEDIUM', 'VULNERABILITY', 'EXPLOIT', 'SQL injection', 'XSS', 'CSRF']
                        vuln_count = sum(1 for indicator in vuln_indicators if indicator.lower() in output.lower())

                    return {
                        "tool": tool_name,
                        "parameters": optimized_params,
                        "status": "success" if result.get('success') else "failed",
                        "timestamp": datetime.now().isoformat(),
                        "execution_time": result.get('execution_time', 0),
                        "stdout": result.get('stdout', ''),
                        "stderr": result.get('stderr', ''),
                        "vulnerabilities_found": vuln_count,
                        "command": result.get('command', ''),
                        "success": result.get('success', False)
                    }
                else:
                    logger.warning(f"[WARN] No execution mapping found for tool: {tool_name}")
                    return {
                        "tool": tool_name,
                        "parameters": optimized_params,
                        "status": "skipped",
                        "timestamp": datetime.now().isoformat(),
                        "error": f"Tool {tool_name} not implemented in execution map",
                        "success": False
                    }

            except Exception as e:
                logger.error(f"[X] Error executing {tool_name}: {str(e)}")
                return {
                    "tool": tool_name,
                    "status": "failed",
                    "timestamp": datetime.now().isoformat(),
                    "error": str(e),
                    "success": False
                }

        # Execute tools in parallel using ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=min(len(selected_tools), 5)) as executor:
            # Submit all tool executions
            future_to_tool = {
                executor.submit(execute_single_tool, tool, target, profile): tool
                for tool in selected_tools
            }

            # Collect results as they complete
            for future in future_to_tool:
                tool_result = future.result()
                scan_results["tools_executed"].append(tool_result)

                # Accumulate vulnerability count
                if tool_result.get("vulnerabilities_found"):
                    scan_results["total_vulnerabilities"] += tool_result["vulnerabilities_found"]

                # Combine outputs
                if tool_result.get("stdout"):
                    scan_results["combined_output"] += f"\n=== {tool_result['tool'].upper()} OUTPUT ===\n"
                    scan_results["combined_output"] += tool_result["stdout"]
                    scan_results["combined_output"] += "\n" + "="*50 + "\n"

        # Create execution summary
        successful_tools = [t for t in scan_results["tools_executed"] if t.get("success")]
        failed_tools = [t for t in scan_results["tools_executed"] if not t.get("success")]

        scan_results["execution_summary"] = {
            "total_tools": len(selected_tools),
            "successful_tools": len(successful_tools),
            "failed_tools": len(failed_tools),
            "success_rate": len(successful_tools) / len(selected_tools) * 100 if selected_tools else 0,
            "total_execution_time": sum(t.get("execution_time", 0) for t in scan_results["tools_executed"]),
            "tools_used": [t["tool"] for t in successful_tools]
        }

        logger.info(f"[OK] Intelligent smart scan completed for {target}")
        logger.info(f"[#] Results: {len(successful_tools)}/{len(selected_tools)} tools successful, {scan_results['total_vulnerabilities']} vulnerabilities found")

        return jsonify({
            "success": True,
            "scan_results": scan_results,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error in intelligent smart scan: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}", "success": False}), 500

# Helper functions for intelligent smart scan tool execution
def execute_nmap_scan(target, params):
    # Execute nmap scan with optimized parameters
    try:
        scan_type = params.get('scan_type', '-sV')
        ports = params.get('ports', '')
        additional_args = params.get('additional_args', '')

        # Build nmap command
        cmd_parts = ['nmap', scan_type]
        if ports:
            cmd_parts.extend(['-p', ports])
        if additional_args:
            cmd_parts.extend(additional_args.split())
        cmd_parts.append(target)

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_gobuster_scan(target, params):
    # Execute gobuster scan with optimized parameters
    try:
        mode = params.get('mode', 'dir')
        wordlist = params.get('wordlist', '/usr/share/wordlists/dirb/common.txt')
        additional_args = params.get('additional_args', '')

        cmd_parts = ['gobuster', mode, '-u', target, '-w', wordlist]
        if additional_args:
            cmd_parts.extend(additional_args.split())

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_nuclei_scan(target, params):
    # Execute nuclei scan with optimized parameters
    try:
        severity = params.get('severity', '')
        tags = params.get('tags', '')
        additional_args = params.get('additional_args', '')

        cmd_parts = ['nuclei', '-u', target]
        if severity:
            cmd_parts.extend(['-severity', severity])
        if tags:
            cmd_parts.extend(['-tags', tags])
        if additional_args:
            cmd_parts.extend(additional_args.split())

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_nikto_scan(target, params):
    # Execute nikto scan with optimized parameters
    try:
        additional_args = params.get('additional_args', '')
        cmd_parts = ['nikto', '-h', target]
        if additional_args:
            cmd_parts.extend(additional_args.split())

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_sqlmap_scan(target, params):
    # Execute sqlmap scan with optimized parameters
    try:
        additional_args = params.get('additional_args', '--batch --random-agent')
        cmd_parts = ['sqlmap', '-u', target]
        if additional_args:
            cmd_parts.extend(additional_args.split())

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_ffuf_scan(target, params):
    # Execute ffuf scan with optimized parameters
    try:
        wordlist = params.get('wordlist', '/usr/share/wordlists/dirb/common.txt')
        additional_args = params.get('additional_args', '')

        # Ensure target has FUZZ placeholder
        if 'FUZZ' not in target:
            target = target.rstrip('/') + '/FUZZ'

        cmd_parts = ['ffuf', '-u', target, '-w', wordlist]
        if additional_args:
            cmd_parts.extend(additional_args.split())

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_feroxbuster_scan(target, params):
    # Execute feroxbuster scan with optimized parameters
    try:
        wordlist = params.get('wordlist', '/usr/share/wordlists/dirb/common.txt')
        additional_args = params.get('additional_args', '')

        cmd_parts = ['feroxbuster', '-u', target, '-w', wordlist]
        if additional_args:
            cmd_parts.extend(additional_args.split())

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_katana_scan(target, params):
    # Execute katana scan with optimized parameters
    try:
        additional_args = params.get('additional_args', '')
        cmd_parts = ['katana', '-u', target]
        if additional_args:
            cmd_parts.extend(additional_args.split())

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_httpx_scan(target, params):
    # Execute httpx scan with optimized parameters
    try:
        additional_args = params.get('additional_args', '-tech-detect -status-code')
        # Use shell command with pipe for httpx
        cmd = f"echo {target} | httpx {additional_args}"

        return execute_command(cmd)
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_wpscan_scan(target, params):
    # Execute wpscan scan with optimized parameters
    try:
        additional_args = params.get('additional_args', '--enumerate p,t,u')
        cmd_parts = ['wpscan', '--url', target]
        if additional_args:
            cmd_parts.extend(additional_args.split())

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_dirsearch_scan(target, params):
    # Execute dirsearch scan with optimized parameters
    try:
        additional_args = params.get('additional_args', '')
        cmd_parts = ['dirsearch', '-u', target]
        if additional_args:
            cmd_parts.extend(additional_args.split())

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_arjun_scan(target, params):
    # Execute arjun scan with optimized parameters
    try:
        additional_args = params.get('additional_args', '')
        cmd_parts = ['arjun', '-u', target]
        if additional_args:
            cmd_parts.extend(additional_args.split())

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_paramspider_scan(target, params):
    # Execute paramspider scan with optimized parameters
    try:
        additional_args = params.get('additional_args', '')
        cmd_parts = ['paramspider', '-d', target]
        if additional_args:
            cmd_parts.extend(additional_args.split())

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_dalfox_scan(target, params):
    # Execute dalfox scan with optimized parameters
    try:
        additional_args = params.get('additional_args', '')
        cmd_parts = ['dalfox', 'url', target]
        if additional_args:
            cmd_parts.extend(additional_args.split())

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_amass_scan(target, params):
    # Execute amass scan with optimized parameters
    try:
        additional_args = params.get('additional_args', '')
        cmd_parts = ['amass', 'enum', '-d', target]
        if additional_args:
            cmd_parts.extend(additional_args.split())

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

def execute_subfinder_scan(target, params):
    # Execute subfinder scan with optimized parameters
    try:
        additional_args = params.get('additional_args', '')
        cmd_parts = ['subfinder', '-d', target]
        if additional_args:
            cmd_parts.extend(additional_args.split())

        return execute_command(' '.join(cmd_parts))
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.route("/api/intelligence/technology-detection", methods=["POST"])
def detect_technologies():
    # Detect technologies and create technology-specific testing recommendations
    try:
        data = request.get_json()
        if not data or 'target' not in data:
            return jsonify({"error": "Target is required"}), 400

        target = data['target']

        logger.info(f"[?] Detecting technologies for {target}")

        # Analyze target
        profile = decision_engine.analyze_target(target)

        # Get technology-specific recommendations
        tech_recommendations = {}
        for tech in profile.technologies:
            if tech == TechnologyStack.WORDPRESS:
                tech_recommendations["WordPress"] = {
                    "tools": ["wpscan", "nuclei"],
                    "focus_areas": ["plugin vulnerabilities", "theme issues", "user enumeration"],
                    "priority": "high"
                }
            elif tech == TechnologyStack.PHP:
                tech_recommendations["PHP"] = {
                    "tools": ["nikto", "sqlmap", "ffuf"],
                    "focus_areas": ["code injection", "file inclusion", "SQL injection"],
                    "priority": "high"
                }
            elif tech == TechnologyStack.NODEJS:
                tech_recommendations["Node.js"] = {
                    "tools": ["nuclei", "ffuf"],
                    "focus_areas": ["prototype pollution", "dependency vulnerabilities"],
                    "priority": "medium"
                }

        logger.info(f"[OK] Technology detection completed for {target}")

        return jsonify({
            "success": True,
            "target": target,
            "detected_technologies": [tech.value for tech in profile.technologies],
            "cms_type": profile.cms_type,
            "technology_recommendations": tech_recommendations,
            "target_profile": profile.to_dict(),
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error in technology detection: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

# ============================================================================
# BUG BOUNTY HUNTING WORKFLOW API ENDPOINTS
# ============================================================================

@app.route("/api/bugbounty/reconnaissance-workflow", methods=["POST"])
def create_reconnaissance_workflow():
    # Create comprehensive reconnaissance workflow for bug bounty hunting
    try:
        data = request.get_json()
        if not data or 'domain' not in data:
            return jsonify({"error": "Domain is required"}), 400

        domain = data['domain']
        scope = data.get('scope', [])
        out_of_scope = data.get('out_of_scope', [])
        program_type = data.get('program_type', 'web')

        logger.info(f"[>] Creating reconnaissance workflow for {domain}")

        # Create bug bounty target
        target = BugBountyTarget(
            domain=domain,
            scope=scope,
            out_of_scope=out_of_scope,
            program_type=program_type
        )

        # Generate reconnaissance workflow
        workflow = bugbounty_manager.create_reconnaissance_workflow(target)

        logger.info(f"[OK] Reconnaissance workflow created for {domain}")

        return jsonify({
            "success": True,
            "workflow": workflow,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error creating reconnaissance workflow: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/bugbounty/vulnerability-hunting-workflow", methods=["POST"])
def create_vulnerability_hunting_workflow():
    # Create vulnerability hunting workflow prioritized by impact
    try:
        data = request.get_json()
        if not data or 'domain' not in data:
            return jsonify({"error": "Domain is required"}), 400

        domain = data['domain']
        priority_vulns = data.get('priority_vulns', ["rce", "sqli", "xss", "idor", "ssrf"])
        bounty_range = data.get('bounty_range', 'unknown')

        logger.info(f"[>] Creating vulnerability hunting workflow for {domain}")

        # Create bug bounty target
        target = BugBountyTarget(
            domain=domain,
            priority_vulns=priority_vulns,
            bounty_range=bounty_range
        )

        # Generate vulnerability hunting workflow
        workflow = bugbounty_manager.create_vulnerability_hunting_workflow(target)

        logger.info(f"[OK] Vulnerability hunting workflow created for {domain}")

        return jsonify({
            "success": True,
            "workflow": workflow,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error creating vulnerability hunting workflow: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/bugbounty/business-logic-workflow", methods=["POST"])
def create_business_logic_workflow():
    # Create business logic testing workflow
    try:
        data = request.get_json()
        if not data or 'domain' not in data:
            return jsonify({"error": "Domain is required"}), 400

        domain = data['domain']
        program_type = data.get('program_type', 'web')

        logger.info(f"[>] Creating business logic testing workflow for {domain}")

        # Create bug bounty target
        target = BugBountyTarget(domain=domain, program_type=program_type)

        # Generate business logic testing workflow
        workflow = bugbounty_manager.create_business_logic_testing_workflow(target)

        logger.info(f"[OK] Business logic testing workflow created for {domain}")

        return jsonify({
            "success": True,
            "workflow": workflow,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error creating business logic workflow: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/bugbounty/osint-workflow", methods=["POST"])
def create_osint_workflow():
    # Create OSINT gathering workflow
    try:
        data = request.get_json()
        if not data or 'domain' not in data:
            return jsonify({"error": "Domain is required"}), 400

        domain = data['domain']

        logger.info(f"[>] Creating OSINT workflow for {domain}")

        # Create bug bounty target
        target = BugBountyTarget(domain=domain)

        # Generate OSINT workflow
        workflow = bugbounty_manager.create_osint_workflow(target)

        logger.info(f"[OK] OSINT workflow created for {domain}")

        return jsonify({
            "success": True,
            "workflow": workflow,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error creating OSINT workflow: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/bugbounty/file-upload-testing", methods=["POST"])
def create_file_upload_testing():
    # Create file upload vulnerability testing workflow
    try:
        data = request.get_json()
        if not data or 'target_url' not in data:
            return jsonify({"error": "Target URL is required"}), 400

        target_url = data['target_url']

        logger.info(f"[>] Creating file upload testing workflow for {target_url}")

        # Generate file upload testing workflow
        workflow = fileupload_framework.create_upload_testing_workflow(target_url)

        # Generate test files
        test_files = fileupload_framework.generate_test_files()
        workflow["test_files"] = test_files

        logger.info(f"[OK] File upload testing workflow created for {target_url}")

        return jsonify({
            "success": True,
            "workflow": workflow,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error creating file upload testing workflow: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/bugbounty/comprehensive-assessment", methods=["POST"])
def create_comprehensive_bugbounty_assessment():
    # Create comprehensive bug bounty assessment combining all workflows
    try:
        data = request.get_json()
        if not data or 'domain' not in data:
            return jsonify({"error": "Domain is required"}), 400

        domain = data['domain']
        scope = data.get('scope', [])
        priority_vulns = data.get('priority_vulns', ["rce", "sqli", "xss", "idor", "ssrf"])
        include_osint = data.get('include_osint', True)
        include_business_logic = data.get('include_business_logic', True)

        logger.info(f"[>] Creating comprehensive bug bounty assessment for {domain}")

        # Create bug bounty target
        target = BugBountyTarget(
            domain=domain,
            scope=scope,
            priority_vulns=priority_vulns
        )

        # Generate all workflows
        assessment = {
            "target": domain,
            "reconnaissance": bugbounty_manager.create_reconnaissance_workflow(target),
            "vulnerability_hunting": bugbounty_manager.create_vulnerability_hunting_workflow(target)
        }

        if include_osint:
            assessment["osint"] = bugbounty_manager.create_osint_workflow(target)

        if include_business_logic:
            assessment["business_logic"] = bugbounty_manager.create_business_logic_testing_workflow(target)

        # Calculate total estimates
        total_time = sum(workflow.get("estimated_time", 0) for workflow in assessment.values() if isinstance(workflow, dict))
        total_tools = sum(workflow.get("tools_count", 0) for workflow in assessment.values() if isinstance(workflow, dict))

        assessment["summary"] = {
            "total_estimated_time": total_time,
            "total_tools": total_tools,
            "workflow_count": len([k for k in assessment.keys() if k != "target"]),
            "priority_score": assessment["vulnerability_hunting"].get("priority_score", 0)
        }

        logger.info(f"[OK] Comprehensive bug bounty assessment created for {domain}")

        return jsonify({
            "success": True,
            "assessment": assessment,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error creating comprehensive assessment: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

# ============================================================================
# SECURITY TOOLS API ENDPOINTS
# ============================================================================

@app.route("/api/tools/nmap", methods=["POST"])
def nmap():
    # Execute nmap scan with enhanced logging, caching, and intelligent error handling
    try:
        params = request.json
        target = params.get("target", "")
        scan_type = params.get("scan_type", "-sCV")
        ports = params.get("ports", "")
        additional_args = params.get("additional_args", "-T4 -Pn")
        use_recovery = params.get("use_recovery", True)

        if not target:
            logger.warning("[>] Nmap called without target parameter")
            return jsonify({
                "error": "Target parameter is required"
            }), 400

        command = f"nmap {scan_type}"

        if ports:
            command += f" -p {ports}"

        if additional_args:
            command += f" {additional_args}"

        command += f" {target}"

        logger.info(f"[?] Starting Nmap scan: {target}")

        # Use intelligent error handling if enabled
        if use_recovery:
            tool_params = {
                "target": target,
                "scan_type": scan_type,
                "ports": ports,
                "additional_args": additional_args
            }
            result = execute_command_with_recovery("nmap", command, tool_params)
        else:
            result = execute_command(command)

        logger.info(f"[#] Nmap scan completed for {target}")
        return jsonify(result)

    except Exception as e:
        logger.error(f"[!!] Error in nmap endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/gobuster", methods=["POST"])
def gobuster():
    # Execute gobuster with enhanced logging and intelligent error handling
    try:
        params = request.json
        url = params.get("url", "")
        mode = params.get("mode", "dir")
        wordlist = params.get("wordlist", "/usr/share/wordlists/dirb/common.txt")
        additional_args = params.get("additional_args", "")
        use_recovery = params.get("use_recovery", True)

        if not url:
            logger.warning("[W] Gobuster called without URL parameter")
            return jsonify({
                "error": "URL parameter is required"
            }), 400

        # Validate mode
        if mode not in ["dir", "dns", "fuzz", "vhost"]:
            logger.warning(f"[X] Invalid gobuster mode: {mode}")
            return jsonify({
                "error": f"Invalid mode: {mode}. Must be one of: dir, dns, fuzz, vhost"
            }), 400

        command = f"gobuster {mode} -u {url} -w {wordlist}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"📁 Starting Gobuster {mode} scan: {url}")

        # Use intelligent error handling if enabled
        if use_recovery:
            tool_params = {
                "target": url,
                "mode": mode,
                "wordlist": wordlist,
                "additional_args": additional_args
            }
            result = execute_command_with_recovery("gobuster", command, tool_params)
        else:
            result = execute_command(command)

        logger.info(f"[#] Gobuster scan completed for {url}")
        return jsonify(result)

    except Exception as e:
        logger.error(f"[!!] Error in gobuster endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/nuclei", methods=["POST"])
def nuclei():
    # Execute Nuclei vulnerability scanner with enhanced logging and intelligent error handling
    try:
        params = request.json
        target = params.get("target", "")
        severity = params.get("severity", "")
        tags = params.get("tags", "")
        template = params.get("template", "")
        additional_args = params.get("additional_args", "")
        use_recovery = params.get("use_recovery", True)

        if not target:
            logger.warning("[>] Nuclei called without target parameter")
            return jsonify({
                "error": "Target parameter is required"
            }), 400

        command = f"nuclei -u {target}"

        if severity:
            command += f" -severity {severity}"

        if tags:
            command += f" -tags {tags}"

        if template:
            command += f" -t {template}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🔬 Starting Nuclei vulnerability scan: {target}")

        # Use intelligent error handling if enabled
        if use_recovery:
            tool_params = {
                "target": target,
                "severity": severity,
                "tags": tags,
                "template": template,
                "additional_args": additional_args
            }
            result = execute_command_with_recovery("nuclei", command, tool_params)
        else:
            result = execute_command(command)

        logger.info(f"[#] Nuclei scan completed for {target}")
        return jsonify(result)

    except Exception as e:
        logger.error(f"[!!] Error in nuclei endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

# ============================================================================
# CLOUD SECURITY TOOLS
# ============================================================================

@app.route("/api/tools/prowler", methods=["POST"])
def prowler():
    # Execute Prowler for AWS security assessment
    try:
        params = request.json
        provider = params.get("provider", "aws")
        profile = params.get("profile", "default")
        region = params.get("region", "")
        checks = params.get("checks", "")
        output_dir = params.get("output_dir", "/tmp/prowler_output")
        output_format = params.get("output_format", "json")
        additional_args = params.get("additional_args", "")

        # Ensure output directory exists
        Path(output_dir).mkdir(parents=True, exist_ok=True)

        command = f"prowler {provider}"

        if profile:
            command += f" --profile {profile}"

        if region:
            command += f" --region {region}"

        if checks:
            command += f" --checks {checks}"

        command += f" --output-directory {output_dir}"
        command += f" --output-format {output_format}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"☁️  Starting Prowler {provider} security assessment")
        result = execute_command(command)
        result["output_directory"] = output_dir
        logger.info(f"[#] Prowler assessment completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in prowler endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/trivy", methods=["POST"])
def trivy():
    # Execute Trivy for container/filesystem vulnerability scanning
    try:
        params = request.json
        scan_type = params.get("scan_type", "image")  # image, fs, repo
        target = params.get("target", "")
        output_format = params.get("output_format", "json")
        severity = params.get("severity", "")
        output_file = params.get("output_file", "")
        additional_args = params.get("additional_args", "")

        if not target:
            logger.warning("[>] Trivy called without target parameter")
            return jsonify({
                "error": "Target parameter is required"
            }), 400

        command = f"trivy {scan_type} {target}"

        if output_format:
            command += f" --format {output_format}"

        if severity:
            command += f" --severity {severity}"

        if output_file:
            command += f" --output {output_file}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting Trivy {scan_type} scan: {target}")
        result = execute_command(command)
        if output_file:
            result["output_file"] = output_file
        logger.info(f"[#] Trivy scan completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in trivy endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

# ============================================================================
# ENHANCED CLOUD AND CONTAINER SECURITY TOOLS (v6.0)
# ============================================================================

@app.route("/api/tools/scout-suite", methods=["POST"])
def scout_suite():
    # Execute Scout Suite for multi-cloud security assessment
    try:
        params = request.json
        provider = params.get("provider", "aws")  # aws, azure, gcp, aliyun, oci
        profile = params.get("profile", "default")
        report_dir = params.get("report_dir", "/tmp/scout-suite")
        services = params.get("services", "")
        exceptions = params.get("exceptions", "")
        additional_args = params.get("additional_args", "")

        # Ensure report directory exists
        Path(report_dir).mkdir(parents=True, exist_ok=True)

        command = f"scout {provider}"

        if profile and provider == "aws":
            command += f" --profile {profile}"

        if services:
            command += f" --services {services}"

        if exceptions:
            command += f" --exceptions {exceptions}"

        command += f" --report-dir {report_dir}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"☁️  Starting Scout Suite {provider} assessment")
        result = execute_command(command)
        result["report_directory"] = report_dir
        logger.info(f"[#] Scout Suite assessment completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in scout-suite endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/cloudmapper", methods=["POST"])
def cloudmapper():
    # Execute CloudMapper for AWS network visualization and security analysis
    try:
        params = request.json
        action = params.get("action", "collect")  # collect, prepare, webserver, find_admins, etc.
        account = params.get("account", "")
        config = params.get("config", "config.json")
        additional_args = params.get("additional_args", "")

        if not account and action != "webserver":
            logger.warning("☁️  CloudMapper called without account parameter")
            return jsonify({"error": "Account parameter is required for most actions"}), 400

        command = f"cloudmapper {action}"

        if account:
            command += f" --account {account}"

        if config:
            command += f" --config {config}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"☁️  Starting CloudMapper {action}")
        result = execute_command(command)
        logger.info(f"[#] CloudMapper {action} completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in cloudmapper endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/pacu", methods=["POST"])
def pacu():
    # Execute Pacu for AWS exploitation framework
    try:
        params = request.json
        session_name = params.get("session_name", "VectorAI_session")
        modules = params.get("modules", "")
        data_services = params.get("data_services", "")
        regions = params.get("regions", "")
        additional_args = params.get("additional_args", "")

        # Create Pacu command sequence
        commands = []
        commands.append(f"set_session {session_name}")

        if data_services:
            commands.append(f"data {data_services}")

        if regions:
            commands.append(f"set_regions {regions}")

        if modules:
            for module in modules.split(","):
                commands.append(f"run {module.strip()}")

        commands.append("exit")

        # Create command file
        command_file = "/tmp/pacu_commands.txt"
        with open(command_file, "w") as f:
            f.write("\n".join(commands))

        command = f"pacu < {command_file}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"☁️  Starting Pacu AWS exploitation")
        result = execute_command(command)

        # Cleanup
        try:
            os.remove(command_file)
        except:
            pass

        logger.info(f"[#] Pacu exploitation completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in pacu endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/kube-hunter", methods=["POST"])
def kube_hunter():
    # Execute kube-hunter for Kubernetes penetration testing
    try:
        params = request.json
        target = params.get("target", "")
        remote = params.get("remote", "")
        cidr = params.get("cidr", "")
        interface = params.get("interface", "")
        active = params.get("active", False)
        report = params.get("report", "json")
        additional_args = params.get("additional_args", "")

        command = "kube-hunter"

        if target:
            command += f" --remote {target}"
        elif remote:
            command += f" --remote {remote}"
        elif cidr:
            command += f" --cidr {cidr}"
        elif interface:
            command += f" --interface {interface}"
        else:
            # Default to pod scanning
            command += " --pod"

        if active:
            command += " --active"

        if report:
            command += f" --report {report}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"☁️  Starting kube-hunter Kubernetes scan")
        result = execute_command(command)
        logger.info(f"[#] kube-hunter scan completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in kube-hunter endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/kube-bench", methods=["POST"])
def kube_bench():
    # Execute kube-bench for CIS Kubernetes benchmark checks
    try:
        params = request.json
        targets = params.get("targets", "")  # master, node, etcd, policies
        version = params.get("version", "")
        config_dir = params.get("config_dir", "")
        output_format = params.get("output_format", "json")
        additional_args = params.get("additional_args", "")

        command = "kube-bench"

        if targets:
            command += f" --targets {targets}"

        if version:
            command += f" --version {version}"

        if config_dir:
            command += f" --config-dir {config_dir}"

        if output_format:
            command += f" --outputfile /tmp/kube-bench-results.{output_format} --json"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"☁️  Starting kube-bench CIS benchmark")
        result = execute_command(command)
        logger.info(f"[#] kube-bench benchmark completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in kube-bench endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/docker-bench-security", methods=["POST"])
def docker_bench_security():
    # Execute Docker Bench for Security for Docker security assessment
    try:
        params = request.json
        checks = params.get("checks", "")  # Specific checks to run
        exclude = params.get("exclude", "")  # Checks to exclude
        output_file = params.get("output_file", "/tmp/docker-bench-results.json")
        additional_args = params.get("additional_args", "")

        command = "docker-bench-security"

        if checks:
            command += f" -c {checks}"

        if exclude:
            command += f" -e {exclude}"

        if output_file:
            command += f" -l {output_file}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🐳 Starting Docker Bench Security assessment")
        result = execute_command(command)
        result["output_file"] = output_file
        logger.info(f"[#] Docker Bench Security completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in docker-bench-security endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/clair", methods=["POST"])
def clair():
    # Execute Clair for container vulnerability analysis
    try:
        params = request.json
        image = params.get("image", "")
        config = params.get("config", "/etc/clair/config.yaml")
        output_format = params.get("output_format", "json")
        additional_args = params.get("additional_args", "")

        if not image:
            logger.warning("🐳 Clair called without image parameter")
            return jsonify({"error": "Image parameter is required"}), 400

        # Use clairctl for scanning
        command = f"clairctl analyze {image}"

        if config:
            command += f" --config {config}"

        if output_format:
            command += f" --format {output_format}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🐳 Starting Clair vulnerability scan: {image}")
        result = execute_command(command)
        logger.info(f"[#] Clair scan completed for {image}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in clair endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/falco", methods=["POST"])
def falco():
    # Execute Falco for runtime security monitoring
    try:
        params = request.json
        config_file = params.get("config_file", "/etc/falco/falco.yaml")
        rules_file = params.get("rules_file", "")
        output_format = params.get("output_format", "json")
        duration = params.get("duration", 60)  # seconds
        additional_args = params.get("additional_args", "")

        command = f"timeout {duration} falco"

        if config_file:
            command += f" --config {config_file}"

        if rules_file:
            command += f" --rules {rules_file}"

        if output_format == "json":
            command += " --json"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🛡️  Starting Falco runtime monitoring for {duration}s")
        result = execute_command(command)
        logger.info(f"[#] Falco monitoring completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in falco endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/checkov", methods=["POST"])
def checkov():
    # Execute Checkov for infrastructure as code security scanning
    try:
        params = request.json
        directory = params.get("directory", ".")
        framework = params.get("framework", "")  # terraform, cloudformation, kubernetes, etc.
        check = params.get("check", "")
        skip_check = params.get("skip_check", "")
        output_format = params.get("output_format", "json")
        additional_args = params.get("additional_args", "")

        command = f"checkov -d {directory}"

        if framework:
            command += f" --framework {framework}"

        if check:
            command += f" --check {check}"

        if skip_check:
            command += f" --skip-check {skip_check}"

        if output_format:
            command += f" --output {output_format}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting Checkov IaC scan: {directory}")
        result = execute_command(command)
        logger.info(f"[#] Checkov scan completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in checkov endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/terrascan", methods=["POST"])
def terrascan():
    # Execute Terrascan for infrastructure as code security scanning
    try:
        params = request.json
        scan_type = params.get("scan_type", "all")  # all, terraform, k8s, etc.
        iac_dir = params.get("iac_dir", ".")
        policy_type = params.get("policy_type", "")
        output_format = params.get("output_format", "json")
        severity = params.get("severity", "")
        additional_args = params.get("additional_args", "")

        command = f"terrascan scan -t {scan_type} -d {iac_dir}"

        if policy_type:
            command += f" -p {policy_type}"

        if output_format:
            command += f" -o {output_format}"

        if severity:
            command += f" --severity {severity}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting Terrascan IaC scan: {iac_dir}")
        result = execute_command(command)
        logger.info(f"[#] Terrascan scan completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in terrascan endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/dirb", methods=["POST"])
def dirb():
    # Execute dirb with enhanced logging
    try:
        params = request.json
        url = params.get("url", "")
        wordlist = params.get("wordlist", "/usr/share/wordlists/dirb/common.txt")
        additional_args = params.get("additional_args", "")

        if not url:
            logger.warning("[W] Dirb called without URL parameter")
            return jsonify({
                "error": "URL parameter is required"
            }), 400

        command = f"dirb {url} {wordlist}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"📁 Starting Dirb scan: {url}")
        result = execute_command(command)
        logger.info(f"[#] Dirb scan completed for {url}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in dirb endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/nikto", methods=["POST"])
def nikto():
    # Execute nikto with enhanced logging
    try:
        params = request.json
        target = params.get("target", "")
        additional_args = params.get("additional_args", "")

        if not target:
            logger.warning("[>] Nikto called without target parameter")
            return jsonify({
                "error": "Target parameter is required"
            }), 400

        command = f"nikto -h {target}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🔬 Starting Nikto scan: {target}")
        result = execute_command(command)
        logger.info(f"[#] Nikto scan completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in nikto endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/sqlmap", methods=["POST"])
def sqlmap():
    # Execute sqlmap with enhanced logging
    try:
        params = request.json
        url = params.get("url", "")
        data = params.get("data", "")
        additional_args = params.get("additional_args", "")

        if not url:
            logger.warning("[>] SQLMap called without URL parameter")
            return jsonify({
                "error": "URL parameter is required"
            }), 400

        command = f"sqlmap -u {url} --batch"

        if data:
            command += f" --data=\"{data}\""

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"💉 Starting SQLMap scan: {url}")
        result = execute_command(command)
        logger.info(f"[#] SQLMap scan completed for {url}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in sqlmap endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/metasploit", methods=["POST"])
def metasploit():
    # Execute metasploit module with enhanced logging
    try:
        params = request.json
        module = params.get("module", "")
        options = params.get("options", {})

        if not module:
            logger.warning("[>] Metasploit called without module parameter")
            return jsonify({
                "error": "Module parameter is required"
            }), 400

        # Create an MSF resource script
        resource_content = f"use {module}\n"
        for key, value in options.items():
            resource_content += f"set {key} {value}\n"
        resource_content += "exploit\n"

        # Save resource script to a temporary file
        resource_file = "/tmp/mcp_msf_resource.rc"
        with open(resource_file, "w") as f:
            f.write(resource_content)

        command = f"msfconsole -q -r {resource_file}"

        logger.info(f"[>] Starting Metasploit module: {module}")
        result = execute_command(command)

        # Clean up the temporary file
        try:
            os.remove(resource_file)
        except Exception as e:
            logger.warning(f"Error removing temporary resource file: {str(e)}")

        logger.info(f"[#] Metasploit module completed: {module}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in metasploit endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/hydra", methods=["POST"])
def hydra():
    # Execute hydra with enhanced logging
    try:
        params = request.json
        target = params.get("target", "")
        service = params.get("service", "")
        username = params.get("username", "")
        username_file = params.get("username_file", "")
        password = params.get("password", "")
        password_file = params.get("password_file", "")
        additional_args = params.get("additional_args", "")

        if not target or not service:
            logger.warning("[>] Hydra called without target or service parameter")
            return jsonify({
                "error": "Target and service parameters are required"
            }), 400

        if not (username or username_file) or not (password or password_file):
            logger.warning("🔑 Hydra called without username/password parameters")
            return jsonify({
                "error": "Username/username_file and password/password_file are required"
            }), 400

        command = f"hydra -t 4"

        if username:
            command += f" -l {username}"
        elif username_file:
            command += f" -L {username_file}"

        if password:
            command += f" -p {password}"
        elif password_file:
            command += f" -P {password_file}"

        if additional_args:
            command += f" {additional_args}"

        command += f" {target} {service}"

        logger.info(f"🔑 Starting Hydra attack: {target}:{service}")
        result = execute_command(command)
        logger.info(f"[#] Hydra attack completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in hydra endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/john", methods=["POST"])
def john():
    # Execute john with enhanced logging
    try:
        params = request.json
        hash_file = params.get("hash_file", "")
        wordlist = params.get("wordlist", "/usr/share/wordlists/rockyou.txt")
        format_type = params.get("format", "")
        additional_args = params.get("additional_args", "")

        if not hash_file:
            logger.warning("🔐 John called without hash_file parameter")
            return jsonify({
                "error": "Hash file parameter is required"
            }), 400

        command = f"john"

        if format_type:
            command += f" --format={format_type}"

        if wordlist:
            command += f" --wordlist={wordlist}"

        if additional_args:
            command += f" {additional_args}"

        command += f" {hash_file}"

        logger.info(f"🔐 Starting John the Ripper: {hash_file}")
        result = execute_command(command)
        logger.info(f"[#] John the Ripper completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in john endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/wpscan", methods=["POST"])
def wpscan():
    # Execute wpscan with enhanced logging
    try:
        params = request.json
        url = params.get("url", "")
        additional_args = params.get("additional_args", "")

        if not url:
            logger.warning("[W] WPScan called without URL parameter")
            return jsonify({
                "error": "URL parameter is required"
            }), 400

        command = f"wpscan --url {url}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting WPScan: {url}")
        result = execute_command(command)
        logger.info(f"[#] WPScan completed for {url}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in wpscan endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/enum4linux", methods=["POST"])
def enum4linux():
    # Execute enum4linux-ng with enhanced logging
    try:
        params = request.json
        target = params.get("target", "")
        additional_args = params.get("additional_args", "-A")

        if not target:
            logger.warning("[>] Enum4linux called without target parameter")
            return jsonify({
                "error": "Target parameter is required"
            }), 400

        # Map legacy flags to enum4linux-ng flags
        if "-a" in additional_args and "-A" not in additional_args:
            additional_args = additional_args.replace("-a", "-A")

        # Use enum4linux-ng instead of legacy enum4linux
        command = f"enum4linux-ng {additional_args} {target}"

        logger.info(f"[?] Starting Enum4linux-ng: {target}")
        result = execute_command(command)
        logger.info(f"[#] Enum4linux completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in enum4linux endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/ffuf", methods=["POST"])
def ffuf():
    # Execute FFuf web fuzzer with enhanced logging
    try:
        params = request.json
        url = params.get("url", "")
        wordlist = params.get("wordlist", "/usr/share/wordlists/dirb/common.txt")
        mode = params.get("mode", "directory")
        match_codes = params.get("match_codes", "200,204,301,302,307,401,403")
        additional_args = params.get("additional_args", "")

        if not url:
            logger.warning("[W] FFuf called without URL parameter")
            return jsonify({
                "error": "URL parameter is required"
            }), 400

        command = f"ffuf"

        if mode == "directory":
            command += f" -u {url}/FUZZ -w {wordlist}"
        elif mode == "vhost":
            command += f" -u {url} -H 'Host: FUZZ' -w {wordlist}"
        elif mode == "parameter":
            command += f" -u {url}?FUZZ=value -w {wordlist}"
        else:
            command += f" -u {url} -w {wordlist}"

        command += f" -mc {match_codes}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting FFuf {mode} fuzzing: {url}")
        result = execute_command(command)
        logger.info(f"[#] FFuf fuzzing completed for {url}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in ffuf endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/netexec", methods=["POST"])
def netexec():
    # Execute NetExec (formerly CrackMapExec) with enhanced logging
    try:
        params = request.json
        target = params.get("target", "")
        protocol = params.get("protocol", "smb")
        username = params.get("username", "")
        password = params.get("password", "")
        hash_value = params.get("hash", "")
        module = params.get("module", "")
        additional_args = params.get("additional_args", "")

        if not target:
            logger.warning("[>] NetExec called without target parameter")
            return jsonify({
                "error": "Target parameter is required"
            }), 400

        command = f"nxc {protocol} {target}"

        if username:
            command += f" -u {username}"

        if password:
            command += f" -p {password}"

        if hash_value:
            command += f" -H {hash_value}"

        if module:
            command += f" -M {module}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting NetExec {protocol} scan: {target}")
        result = execute_command(command)
        logger.info(f"[#] NetExec scan completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in netexec endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/amass", methods=["POST"])
def amass():
    # Execute Amass for subdomain enumeration with enhanced logging
    try:
        params = request.json
        domain = params.get("domain", "")
        mode = params.get("mode", "enum")
        additional_args = params.get("additional_args", "")

        if not domain:
            logger.warning("[W] Amass called without domain parameter")
            return jsonify({
                "error": "Domain parameter is required"
            }), 400

        command = f"amass {mode}"

        if mode == "enum":
            command += f" -d {domain}"
        else:
            command += f" -d {domain}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting Amass {mode}: {domain}")
        result = execute_command(command)
        logger.info(f"[#] Amass completed for {domain}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in amass endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/hashcat", methods=["POST"])
def hashcat():
    # Execute Hashcat for password cracking with enhanced logging
    try:
        params = request.json
        hash_file = params.get("hash_file", "")
        hash_type = params.get("hash_type", "")
        attack_mode = params.get("attack_mode", "0")
        wordlist = params.get("wordlist", "/usr/share/wordlists/rockyou.txt")
        mask = params.get("mask", "")
        additional_args = params.get("additional_args", "")

        if not hash_file:
            logger.warning("🔐 Hashcat called without hash_file parameter")
            return jsonify({
                "error": "Hash file parameter is required"
            }), 400

        if not hash_type:
            logger.warning("🔐 Hashcat called without hash_type parameter")
            return jsonify({
                "error": "Hash type parameter is required"
            }), 400

        command = f"hashcat -m {hash_type} -a {attack_mode} {hash_file}"

        if attack_mode == "0" and wordlist:
            command += f" {wordlist}"
        elif attack_mode == "3" and mask:
            command += f" {mask}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🔐 Starting Hashcat attack: mode {attack_mode}")
        result = execute_command(command)
        logger.info(f"[#] Hashcat attack completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in hashcat endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/subfinder", methods=["POST"])
def subfinder():
    # Execute Subfinder for passive subdomain enumeration with enhanced logging
    try:
        params = request.json
        domain = params.get("domain", "")
        silent = params.get("silent", True)
        all_sources = params.get("all_sources", False)
        additional_args = params.get("additional_args", "")

        if not domain:
            logger.warning("[W] Subfinder called without domain parameter")
            return jsonify({
                "error": "Domain parameter is required"
            }), 400

        command = f"subfinder -d {domain}"

        if silent:
            command += " -silent"

        if all_sources:
            command += " -all"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting Subfinder: {domain}")
        result = execute_command(command)
        logger.info(f"[#] Subfinder completed for {domain}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in subfinder endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/smbmap", methods=["POST"])
def smbmap():
    # Execute SMBMap for SMB share enumeration with enhanced logging
    try:
        params = request.json
        target = params.get("target", "")
        username = params.get("username", "")
        password = params.get("password", "")
        domain = params.get("domain", "")
        additional_args = params.get("additional_args", "")

        if not target:
            logger.warning("[>] SMBMap called without target parameter")
            return jsonify({
                "error": "Target parameter is required"
            }), 400

        command = f"smbmap -H {target}"

        if username:
            command += f" -u {username}"

        if password:
            command += f" -p {password}"

        if domain:
            command += f" -d {domain}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting SMBMap: {target}")
        result = execute_command(command)
        logger.info(f"[#] SMBMap completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in smbmap endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

# ============================================================================
# ENHANCED NETWORK PENETRATION TESTING TOOLS (v6.0)
# ============================================================================

@app.route("/api/tools/rustscan", methods=["POST"])
def rustscan():
    # Execute Rustscan for ultra-fast port scanning with enhanced logging
    try:
        params = request.json
        target = params.get("target", "")
        ports = params.get("ports", "")
        ulimit = params.get("ulimit", 5000)
        batch_size = params.get("batch_size", 4500)
        timeout = params.get("timeout", 1500)
        scripts = params.get("scripts", "")
        additional_args = params.get("additional_args", "")

        if not target:
            logger.warning("[>] Rustscan called without target parameter")
            return jsonify({"error": "Target parameter is required"}), 400

        command = f"rustscan -a {target} --ulimit {ulimit} -b {batch_size} -t {timeout}"

        if ports:
            command += f" -p {ports}"

        if scripts:
            command += f" -- -sC -sV"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[*] Starting Rustscan: {target}")
        result = execute_command(command)
        logger.info(f"[#] Rustscan completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in rustscan endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/masscan", methods=["POST"])
def masscan():
    # Execute Masscan for high-speed Internet-scale port scanning with intelligent rate limiting
    try:
        params = request.json
        target = params.get("target", "")
        ports = params.get("ports", "1-65535")
        rate = params.get("rate", 1000)
        interface = params.get("interface", "")
        router_mac = params.get("router_mac", "")
        source_ip = params.get("source_ip", "")
        banners = params.get("banners", False)
        additional_args = params.get("additional_args", "")

        if not target:
            logger.warning("[>] Masscan called without target parameter")
            return jsonify({"error": "Target parameter is required"}), 400

        command = f"masscan {target} -p{ports} --rate={rate}"

        if interface:
            command += f" -e {interface}"

        if router_mac:
            command += f" --router-mac {router_mac}"

        if source_ip:
            command += f" --source-ip {source_ip}"

        if banners:
            command += " --banners"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[>] Starting Masscan: {target} at rate {rate}")
        result = execute_command(command)
        logger.info(f"[#] Masscan completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in masscan endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/nmap-advanced", methods=["POST"])
def nmap_advanced():
    # Execute advanced Nmap scans with custom NSE scripts and optimized timing
    try:
        params = request.json
        target = params.get("target", "")
        scan_type = params.get("scan_type", "-sS")
        ports = params.get("ports", "")
        timing = params.get("timing", "T4")
        nse_scripts = params.get("nse_scripts", "")
        os_detection = params.get("os_detection", False)
        version_detection = params.get("version_detection", False)
        aggressive = params.get("aggressive", False)
        stealth = params.get("stealth", False)
        additional_args = params.get("additional_args", "")

        if not target:
            logger.warning("[>] Advanced Nmap called without target parameter")
            return jsonify({"error": "Target parameter is required"}), 400

        command = f"nmap {scan_type} {target}"

        if ports:
            command += f" -p {ports}"

        if stealth:
            command += " -T2 -f --mtu 24"
        else:
            command += f" -{timing}"

        if os_detection:
            command += " -O"

        if version_detection:
            command += " -sV"

        if aggressive:
            command += " -A"

        if nse_scripts:
            command += f" --script={nse_scripts}"
        elif not aggressive:  # Default useful scripts if not aggressive
            command += " --script=default,discovery,safe"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting Advanced Nmap: {target}")
        result = execute_command(command)
        logger.info(f"[#] Advanced Nmap completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in advanced nmap endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/autorecon", methods=["POST"])
def autorecon():
    # Execute AutoRecon for comprehensive automated reconnaissance
    try:
        params = request.json
        target = params.get("target", "")
        output_dir = params.get("output_dir", "/tmp/autorecon")
        port_scans = params.get("port_scans", "top-100-ports")
        service_scans = params.get("service_scans", "default")
        heartbeat = params.get("heartbeat", 60)
        timeout = params.get("timeout", 300)
        additional_args = params.get("additional_args", "")

        if not target:
            logger.warning("[>] AutoRecon called without target parameter")
            return jsonify({"error": "Target parameter is required"}), 400

        command = f"autorecon {target} -o {output_dir} --heartbeat {heartbeat} --timeout {timeout}"

        if port_scans != "default":
            command += f" --port-scans {port_scans}"

        if service_scans != "default":
            command += f" --service-scans {service_scans}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🔄 Starting AutoRecon: {target}")
        result = execute_command(command)
        logger.info(f"[#] AutoRecon completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in autorecon endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/enum4linux-ng", methods=["POST"])
def enum4linux_ng():
    # Execute Enum4linux-ng for advanced SMB enumeration with enhanced logging
    try:
        params = request.json
        target = params.get("target", "")
        username = params.get("username", "")
        password = params.get("password", "")
        domain = params.get("domain", "")
        shares = params.get("shares", True)
        users = params.get("users", True)
        groups = params.get("groups", True)
        policy = params.get("policy", True)
        additional_args = params.get("additional_args", "")

        if not target:
            logger.warning("[>] Enum4linux-ng called without target parameter")
            return jsonify({"error": "Target parameter is required"}), 400

        command = f"enum4linux-ng {target}"

        if username:
            command += f" -u {username}"

        if password:
            command += f" -p {password}"

        if domain:
            command += f" -d {domain}"

        # Add specific enumeration options
        enum_options = []
        if shares:
            enum_options.append("S")
        if users:
            enum_options.append("U")
        if groups:
            enum_options.append("G")
        if policy:
            enum_options.append("P")

        if enum_options:
            command += f" -A {','.join(enum_options)}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting Enum4linux-ng: {target}")
        result = execute_command(command)
        logger.info(f"[#] Enum4linux-ng completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in enum4linux-ng endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/rpcclient", methods=["POST"])
def rpcclient():
    # Execute rpcclient for RPC enumeration with enhanced logging
    try:
        params = request.json
        target = params.get("target", "")
        username = params.get("username", "")
        password = params.get("password", "")
        domain = params.get("domain", "")
        commands = params.get("commands", "enumdomusers;enumdomgroups;querydominfo")
        additional_args = params.get("additional_args", "")

        if not target:
            logger.warning("[>] rpcclient called without target parameter")
            return jsonify({"error": "Target parameter is required"}), 400

        # Build authentication string
        auth_string = ""
        if username and password:
            auth_string = f"-U {username}%{password}"
        elif username:
            auth_string = f"-U {username}"
        else:
            auth_string = "-U ''"  # Anonymous

        if domain:
            auth_string += f" -W {domain}"

        # Create command sequence
        command_sequence = commands.replace(";", "\n")

        command = f"echo -e '{command_sequence}' | rpcclient {auth_string} {target}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting rpcclient: {target}")
        result = execute_command(command)
        logger.info(f"[#] rpcclient completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in rpcclient endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/nbtscan", methods=["POST"])
def nbtscan():
    # Execute nbtscan for NetBIOS name scanning with enhanced logging
    try:
        params = request.json
        target = params.get("target", "")
        verbose = params.get("verbose", False)
        timeout = params.get("timeout", 2)
        additional_args = params.get("additional_args", "")

        if not target:
            logger.warning("[>] nbtscan called without target parameter")
            return jsonify({"error": "Target parameter is required"}), 400

        command = f"nbtscan -t {timeout}"

        if verbose:
            command += " -v"

        command += f" {target}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting nbtscan: {target}")
        result = execute_command(command)
        logger.info(f"[#] nbtscan completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in nbtscan endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/arp-scan", methods=["POST"])
def arp_scan():
    # Execute arp-scan for network discovery with enhanced logging
    try:
        params = request.json
        target = params.get("target", "")
        interface = params.get("interface", "")
        local_network = params.get("local_network", False)
        timeout = params.get("timeout", 500)
        retry = params.get("retry", 3)
        additional_args = params.get("additional_args", "")

        if not target and not local_network:
            logger.warning("[>] arp-scan called without target parameter")
            return jsonify({"error": "Target parameter or local_network flag is required"}), 400

        command = f"arp-scan -t {timeout} -r {retry}"

        if interface:
            command += f" -I {interface}"

        if local_network:
            command += " -l"
        else:
            command += f" {target}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting arp-scan: {target if target else 'local network'}")
        result = execute_command(command)
        logger.info(f"[#] arp-scan completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in arp-scan endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/responder", methods=["POST"])
def responder():
    # Execute Responder for credential harvesting with enhanced logging
    try:
        params = request.json
        interface = params.get("interface", "eth0")
        analyze = params.get("analyze", False)
        wpad = params.get("wpad", True)
        force_wpad_auth = params.get("force_wpad_auth", False)
        fingerprint = params.get("fingerprint", False)
        duration = params.get("duration", 300)  # 5 minutes default
        additional_args = params.get("additional_args", "")

        if not interface:
            logger.warning("[>] Responder called without interface parameter")
            return jsonify({"error": "Interface parameter is required"}), 400

        command = f"timeout {duration} responder -I {interface}"

        if analyze:
            command += " -A"

        if wpad:
            command += " -w"

        if force_wpad_auth:
            command += " -F"

        if fingerprint:
            command += " -f"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting Responder on interface: {interface}")
        result = execute_command(command)
        logger.info(f"[#] Responder completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in responder endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/volatility", methods=["POST"])
def volatility():
    # Execute Volatility for memory forensics with enhanced logging
    try:
        params = request.json
        memory_file = params.get("memory_file", "")
        plugin = params.get("plugin", "")
        profile = params.get("profile", "")
        additional_args = params.get("additional_args", "")

        if not memory_file:
            logger.warning("🧠 Volatility called without memory_file parameter")
            return jsonify({
                "error": "Memory file parameter is required"
            }), 400

        if not plugin:
            logger.warning("🧠 Volatility called without plugin parameter")
            return jsonify({
                "error": "Plugin parameter is required"
            }), 400

        command = f"volatility -f {memory_file}"

        if profile:
            command += f" --profile={profile}"

        command += f" {plugin}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🧠 Starting Volatility analysis: {plugin}")
        result = execute_command(command)
        logger.info(f"[#] Volatility analysis completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in volatility endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/msfvenom", methods=["POST"])
def msfvenom():
    # Execute MSFVenom to generate payloads with enhanced logging
    try:
        params = request.json
        payload = params.get("payload", "")
        format_type = params.get("format", "")
        output_file = params.get("output_file", "")
        encoder = params.get("encoder", "")
        iterations = params.get("iterations", "")
        additional_args = params.get("additional_args", "")

        if not payload:
            logger.warning("[>] MSFVenom called without payload parameter")
            return jsonify({
                "error": "Payload parameter is required"
            }), 400

        command = f"msfvenom -p {payload}"

        if format_type:
            command += f" -f {format_type}"

        if output_file:
            command += f" -o {output_file}"

        if encoder:
            command += f" -e {encoder}"

        if iterations:
            command += f" -i {iterations}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[>] Starting MSFVenom payload generation: {payload}")
        result = execute_command(command)
        logger.info(f"[#] MSFVenom payload generated")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in msfvenom endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

# ============================================================================
# BINARY ANALYSIS & REVERSE ENGINEERING TOOLS
# ============================================================================

@app.route("/api/tools/gdb", methods=["POST"])
def gdb():
    # Execute GDB for binary analysis and debugging with enhanced logging
    try:
        params = request.json
        binary = params.get("binary", "")
        commands = params.get("commands", "")
        script_file = params.get("script_file", "")
        additional_args = params.get("additional_args", "")

        if not binary:
            logger.warning("[+] GDB called without binary parameter")
            return jsonify({
                "error": "Binary parameter is required"
            }), 400

        command = f"gdb {binary}"

        if script_file:
            command += f" -x {script_file}"

        if commands:
            temp_script = "/tmp/gdb_commands.txt"
            with open(temp_script, "w") as f:
                f.write(commands)
            command += f" -x {temp_script}"

        if additional_args:
            command += f" {additional_args}"

        command += " -batch"

        logger.info(f"[+] Starting GDB analysis: {binary}")
        result = execute_command(command)

        if commands and os.path.exists("/tmp/gdb_commands.txt"):
            try:
                os.remove("/tmp/gdb_commands.txt")
            except:
                pass

        logger.info(f"[#] GDB analysis completed for {binary}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in gdb endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/radare2", methods=["POST"])
def radare2():
    # Execute Radare2 for binary analysis and reverse engineering with enhanced logging
    try:
        params = request.json
        binary = params.get("binary", "")
        commands = params.get("commands", "")
        additional_args = params.get("additional_args", "")

        if not binary:
            logger.warning("[+] Radare2 called without binary parameter")
            return jsonify({
                "error": "Binary parameter is required"
            }), 400

        if commands:
            temp_script = "/tmp/r2_commands.txt"
            with open(temp_script, "w") as f:
                f.write(commands)
            command = f"r2 -i {temp_script} -q {binary}"
        else:
            command = f"r2 -q {binary}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[+] Starting Radare2 analysis: {binary}")
        result = execute_command(command)

        if commands and os.path.exists("/tmp/r2_commands.txt"):
            try:
                os.remove("/tmp/r2_commands.txt")
            except:
                pass

        logger.info(f"[#] Radare2 analysis completed for {binary}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in radare2 endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/binwalk", methods=["POST"])
def binwalk():
    # Execute Binwalk for firmware and file analysis with enhanced logging
    try:
        params = request.json
        file_path = params.get("file_path", "")
        extract = params.get("extract", False)
        additional_args = params.get("additional_args", "")

        if not file_path:
            logger.warning("[+] Binwalk called without file_path parameter")
            return jsonify({
                "error": "File path parameter is required"
            }), 400

        command = f"binwalk"

        if extract:
            command += " -e"

        if additional_args:
            command += f" {additional_args}"

        command += f" {file_path}"

        logger.info(f"[+] Starting Binwalk analysis: {file_path}")
        result = execute_command(command)
        logger.info(f"[#] Binwalk analysis completed for {file_path}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in binwalk endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/ropgadget", methods=["POST"])
def ropgadget():
    # Search for ROP gadgets in a binary using ROPgadget with enhanced logging
    try:
        params = request.json
        binary = params.get("binary", "")
        gadget_type = params.get("gadget_type", "")
        additional_args = params.get("additional_args", "")

        if not binary:
            logger.warning("[+] ROPgadget called without binary parameter")
            return jsonify({
                "error": "Binary parameter is required"
            }), 400

        command = f"ROPgadget --binary {binary}"

        if gadget_type:
            command += f" --only '{gadget_type}'"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[+] Starting ROPgadget search: {binary}")
        result = execute_command(command)
        logger.info(f"[#] ROPgadget search completed for {binary}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in ropgadget endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/checksec", methods=["POST"])
def checksec():
    # Check security features of a binary with enhanced logging
    try:
        params = request.json
        binary = params.get("binary", "")

        if not binary:
            logger.warning("[+] Checksec called without binary parameter")
            return jsonify({
                "error": "Binary parameter is required"
            }), 400

        command = f"checksec --file={binary}"

        logger.info(f"[+] Starting Checksec analysis: {binary}")
        result = execute_command(command)
        logger.info(f"[#] Checksec analysis completed for {binary}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in checksec endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/xxd", methods=["POST"])
def xxd():
    # Create a hex dump of a file using xxd with enhanced logging
    try:
        params = request.json
        file_path = params.get("file_path", "")
        offset = params.get("offset", "0")
        length = params.get("length", "")
        additional_args = params.get("additional_args", "")

        if not file_path:
            logger.warning("[+] XXD called without file_path parameter")
            return jsonify({
                "error": "File path parameter is required"
            }), 400

        command = f"xxd -s {offset}"

        if length:
            command += f" -l {length}"

        if additional_args:
            command += f" {additional_args}"

        command += f" {file_path}"

        logger.info(f"[+] Starting XXD hex dump: {file_path}")
        result = execute_command(command)
        logger.info(f"[#] XXD hex dump completed for {file_path}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in xxd endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/strings", methods=["POST"])
def strings():
    # Extract strings from a binary file with enhanced logging
    try:
        params = request.json
        file_path = params.get("file_path", "")
        min_len = params.get("min_len", 4)
        additional_args = params.get("additional_args", "")

        if not file_path:
            logger.warning("[+] Strings called without file_path parameter")
            return jsonify({
                "error": "File path parameter is required"
            }), 400

        command = f"strings -n {min_len}"

        if additional_args:
            command += f" {additional_args}"

        command += f" {file_path}"

        logger.info(f"[+] Starting Strings extraction: {file_path}")
        result = execute_command(command)
        logger.info(f"[#] Strings extraction completed for {file_path}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in strings endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/objdump", methods=["POST"])
def objdump():
    # Analyze a binary using objdump with enhanced logging
    try:
        params = request.json
        binary = params.get("binary", "")
        disassemble = params.get("disassemble", True)
        additional_args = params.get("additional_args", "")

        if not binary:
            logger.warning("[+] Objdump called without binary parameter")
            return jsonify({
                "error": "Binary parameter is required"
            }), 400

        command = f"objdump"

        if disassemble:
            command += " -d"
        else:
            command += " -x"

        if additional_args:
            command += f" {additional_args}"

        command += f" {binary}"

        logger.info(f"[+] Starting Objdump analysis: {binary}")
        result = execute_command(command)
        logger.info(f"[#] Objdump analysis completed for {binary}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in objdump endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

# ============================================================================
# ENHANCED BINARY ANALYSIS AND EXPLOITATION FRAMEWORK (v6.0)
# ============================================================================

@app.route("/api/tools/ghidra", methods=["POST"])
def ghidra():
    # Execute Ghidra for advanced binary analysis and reverse engineering
    try:
        params = request.json
        binary = params.get("binary", "")
        project_name = params.get("project_name", "VectorAI_analysis")
        script_file = params.get("script_file", "")
        analysis_timeout = params.get("analysis_timeout", 300)
        output_format = params.get("output_format", "xml")
        additional_args = params.get("additional_args", "")

        if not binary:
            logger.warning("[+] Ghidra called without binary parameter")
            return jsonify({"error": "Binary parameter is required"}), 400

        # Create Ghidra project directory
        project_dir = f"/tmp/ghidra_projects/{project_name}"
        os.makedirs(project_dir, exist_ok=True)

        # Base Ghidra command for headless analysis
        command = f"analyzeHeadless {project_dir} {project_name} -import {binary} -deleteProject"

        if script_file:
            command += f" -postScript {script_file}"

        if output_format == "xml":
            command += f" -postScript ExportXml.java {project_dir}/analysis.xml"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[+] Starting Ghidra analysis: {binary}")
        result = execute_command(command, timeout=analysis_timeout)
        logger.info(f"[#] Ghidra analysis completed for {binary}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in ghidra endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/pwntools", methods=["POST"])
def pwntools():
    # Execute Pwntools for exploit development and automation
    try:
        params = request.json
        script_content = params.get("script_content", "")
        target_binary = params.get("target_binary", "")
        target_host = params.get("target_host", "")
        target_port = params.get("target_port", 0)
        exploit_type = params.get("exploit_type", "local")  # local, remote, format_string, rop
        additional_args = params.get("additional_args", "")

        if not script_content and not target_binary:
            logger.warning("[+] Pwntools called without script content or target binary")
            return jsonify({"error": "Script content or target binary is required"}), 400

        # Create temporary Python script
        script_file = "/tmp/pwntools_exploit.py"

        if script_content:
            # Use provided script content
            with open(script_file, "w") as f:
                f.write(script_content)
        else:
            # Generate basic exploit template
            template = (
                "#!/usr/bin/env python3\n"
                "from pwn import *\n"
                "\n# Configuration\n"
                "context.arch = 'amd64'\n"
                "context.os = 'linux'\n"
                "context.log_level = 'info'\n"
                "\n# Target configuration\n"
                f"binary = '{target_binary}' if '{target_binary}' else None\n"
                f"host = '{target_host}' if '{target_host}' else None\n"
                f"port = {target_port} if {target_port} else None\n"
                "\n# Exploit logic\n"
                "if binary:\n"
                "    p = process(binary)\n"
                "    log.info(f\"Started local process: {binary}\")\n"
                "elif host and port:\n"
                "    p = remote(host, port)\n"
                "    log.info(f\"Connected to {host}:{port}\")\n"
                "else:\n"
                "    log.error(\"No target specified\")\n"
                "    exit(1)\n"
                "\n# Basic interaction\n"
                "p.interactive()\n"
            )
            with open(script_file, "w") as f:
                f.write(template)

        command = f"python3 {script_file}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[+] Starting Pwntools exploit: {exploit_type}")
        result = execute_command(command)

        # Cleanup
        try:
            os.remove(script_file)
        except:
            pass

        logger.info(f"[#] Pwntools exploit completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in pwntools endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/one-gadget", methods=["POST"])
def one_gadget():
    # Execute one_gadget to find one-shot RCE gadgets in libc
    try:
        params = request.json
        libc_path = params.get("libc_path", "")
        level = params.get("level", 1)  # 0, 1, 2 for different constraint levels
        additional_args = params.get("additional_args", "")

        if not libc_path:
            logger.warning("[+] one_gadget called without libc_path parameter")
            return jsonify({"error": "libc_path parameter is required"}), 400

        command = f"one_gadget {libc_path} --level {level}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[+] Starting one_gadget analysis: {libc_path}")
        result = execute_command(command)
        logger.info(f"[#] one_gadget analysis completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in one_gadget endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/libc-database", methods=["POST"])
def libc_database():
    # Execute libc-database for libc identification and offset lookup
    try:
        params = request.json
        action = params.get("action", "find")  # find, dump, download
        symbols = params.get("symbols", "")  # format: "symbol1:offset1 symbol2:offset2"
        libc_id = params.get("libc_id", "")
        additional_args = params.get("additional_args", "")

        if action == "find" and not symbols:
            logger.warning("[+] libc-database find called without symbols")
            return jsonify({"error": "Symbols parameter is required for find action"}), 400

        if action in ["dump", "download"] and not libc_id:
            logger.warning("[+] libc-database called without libc_id for dump/download")
            return jsonify({"error": "libc_id parameter is required for dump/download actions"}), 400

        # Navigate to libc-database directory (assuming it's installed)
        base_command = "cd /opt/libc-database 2>/dev/null || cd ~/libc-database 2>/dev/null || echo 'libc-database not found'"

        if action == "find":
            command = f"{base_command} && ./find {symbols}"
        elif action == "dump":
            command = f"{base_command} && ./dump {libc_id}"
        elif action == "download":
            command = f"{base_command} && ./download {libc_id}"
        else:
            return jsonify({"error": f"Invalid action: {action}"}), 400

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[+] Starting libc-database {action}: {symbols or libc_id}")
        result = execute_command(command)
        logger.info(f"[#] libc-database {action} completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in libc-database endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/gdb-peda", methods=["POST"])
def gdb_peda():
    # Execute GDB with PEDA for enhanced debugging and exploitation
    try:
        params = request.json
        binary = params.get("binary", "")
        commands = params.get("commands", "")
        attach_pid = params.get("attach_pid", 0)
        core_file = params.get("core_file", "")
        additional_args = params.get("additional_args", "")

        if not binary and not attach_pid and not core_file:
            logger.warning("[+] GDB-PEDA called without binary, PID, or core file")
            return jsonify({"error": "Binary, PID, or core file parameter is required"}), 400

        # Base GDB command with PEDA
        command = "gdb -q"

        if binary:
            command += f" {binary}"

        if core_file:
            command += f" {core_file}"

        if attach_pid:
            command += f" -p {attach_pid}"

        # Create command script
        if commands:
            temp_script = "/tmp/gdb_peda_commands.txt"
            peda_commands = f"source ~/peda/peda.py\n{commands}\nquit\n"
            with open(temp_script, "w") as f:
                f.write(peda_commands)
            command += f" -x {temp_script}"
        else:
            # Default PEDA initialization
            command += " -ex 'source ~/peda/peda.py' -ex 'quit'"

        if additional_args:
            command += f" {additional_args}"

        target_info = binary or f'PID {attach_pid}' or core_file
        logger.info(f"[+] Starting GDB-PEDA analysis: {target_info}")
        result = execute_command(command)

        # Cleanup
        if commands and os.path.exists("/tmp/gdb_peda_commands.txt"):
            try:
                os.remove("/tmp/gdb_peda_commands.txt")
            except:
                pass

        logger.info(f"[#] GDB-PEDA analysis completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in gdb-peda endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/angr", methods=["POST"])
def angr():
    # Execute angr for symbolic execution and binary analysis
    try:
        params = request.json
        binary = params.get("binary", "")
        script_content = params.get("script_content", "")
        find_address = params.get("find_address", "")
        avoid_addresses = params.get("avoid_addresses", "")
        analysis_type = params.get("analysis_type", "symbolic")  # symbolic, cfg, static
        additional_args = params.get("additional_args", "")

        if not binary:
            logger.warning("[+] angr called without binary parameter")
            return jsonify({"error": "Binary parameter is required"}), 400

        # Create angr script
        script_file = "/tmp/angr_analysis.py"

        if script_content:
            with open(script_file, "w") as f:
                f.write(script_content)
        else:
            # Generate basic angr template
            template = (
                "#!/usr/bin/env python3\n"
                "import angr\n"
                "import sys\n"
                "\n# Load binary\n"
                f"project = angr.Project('{binary}', auto_load_libs=False)\n"
                f"print(f\"Loaded binary: {binary}\")\n"
                "print(f\"Architecture: {project.arch}\")\n"
                "print(f\"Entry point: {hex(project.entry)}\")\n"
                "\n"
            )
            if analysis_type == "symbolic":
                template += (
                    "\n# Symbolic execution\n"
                    "state = project.factory.entry_state()\n"
                    "simgr = project.factory.simulation_manager(state)\n"
                    "\n# Find and avoid addresses\n"
                    f"find_addr = {find_address if find_address else 'None'}\n"
                    f"avoid_addrs = {avoid_addresses.split(',') if avoid_addresses else '[]'}\n"
                    "\nif find_addr:\n"
                    "    simgr.explore(find=find_addr, avoid=avoid_addrs)\n"
                    "    if simgr.found:\n"
                    "        print(\"Found solution!\")\n"
                    "        solution_state = simgr.found[0]\n"
                    "        print(f\"Input: {solution_state.posix.dumps(0)}\")\n"
                    "    else:\n"
                    "        print(\"No solution found\")\n"
                    "else:\n"
                    "    print(\"No find address specified, running basic analysis\")\n"
                )
            elif analysis_type == "cfg":
                template += (
                    "\n# Control Flow Graph analysis\n"
                    "cfg = project.analyses.CFGFast()\n"
                    "print(f\"CFG nodes: {len(cfg.graph.nodes())}\")\n"
                    "print(f\"CFG edges: {len(cfg.graph.edges())}\")\n"
                    "\n# Function analysis\n"
                    "for func_addr, func in cfg.functions.items():\n"
                    "    print(f\"Function: {func.name} at {hex(func_addr)}\")\n"
                )

            with open(script_file, "w") as f:
                f.write(template)

        command = f"python3 {script_file}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[+] Starting angr analysis: {binary}")
        result = execute_command(command, timeout=600)  # Longer timeout for symbolic execution

        # Cleanup
        try:
            os.remove(script_file)
        except:
            pass

        logger.info(f"[#] angr analysis completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in angr endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/ropper", methods=["POST"])
def ropper():
    # Execute ropper for advanced ROP/JOP gadget searching
    try:
        params = request.json
        binary = params.get("binary", "")
        gadget_type = params.get("gadget_type", "rop")  # rop, jop, sys, all
        quality = params.get("quality", 1)  # 1-5, higher = better quality
        arch = params.get("arch", "")  # x86, x86_64, arm, etc.
        search_string = params.get("search_string", "")
        additional_args = params.get("additional_args", "")

        if not binary:
            logger.warning("[+] ropper called without binary parameter")
            return jsonify({"error": "Binary parameter is required"}), 400

        command = f"ropper --file {binary}"

        if gadget_type == "rop":
            command += " --rop"
        elif gadget_type == "jop":
            command += " --jop"
        elif gadget_type == "sys":
            command += " --sys"
        elif gadget_type == "all":
            command += " --all"

        if quality > 1:
            command += f" --quality {quality}"

        if arch:
            command += f" --arch {arch}"

        if search_string:
            command += f" --search '{search_string}'"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[+] Starting ropper analysis: {binary}")
        result = execute_command(command)
        logger.info(f"[#] ropper analysis completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in ropper endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/pwninit", methods=["POST"])
def pwninit():
    # Execute pwninit for CTF binary exploitation setup
    try:
        params = request.json
        binary = params.get("binary", "")
        libc = params.get("libc", "")
        ld = params.get("ld", "")
        template_type = params.get("template_type", "python")  # python, c
        additional_args = params.get("additional_args", "")

        if not binary:
            logger.warning("[+] pwninit called without binary parameter")
            return jsonify({"error": "Binary parameter is required"}), 400

        command = f"pwninit --bin {binary}"

        if libc:
            command += f" --libc {libc}"

        if ld:
            command += f" --ld {ld}"

        if template_type:
            command += f" --template {template_type}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[+] Starting pwninit setup: {binary}")
        result = execute_command(command)
        logger.info(f"[#] pwninit setup completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in pwninit endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

# ============================================================================
# ADDITIONAL WEB SECURITY TOOLS
# ============================================================================

@app.route("/api/tools/feroxbuster", methods=["POST"])
def feroxbuster():
    # Execute Feroxbuster for recursive content discovery with enhanced logging
    try:
        params = request.json
        url = params.get("url", "")
        wordlist = params.get("wordlist", "/usr/share/wordlists/dirb/common.txt")
        threads = params.get("threads", 10)
        additional_args = params.get("additional_args", "")

        if not url:
            logger.warning("[W] Feroxbuster called without URL parameter")
            return jsonify({
                "error": "URL parameter is required"
            }), 400

        command = f"feroxbuster -u {url} -w {wordlist} -t {threads}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting Feroxbuster scan: {url}")
        result = execute_command(command)
        logger.info(f"[#] Feroxbuster scan completed for {url}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in feroxbuster endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/dotdotpwn", methods=["POST"])
def dotdotpwn():
    # Execute DotDotPwn for directory traversal testing with enhanced logging
    try:
        params = request.json
        target = params.get("target", "")
        module = params.get("module", "http")
        additional_args = params.get("additional_args", "")

        if not target:
            logger.warning("[>] DotDotPwn called without target parameter")
            return jsonify({
                "error": "Target parameter is required"
            }), 400

        command = f"dotdotpwn -m {module} -h {target}"

        if additional_args:
            command += f" {additional_args}"

        command += " -b"

        logger.info(f"[?] Starting DotDotPwn scan: {target}")
        result = execute_command(command)
        logger.info(f"[#] DotDotPwn scan completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in dotdotpwn endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/xsser", methods=["POST"])
def xsser():
    # Execute XSSer for XSS vulnerability testing with enhanced logging
    try:
        params = request.json
        url = params.get("url", "")
        params_str = params.get("params", "")
        additional_args = params.get("additional_args", "")

        if not url:
            logger.warning("[W] XSSer called without URL parameter")
            return jsonify({
                "error": "URL parameter is required"
            }), 400

        command = f"xsser --url '{url}'"

        if params_str:
            command += f" --param='{params_str}'"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting XSSer scan: {url}")
        result = execute_command(command)
        logger.info(f"[#] XSSer scan completed for {url}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in xsser endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/wfuzz", methods=["POST"])
def wfuzz():
    # Execute Wfuzz for web application fuzzing with enhanced logging
    try:
        params = request.json
        url = params.get("url", "")
        wordlist = params.get("wordlist", "/usr/share/wordlists/dirb/common.txt")
        additional_args = params.get("additional_args", "")

        if not url:
            logger.warning("[W] Wfuzz called without URL parameter")
            return jsonify({
                "error": "URL parameter is required"
            }), 400

        command = f"wfuzz -w {wordlist} '{url}'"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting Wfuzz scan: {url}")
        result = execute_command(command)
        logger.info(f"[#] Wfuzz scan completed for {url}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in wfuzz endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

# ============================================================================
# ENHANCED WEB APPLICATION SECURITY TOOLS (v6.0)
# ============================================================================

@app.route("/api/tools/dirsearch", methods=["POST"])
def dirsearch():
    # Execute Dirsearch for advanced directory and file discovery with enhanced logging
    try:
        params = request.json
        url = params.get("url", "")
        extensions = params.get("extensions", "php,html,js,txt,xml,json")
        wordlist = params.get("wordlist", "/usr/share/wordlists/dirsearch/common.txt")
        threads = params.get("threads", 30)
        recursive = params.get("recursive", False)
        additional_args = params.get("additional_args", "")

        if not url:
            logger.warning("[W] Dirsearch called without URL parameter")
            return jsonify({"error": "URL parameter is required"}), 400

        command = f"dirsearch -u {url} -e {extensions} -w {wordlist} -t {threads}"

        if recursive:
            command += " -r"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"📁 Starting Dirsearch scan: {url}")
        result = execute_command(command)
        logger.info(f"[#] Dirsearch scan completed for {url}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in dirsearch endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/katana", methods=["POST"])
def katana():
    # Execute Katana for next-generation crawling and spidering with enhanced logging
    try:
        params = request.json
        url = params.get("url", "")
        depth = params.get("depth", 3)
        js_crawl = params.get("js_crawl", True)
        form_extraction = params.get("form_extraction", True)
        output_format = params.get("output_format", "json")
        additional_args = params.get("additional_args", "")

        if not url:
            logger.warning("[W] Katana called without URL parameter")
            return jsonify({"error": "URL parameter is required"}), 400

        command = f"katana -u {url} -d {depth}"

        if js_crawl:
            command += " -jc"

        if form_extraction:
            command += " -fx"

        if output_format == "json":
            command += " -jsonl"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"⚔️  Starting Katana crawl: {url}")
        result = execute_command(command)
        logger.info(f"[#] Katana crawl completed for {url}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in katana endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/gau", methods=["POST"])
def gau():
    # Execute Gau (Get All URLs) for URL discovery from multiple sources with enhanced logging
    try:
        params = request.json
        domain = params.get("domain", "")
        providers = params.get("providers", "wayback,commoncrawl,otx,urlscan")
        include_subs = params.get("include_subs", True)
        blacklist = params.get("blacklist", "png,jpg,gif,jpeg,swf,woff,svg,pdf,css,ico")
        additional_args = params.get("additional_args", "")

        if not domain:
            logger.warning("[W] Gau called without domain parameter")
            return jsonify({"error": "Domain parameter is required"}), 400

        command = f"gau {domain}"

        if providers != "wayback,commoncrawl,otx,urlscan":
            command += f" --providers {providers}"

        if include_subs:
            command += " --subs"

        if blacklist:
            command += f" --blacklist {blacklist}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"📡 Starting Gau URL discovery: {domain}")
        result = execute_command(command)
        logger.info(f"[#] Gau URL discovery completed for {domain}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in gau endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/waybackurls", methods=["POST"])
def waybackurls():
    # Execute Waybackurls for historical URL discovery with enhanced logging
    try:
        params = request.json
        domain = params.get("domain", "")
        get_versions = params.get("get_versions", False)
        no_subs = params.get("no_subs", False)
        additional_args = params.get("additional_args", "")

        if not domain:
            logger.warning("[W] Waybackurls called without domain parameter")
            return jsonify({"error": "Domain parameter is required"}), 400

        command = f"waybackurls {domain}"

        if get_versions:
            command += " --get-versions"

        if no_subs:
            command += " --no-subs"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🕰️  Starting Waybackurls discovery: {domain}")
        result = execute_command(command)
        logger.info(f"[#] Waybackurls discovery completed for {domain}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in waybackurls endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/arjun", methods=["POST"])
def arjun():
    # Execute Arjun for HTTP parameter discovery with enhanced logging
    try:
        params = request.json
        url = params.get("url", "")
        method = params.get("method", "GET")
        wordlist = params.get("wordlist", "")
        delay = params.get("delay", 0)
        threads = params.get("threads", 25)
        stable = params.get("stable", False)
        additional_args = params.get("additional_args", "")

        if not url:
            logger.warning("[W] Arjun called without URL parameter")
            return jsonify({"error": "URL parameter is required"}), 400

        command = f"arjun -u {url} -m {method} -t {threads}"

        if wordlist:
            command += f" -w {wordlist}"

        if delay > 0:
            command += f" -d {delay}"

        if stable:
            command += " --stable"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[>] Starting Arjun parameter discovery: {url}")
        result = execute_command(command)
        logger.info(f"[#] Arjun parameter discovery completed for {url}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in arjun endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/paramspider", methods=["POST"])
def paramspider():
    # Execute ParamSpider for parameter mining from web archives with enhanced logging
    try:
        params = request.json
        domain = params.get("domain", "")
        level = params.get("level", 2)
        exclude = params.get("exclude", "png,jpg,gif,jpeg,swf,woff,svg,pdf,css,ico")
        output = params.get("output", "")
        additional_args = params.get("additional_args", "")

        if not domain:
            logger.warning("[W] ParamSpider called without domain parameter")
            return jsonify({"error": "Domain parameter is required"}), 400

        command = f"paramspider -d {domain} -l {level}"

        if exclude:
            command += f" --exclude {exclude}"

        if output:
            command += f" -o {output}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🕷️  Starting ParamSpider mining: {domain}")
        result = execute_command(command)
        logger.info(f"[#] ParamSpider mining completed for {domain}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in paramspider endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/x8", methods=["POST"])
def x8():
    # Execute x8 for hidden parameter discovery with enhanced logging
    try:
        params = request.json
        url = params.get("url", "")
        wordlist = params.get("wordlist", "/usr/share/wordlists/x8/params.txt")
        method = params.get("method", "GET")
        body = params.get("body", "")
        headers = params.get("headers", "")
        additional_args = params.get("additional_args", "")

        if not url:
            logger.warning("[W] x8 called without URL parameter")
            return jsonify({"error": "URL parameter is required"}), 400

        command = f"x8 -u {url} -w {wordlist} -X {method}"

        if body:
            command += f" -b '{body}'"

        if headers:
            command += f" -H '{headers}'"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting x8 parameter discovery: {url}")
        result = execute_command(command)
        logger.info(f"[#] x8 parameter discovery completed for {url}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in x8 endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/jaeles", methods=["POST"])
def jaeles():
    # Execute Jaeles for advanced vulnerability scanning with custom signatures
    try:
        params = request.json
        url = params.get("url", "")
        signatures = params.get("signatures", "")
        config = params.get("config", "")
        threads = params.get("threads", 20)
        timeout = params.get("timeout", 20)
        additional_args = params.get("additional_args", "")

        if not url:
            logger.warning("[W] Jaeles called without URL parameter")
            return jsonify({"error": "URL parameter is required"}), 400

        command = f"jaeles scan -u {url} -c {threads} --timeout {timeout}"

        if signatures:
            command += f" -s {signatures}"

        if config:
            command += f" --config {config}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🔬 Starting Jaeles vulnerability scan: {url}")
        result = execute_command(command)
        logger.info(f"[#] Jaeles vulnerability scan completed for {url}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in jaeles endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/dalfox", methods=["POST"])
def dalfox():
    # Execute Dalfox for advanced XSS vulnerability scanning with enhanced logging
    try:
        params = request.json
        url = params.get("url", "")
        pipe_mode = params.get("pipe_mode", False)
        blind = params.get("blind", False)
        mining_dom = params.get("mining_dom", True)
        mining_dict = params.get("mining_dict", True)
        custom_payload = params.get("custom_payload", "")
        additional_args = params.get("additional_args", "")

        if not url and not pipe_mode:
            logger.warning("[W] Dalfox called without URL parameter")
            return jsonify({"error": "URL parameter is required"}), 400

        if pipe_mode:
            command = "dalfox pipe"
        else:
            command = f"dalfox url {url}"

        if blind:
            command += " --blind"

        if mining_dom:
            command += " --mining-dom"

        if mining_dict:
            command += " --mining-dict"

        if custom_payload:
            command += f" --custom-payload '{custom_payload}'"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[>] Starting Dalfox XSS scan: {url if url else 'pipe mode'}")
        result = execute_command(command)
        logger.info(f"[#] Dalfox XSS scan completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in dalfox endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/httpx", methods=["POST"])
def httpx():
    # Execute httpx for fast HTTP probing and technology detection
    try:
        params = request.json
        target = params.get("target", "")
        probe = params.get("probe", True)
        tech_detect = params.get("tech_detect", False)
        status_code = params.get("status_code", False)
        content_length = params.get("content_length", False)
        title = params.get("title", False)
        web_server = params.get("web_server", False)
        threads = params.get("threads", 50)
        additional_args = params.get("additional_args", "")

        if not target:
            logger.warning("[W] httpx called without target parameter")
            return jsonify({"error": "Target parameter is required"}), 400

        command = f"httpx-toolkit -l {target} -t {threads}"

        if probe:
            command += " -probe"

        if tech_detect:
            command += " -tech-detect"

        if status_code:
            command += " -sc"

        if content_length:
            command += " -cl"

        if title:
            command += " -title"

        if web_server:
            command += " -server"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🌍 Starting httpx probe: {target}")
        result = execute_command(command)
        logger.info(f"[#] httpx probe completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in httpx endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/anew", methods=["POST"])
def anew():
    # Execute anew for appending new lines to files (useful for data processing)
    try:
        params = request.json
        input_data = params.get("input_data", "")
        output_file = params.get("output_file", "")
        additional_args = params.get("additional_args", "")

        if not input_data:
            logger.warning("[N] Anew called without input data")
            return jsonify({"error": "Input data is required"}), 400

        if output_file:
            command = f"echo '{input_data}' | anew {output_file}"
        else:
            command = f"echo '{input_data}' | anew"

        if additional_args:
            command += f" {additional_args}"

        logger.info("[N] Starting anew data processing")
        result = execute_command(command)
        logger.info("[#] anew data processing completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in anew endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/qsreplace", methods=["POST"])
def qsreplace():
    # Execute qsreplace for query string parameter replacement
    try:
        params = request.json
        urls = params.get("urls", "")
        replacement = params.get("replacement", "FUZZ")
        additional_args = params.get("additional_args", "")

        if not urls:
            logger.warning("[W] qsreplace called without URLs")
            return jsonify({"error": "URLs parameter is required"}), 400

        command = f"echo '{urls}' | qsreplace '{replacement}'"

        if additional_args:
            command += f" {additional_args}"

        logger.info("🔄 Starting qsreplace parameter replacement")
        result = execute_command(command)
        logger.info("[#] qsreplace parameter replacement completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in qsreplace endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/uro", methods=["POST"])
def uro():
    # Execute uro for filtering out similar URLs
    try:
        params = request.json
        urls = params.get("urls", "")
        whitelist = params.get("whitelist", "")
        blacklist = params.get("blacklist", "")
        additional_args = params.get("additional_args", "")

        if not urls:
            logger.warning("[W] uro called without URLs")
            return jsonify({"error": "URLs parameter is required"}), 400

        command = f"echo '{urls}' | uro"

        if whitelist:
            command += f" --whitelist {whitelist}"

        if blacklist:
            command += f" --blacklist {blacklist}"

        if additional_args:
            command += f" {additional_args}"

        logger.info("[?] Starting uro URL filtering")
        result = execute_command(command)
        logger.info("[#] uro URL filtering completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in uro endpoint: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

# ============================================================================
# ADVANCED WEB SECURITY TOOLS CONTINUED
# ============================================================================

# ============================================================================
# ENHANCED HTTP TESTING FRAMEWORK (BURP SUITE ALTERNATIVE)
# ============================================================================

# HTTPTestingFramework moved to vectorai_app.workflows.http_testing
# BrowserAgent moved to vectorai_app.workflows.browser
# Global instances
http_framework = HTTPTestingFramework()
browser_agent = BrowserAgent()

@app.route("/api/tools/http-framework", methods=["POST"])
def http_framework_endpoint():
    # Enhanced HTTP testing framework (Burp Suite alternative)
    try:
        params = request.json
        action = params.get("action", "request")  # request, spider, proxy_history, set_rules, set_scope, repeater, intruder
        url = params.get("url", "")
        method = params.get("method", "GET")
        data = params.get("data", {})
        headers = params.get("headers", {})
        cookies = params.get("cookies", {})

        logger.info(f"{ModernVisualEngine.create_section_header('HTTP FRAMEWORK', '[!]', 'FIRE_RED')}")

        if action == "request":
            if not url:
                return jsonify({"error": "URL parameter is required for request action"}), 400

            request_command = f"{method} {url}"
            logger.info(f"{ModernVisualEngine.format_command_execution(request_command, 'STARTING')}")
            result = http_framework.intercept_request(url, method, data, headers, cookies)

            if result.get("success"):
                logger.info(f"{ModernVisualEngine.format_tool_status('HTTP-Framework', 'SUCCESS', url)}")
            else:
                logger.error(f"{ModernVisualEngine.format_tool_status('HTTP-Framework', 'FAILED', url)}")

            return jsonify(result)

        elif action == "spider":
            if not url:
                return jsonify({"error": "URL parameter is required for spider action"}), 400

            max_depth = params.get("max_depth", 3)
            max_pages = params.get("max_pages", 100)

            spider_command = f"Spider {url}"
            logger.info(f"{ModernVisualEngine.format_command_execution(spider_command, 'STARTING')}")
            result = http_framework.spider_website(url, max_depth, max_pages)

            if result.get("success"):
                total_pages = result.get("total_pages", 0)
                pages_info = f"{total_pages} pages"
                logger.info(f"{ModernVisualEngine.format_tool_status('HTTP-Spider', 'SUCCESS', pages_info)}")
            else:
                logger.error(f"{ModernVisualEngine.format_tool_status('HTTP-Spider', 'FAILED', url)}")

            return jsonify(result)

        elif action == "proxy_history":
            return jsonify({
                "success": True,
                "history": http_framework.proxy_history[-100:],  # Last 100 requests
                "total_requests": len(http_framework.proxy_history),
                "vulnerabilities": http_framework.vulnerabilities,
            })

        elif action == "set_rules":
            rules = params.get("rules", [])
            http_framework.set_match_replace_rules(rules)
            return jsonify({"success": True, "rules_set": len(rules)})

        elif action == "set_scope":
            scope_host = params.get("host")
            include_sub = params.get("include_subdomains", True)
            if not scope_host:
                return jsonify({"error": "host parameter required"}), 400
            http_framework.set_scope(scope_host, include_sub)
            return jsonify({"success": True, "scope": http_framework.scope})

        elif action == "repeater":
            request_spec = params.get("request") or {}
            result = http_framework.send_custom_request(request_spec)
            return jsonify(result)

        elif action == "intruder":
            if not url:
                return jsonify({"error": "URL parameter required"}), 400
            method = params.get("method", "GET")
            location = params.get("location", "query")
            fuzz_params = params.get("params", [])
            payloads = params.get("payloads", [])
            base_data = params.get("base_data", {})
            max_requests = params.get("max_requests", 100)
            result = http_framework.intruder_sniper(
                url, method, location, fuzz_params, payloads, base_data, max_requests
            )
            return jsonify(result)

        else:
            return jsonify({"error": f"Unknown action: {action}"}), 400

    except Exception as e:
        logger.error(f"{ModernVisualEngine.format_error_card('ERROR', 'HTTP-Framework', str(e))}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/browser-agent", methods=["POST"])
def browser_agent_endpoint():
    # AI-powered browser agent for web application inspection
    try:
        params = request.json or {}
        action = params.get("action", "navigate")  # navigate, screenshot, close
        url = params.get("url", "")
        headless = params.get("headless", True)
        wait_time = params.get("wait_time", 5)
        proxy_port = params.get("proxy_port")
        active_tests = params.get("active_tests", False)

        logger.info(
            f"{ModernVisualEngine.create_section_header('BROWSER AGENT', '[W]', 'CRIMSON')}"
        )

        if action == "navigate":
            if not url:
                return (
                    jsonify({"error": "URL parameter is required for navigate action"}),
                    400,
                )

            # Setup browser if not already done
            if not browser_agent.driver:
                setup_success = browser_agent.setup_browser(headless, proxy_port)
                if not setup_success:
                    return jsonify({"error": "Failed to setup browser"}), 500

            result = browser_agent.navigate_and_inspect(url, wait_time)
            if result.get("success") and active_tests:
                active_results = browser_agent.run_active_tests(
                    result.get("page_info", {})
                )
                result["active_tests"] = active_results
                if active_results["active_findings"]:
                    logger.warning(
                        ModernVisualEngine.format_error_card(
                            "WARNING",
                            "BrowserAgent",
                            f"Active findings: {len(active_results['active_findings'])}",
                        )
                    )
            return jsonify(result)

        elif action == "screenshot":
            if not browser_agent.driver:
                return (
                    jsonify(
                        {"error": "Browser not initialized. Use navigate action first."}
                    ),
                    400,
                )

            screenshot_path = f"/tmp/VectorAI_screenshot_{int(time.time())}.png"
            browser_agent.driver.save_screenshot(screenshot_path)

            return jsonify(
                {
                    "success": True,
                    "screenshot": screenshot_path,
                    "current_url": browser_agent.driver.current_url,
                    "timestamp": datetime.now().isoformat(),
                }
            )

        elif action == "close":
            browser_agent.close_browser()
            return jsonify({"success": True, "message": "Browser closed successfully"})

        elif action == "status":
            return jsonify(
                {
                    "success": True,
                    "browser_active": browser_agent.driver is not None,
                    "screenshots_taken": len(browser_agent.screenshots),
                    "pages_visited": len(browser_agent.page_sources),
                }
            )

        else:
            return jsonify({"error": f"Unknown action: {action}"}), 400

    except Exception as e:
        logger.error(
            f"{ModernVisualEngine.format_error_card('ERROR', 'BrowserAgent', str(e))}"
        )
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/tools/burpsuite-alternative", methods=["POST"])
def burpsuite_alternative():
    # Comprehensive Burp Suite alternative combining HTTP framework and browser agent
    try:
        params = request.json
        target = params.get("target", "")
        scan_type = params.get("scan_type", "comprehensive")  # comprehensive, spider, passive, active
        headless = params.get("headless", True)
        max_depth = params.get("max_depth", 3)
        max_pages = params.get("max_pages", 50)

        if not target:
            return jsonify({"error": "Target parameter is required"}), 400

        logger.info(f"{ModernVisualEngine.create_section_header('BURP SUITE ALTERNATIVE', '[!]', 'BLOOD_RED')}")
        scan_message = f'Starting {scan_type} scan of {target}'
        logger.info(f"{ModernVisualEngine.format_highlighted_text(scan_message, 'RED')}")

        results = {
            'target': target,
            'scan_type': scan_type,
            'timestamp': datetime.now().isoformat(),
            'success': True
        }

        # Phase 1: Browser-based reconnaissance
        if scan_type in ['comprehensive', 'spider']:
            logger.info(f"{ModernVisualEngine.format_tool_status('BrowserAgent', 'RUNNING', 'Reconnaissance Phase')}")

            if not browser_agent.driver:
                browser_agent.setup_browser(headless)

            browser_result = browser_agent.navigate_and_inspect(target)
            results['browser_analysis'] = browser_result

        # Phase 2: HTTP spidering
        if scan_type in ['comprehensive', 'spider']:
            logger.info(f"{ModernVisualEngine.format_tool_status('HTTP-Spider', 'RUNNING', 'Discovery Phase')}")

            spider_result = http_framework.spider_website(target, max_depth, max_pages)
            results['spider_analysis'] = spider_result

        # Phase 3: Vulnerability analysis
        if scan_type in ['comprehensive', 'active']:
            logger.info(f"{ModernVisualEngine.format_tool_status('VulnScanner', 'RUNNING', 'Analysis Phase')}")

            # Test discovered endpoints
            discovered_urls = results.get('spider_analysis', {}).get('discovered_urls', [target])
            vuln_results = []

            for url in discovered_urls[:20]:  # Limit to 20 URLs
                test_result = http_framework.intercept_request(url)
                if test_result.get('success'):
                    vuln_results.append(test_result)

            results['vulnerability_analysis'] = {
                'tested_urls': len(vuln_results),
                'total_vulnerabilities': len(http_framework.vulnerabilities),
                'recent_vulnerabilities': http_framework._get_recent_vulns(20)
            }

        # Generate summary
        total_vulns = len(http_framework.vulnerabilities)
        vuln_summary = {}
        for vuln in http_framework.vulnerabilities:
            severity = vuln.get('severity', 'unknown')
            vuln_summary[severity] = vuln_summary.get(severity, 0) + 1

        results['summary'] = {
            'total_vulnerabilities': total_vulns,
            'vulnerability_breakdown': vuln_summary,
            'pages_analyzed': len(results.get('spider_analysis', {}).get('discovered_urls', [])),
            'security_score': max(0, 100 - (total_vulns * 5))
        }

        # Display summary with enhanced colors
        logger.info(f"{ModernVisualEngine.create_section_header('SCAN COMPLETE', '[OK]', 'SUCCESS')}")
        vuln_message = f'Found {total_vulns} vulnerabilities'
        color_choice = 'YELLOW' if total_vulns > 0 else 'GREEN'
        logger.info(f"{ModernVisualEngine.format_highlighted_text(vuln_message, color_choice)}")

        for severity, count in vuln_summary.items():
            logger.info(f"  {ModernVisualEngine.format_vulnerability_severity(severity, count)}")

        return jsonify(results)

    except Exception as e:
        logger.error(f"{ModernVisualEngine.format_error_card('CRITICAL', 'BurpAlternative', str(e))}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500
        logger.error(f"[!!] Error in burpsuite endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/zap", methods=["POST"])
def zap():
    # Execute OWASP ZAP with enhanced logging
    try:
        params = request.json
        target = params.get("target", "")
        scan_type = params.get("scan_type", "baseline")
        api_key = params.get("api_key", "")
        daemon = params.get("daemon", False)
        port = params.get("port", "8090")
        host = params.get("host", "0.0.0.0")
        format_type = params.get("format", "xml")
        output_file = params.get("output_file", "")
        additional_args = params.get("additional_args", "")

        if not target and scan_type != "daemon":
            logger.warning("[>] ZAP called without target parameter")
            return jsonify({
                "error": "Target parameter is required for scans"
            }), 400

        if daemon:
            command = f"zaproxy -daemon -host {host} -port {port}"
            if api_key:
                command += f" -config api.key={api_key}"
        else:
            command = f"zaproxy -cmd -quickurl {target}"

            if format_type:
                command += f" -quickout {format_type}"

            if output_file:
                command += f" -quickprogress -dir \"{output_file}\""

            if api_key:
                command += f" -config api.key={api_key}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting ZAP scan: {target}")
        result = execute_command(command)
        logger.info(f"[#] ZAP scan completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in zap endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/wafw00f", methods=["POST"])
def wafw00f():
    # Execute wafw00f to identify and fingerprint WAF products with enhanced logging
    try:
        params = request.json
        target = params.get("target", "")
        additional_args = params.get("additional_args", "")

        if not target:
            logger.warning("🛡️ Wafw00f called without target parameter")
            return jsonify({
                "error": "Target parameter is required"
            }), 400

        command = f"wafw00f {target}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🛡️ Starting Wafw00f WAF detection: {target}")
        result = execute_command(command)
        logger.info(f"[#] Wafw00f completed for {target}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in wafw00f endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/fierce", methods=["POST"])
def fierce():
    # Execute fierce for DNS reconnaissance with enhanced logging
    try:
        params = request.json
        domain = params.get("domain", "")
        dns_server = params.get("dns_server", "")
        additional_args = params.get("additional_args", "")

        if not domain:
            logger.warning("[W] Fierce called without domain parameter")
            return jsonify({
                "error": "Domain parameter is required"
            }), 400

        command = f"fierce --domain {domain}"

        if dns_server:
            command += f" --dns-servers {dns_server}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting Fierce DNS recon: {domain}")
        result = execute_command(command)
        logger.info(f"[#] Fierce completed for {domain}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in fierce endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/dnsenum", methods=["POST"])
def dnsenum():
    # Execute dnsenum for DNS enumeration with enhanced logging
    try:
        params = request.json
        domain = params.get("domain", "")
        dns_server = params.get("dns_server", "")
        wordlist = params.get("wordlist", "")
        additional_args = params.get("additional_args", "")

        if not domain:
            logger.warning("[W] DNSenum called without domain parameter")
            return jsonify({
                "error": "Domain parameter is required"
            }), 400

        command = f"dnsenum {domain}"

        if dns_server:
            command += f" --dnsserver {dns_server}"

        if wordlist:
            command += f" --file {wordlist}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"[?] Starting DNSenum: {domain}")
        result = execute_command(command)
        logger.info(f"[#] DNSenum completed for {domain}")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in dnsenum endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

# Python Environment Management Endpoints
@app.route("/api/python/install", methods=["POST"])
def install_python_package():
    # Install a Python package in a virtual environment
    try:
        params = request.json
        package = params.get("package", "")
        env_name = params.get("env_name", "default")

        if not package:
            return jsonify({"error": "Package name is required"}), 400

        logger.info(f"📦 Installing Python package: {package} in env {env_name}")
        success = env_manager.install_package(env_name, package)

        if success:
            return jsonify({
                "success": True,
                "message": f"Package {package} installed successfully",
                "env_name": env_name
            })
        else:
            return jsonify({
                "success": False,
                "error": f"Failed to install package {package}"
            }), 500

    except Exception as e:
        logger.error(f"[!!] Error installing Python package: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/python/execute", methods=["POST"])
def execute_python_script():
    # Execute a Python script in a virtual environment
    try:
        params = request.json
        script = params.get("script", "")
        env_name = params.get("env_name", "default")
        filename = params.get("filename", f"script_{int(time.time())}.py")

        if not script:
            return jsonify({"error": "Script content is required"}), 400

        # Create script file
        script_result = file_manager.create_file(filename, script)
        if not script_result["success"]:
            return jsonify(script_result), 500

        # Get Python path for environment
        python_path = env_manager.get_python_path(env_name)
        script_path = script_result["path"]

        # Execute script
        command = f"{python_path} {script_path}"
        logger.info(f"🐍 Executing Python script in env {env_name}: {filename}")
        result = execute_command(command, use_cache=False)

        # Clean up script file
        file_manager.delete_file(filename)

        result["env_name"] = env_name
        result["script_filename"] = filename
        logger.info(f"[#] Python script execution completed")
        return jsonify(result)

    except Exception as e:
        logger.error(f"[!!] Error executing Python script: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

# ============================================================================
# AI-POWERED PAYLOAD GENERATION (v5.0 ENHANCEMENT) UNDER DEVELOPMENT
# ============================================================================

# AIPayloadGenerator moved to vectorai_app.core.payloads
ai_payload_generator = AIPayloadGenerator()

@app.route("/api/ai/generate_payload", methods=["POST"])
def ai_generate_payload():
    # Generate AI-powered contextual payloads for security testing
    try:
        params = request.json
        target_info = {
            "attack_type": params.get("attack_type", "xss"),
            "complexity": params.get("complexity", "basic"),
            "technology": params.get("technology", ""),
            "url": params.get("url", "")
        }

        logger.info(f"🤖 Generating AI payloads for {target_info['attack_type']} attack")
        result = ai_payload_generator.generate_contextual_payload(target_info)

        logger.info(f"[OK] Generated {result['payload_count']} contextual payloads")

        return jsonify({
            "success": True,
            "ai_payload_generation": result,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error in AI payload generation: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/ai/test_payload", methods=["POST"])
def ai_test_payload():
    # Test generated payload against target with AI analysis
    try:
        params = request.json
        payload = params.get("payload", "")
        target_url = params.get("target_url", "")
        method = params.get("method", "GET")

        if not payload or not target_url:
            return jsonify({
                "success": False,
                "error": "Payload and target_url are required"
            }), 400

        logger.info(f"🧪 Testing AI-generated payload against {target_url}")

        # Create test command based on method and payload
        if method.upper() == "GET":
            encoded_payload = payload.replace(" ", "%20").replace("'", "%27")
            test_command = f"curl -s '{target_url}?test={encoded_payload}'"
        else:
            test_command = f"curl -s -X POST -d 'test={payload}' '{target_url}'"

        # Execute test
        result = execute_command(test_command, use_cache=False)

        # AI analysis of results
        analysis = {
            "payload_tested": payload,
            "target_url": target_url,
            "method": method,
            "response_size": len(result.get("stdout", "")),
            "success": result.get("success", False),
            "potential_vulnerability": payload.lower() in result.get("stdout", "").lower(),
            "recommendations": [
                "Analyze response for payload reflection",
                "Check for error messages indicating vulnerability",
                "Monitor application behavior changes"
            ]
        }

        logger.info(f"[?] Payload test completed | Potential vuln: {analysis['potential_vulnerability']}")

        return jsonify({
            "success": True,
            "test_result": result,
            "ai_analysis": analysis,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error in AI payload testing: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"Server error: {str(e)}"
        }), 500

# ============================================================================
# ADVANCED API TESTING TOOLS (v5.0 ENHANCEMENT)
# ============================================================================

@app.route("/api/tools/api_fuzzer", methods=["POST"])
def api_fuzzer():
    # Advanced API endpoint fuzzing with intelligent parameter discovery
    try:
        params = request.json
        base_url = params.get("base_url", "")
        endpoints = params.get("endpoints", [])
        methods = params.get("methods", ["GET", "POST", "PUT", "DELETE"])
        wordlist = params.get("wordlist", "/usr/share/wordlists/api/api-endpoints.txt")

        if not base_url:
            logger.warning("[W] API Fuzzer called without base_url parameter")
            return jsonify({
                "error": "Base URL parameter is required"
            }), 400

        # Create comprehensive API fuzzing command
        if endpoints:
            # Test specific endpoints
            results = []
            for endpoint in endpoints:
                for method in methods:
                    test_url = f"{base_url.rstrip('/')}/{endpoint.lstrip('/')}"
                    command = f"curl -s -X {method} -w '%{{http_code}}|%{{size_download}}' '{test_url}'"
                    result = execute_command(command, use_cache=False)
                    results.append({
                        "endpoint": endpoint,
                        "method": method,
                        "result": result
                    })

            logger.info(f"[?] API endpoint testing completed for {len(endpoints)} endpoints")
            return jsonify({
                "success": True,
                "fuzzing_type": "endpoint_testing",
                "results": results
            })
        else:
            # Discover endpoints using wordlist
            command = f"ffuf -u {base_url}/FUZZ -w {wordlist} -mc 200,201,202,204,301,302,307,401,403,405 -t 50"

            logger.info(f"[?] Starting API endpoint discovery: {base_url}")
            result = execute_command(command)
            logger.info(f"[#] API endpoint discovery completed")

            return jsonify({
                "success": True,
                "fuzzing_type": "endpoint_discovery",
                "result": result
            })

    except Exception as e:
        logger.error(f"[!!] Error in API fuzzer: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/graphql_scanner", methods=["POST"])
def graphql_scanner():
    # Advanced GraphQL security scanning and introspection
    try:
        params = request.json
        endpoint = params.get("endpoint", "")
        introspection = params.get("introspection", True)
        query_depth = params.get("query_depth", 10)
        mutations = params.get("test_mutations", True)

        if not endpoint:
            logger.warning("[W] GraphQL Scanner called without endpoint parameter")
            return jsonify({
                "error": "GraphQL endpoint parameter is required"
            }), 400

        logger.info(f"[?] Starting GraphQL security scan: {endpoint}")

        results = {
            "endpoint": endpoint,
            "tests_performed": [],
            "vulnerabilities": [],
            "recommendations": []
        }

        # Test 1: Introspection query
        if introspection:
            introspection_query = '''
            {
                __schema {
                    types {
                        name
                        fields {
                            name
                            type {
                                name
                            }
                        }
                    }
                }
            }
            '''

            clean_query = introspection_query.replace('\n', ' ').replace('  ', ' ').strip()
            command = f"curl -s -X POST -H 'Content-Type: application/json' -d '{{\"query\":\"{clean_query}\"}}' '{endpoint}'"
            result = execute_command(command, use_cache=False)

            results["tests_performed"].append("introspection_query")

            if "data" in result.get("stdout", ""):
                results["vulnerabilities"].append({
                    "type": "introspection_enabled",
                    "severity": "MEDIUM",
                    "description": "GraphQL introspection is enabled"
                })

        # Test 2: Query depth analysis
        deep_query = "{ " * query_depth + "field" + " }" * query_depth
        command = f"curl -s -X POST -H 'Content-Type: application/json' -d '{{\"query\":\"{deep_query}\"}}' {endpoint}"
        depth_result = execute_command(command, use_cache=False)

        results["tests_performed"].append("query_depth_analysis")

        if "error" not in depth_result.get("stdout", "").lower():
            results["vulnerabilities"].append({
                "type": "no_query_depth_limit",
                "severity": "HIGH",
                "description": f"No query depth limiting detected (tested depth: {query_depth})"
            })

        # Test 3: Batch query testing
        batch_query = '[' + ','.join(['{\"query\":\"{field}\"}' for _ in range(10)]) + ']'
        command = f"curl -s -X POST -H 'Content-Type: application/json' -d '{batch_query}' {endpoint}"
        batch_result = execute_command(command, use_cache=False)

        results["tests_performed"].append("batch_query_testing")

        if "data" in batch_result.get("stdout", "") and batch_result.get("success"):
            results["vulnerabilities"].append({
                "type": "batch_queries_allowed",
                "severity": "MEDIUM",
                "description": "Batch queries are allowed without rate limiting"
            })

        # Generate recommendations
        if results["vulnerabilities"]:
            results["recommendations"] = [
                "Disable introspection in production",
                "Implement query depth limiting",
                "Add rate limiting for batch queries",
                "Implement query complexity analysis",
                "Add authentication for sensitive operations"
            ]

        logger.info(f"[#] GraphQL scan completed | Vulnerabilities found: {len(results['vulnerabilities'])}")

        return jsonify({
            "success": True,
            "graphql_scan_results": results
        })

    except Exception as e:
        logger.error(f"[!!] Error in GraphQL scanner: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/jwt_analyzer", methods=["POST"])
def jwt_analyzer():
    # Advanced JWT token analysis and vulnerability testing
    try:
        params = request.json
        jwt_token = params.get("jwt_token", "")
        target_url = params.get("target_url", "")

        if not jwt_token:
            logger.warning("🔐 JWT Analyzer called without jwt_token parameter")
            return jsonify({
                "error": "JWT token parameter is required"
            }), 400

        logger.info(f"[?] Starting JWT security analysis")

        results = {
            "token": jwt_token[:50] + "..." if len(jwt_token) > 50 else jwt_token,
            "vulnerabilities": [],
            "token_info": {},
            "attack_vectors": []
        }

        # Decode JWT header and payload (basic analysis)
        try:
            parts = jwt_token.split('.')
            if len(parts) >= 2:
                # Decode header
                import base64
                import json

                # Add padding if needed
                header_b64 = parts[0] + '=' * (4 - len(parts[0]) % 4)
                payload_b64 = parts[1] + '=' * (4 - len(parts[1]) % 4)

                try:
                    header = json.loads(base64.b64decode(header_b64))
                    payload = json.loads(base64.b64decode(payload_b64))

                    results["token_info"] = {
                        "header": header,
                        "payload": payload,
                        "algorithm": header.get("alg", "unknown")
                    }

                    # Check for vulnerabilities
                    algorithm = header.get("alg", "").lower()

                    if algorithm == "none":
                        results["vulnerabilities"].append({
                            "type": "none_algorithm",
                            "severity": "CRITICAL",
                            "description": "JWT uses 'none' algorithm - no signature verification"
                        })

                    if algorithm in ["hs256", "hs384", "hs512"]:
                        results["attack_vectors"].append("hmac_key_confusion")
                        results["vulnerabilities"].append({
                            "type": "hmac_algorithm",
                            "severity": "MEDIUM",
                            "description": "HMAC algorithm detected - vulnerable to key confusion attacks"
                        })

                    # Check token expiration
                    exp = payload.get("exp")
                    if not exp:
                        results["vulnerabilities"].append({
                            "type": "no_expiration",
                            "severity": "HIGH",
                            "description": "JWT token has no expiration time"
                        })

                except Exception as decode_error:
                    results["vulnerabilities"].append({
                        "type": "malformed_token",
                        "severity": "HIGH",
                        "description": f"Token decoding failed: {str(decode_error)}"
                    })

        except Exception as e:
            results["vulnerabilities"].append({
                "type": "invalid_format",
                "severity": "HIGH",
                "description": "Invalid JWT token format"
            })

        # Test token manipulation if target URL provided
        if target_url:
            # Test none algorithm attack
            none_token_parts = jwt_token.split('.')
            if len(none_token_parts) >= 2:
                # Create none algorithm token
                none_header = base64.b64encode('{"alg":"none","typ":"JWT"}'.encode()).decode().rstrip('=')
                none_token = f"{none_header}.{none_token_parts[1]}."

                command = f"curl -s -H 'Authorization: Bearer {none_token}' '{target_url}'"
                none_result = execute_command(command, use_cache=False)

                if "200" in none_result.get("stdout", "") or "success" in none_result.get("stdout", "").lower():
                    results["vulnerabilities"].append({
                        "type": "none_algorithm_accepted",
                        "severity": "CRITICAL",
                        "description": "Server accepts tokens with 'none' algorithm"
                    })

        logger.info(f"[#] JWT analysis completed | Vulnerabilities found: {len(results['vulnerabilities'])}")

        return jsonify({
            "success": True,
            "jwt_analysis_results": results
        })

    except Exception as e:
        logger.error(f"[!!] Error in JWT analyzer: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/api_schema_analyzer", methods=["POST"])
def api_schema_analyzer():
    # Analyze API schemas and identify potential security issues
    try:
        params = request.json
        schema_url = params.get("schema_url", "")
        schema_type = params.get("schema_type", "openapi")  # openapi, swagger, graphql

        if not schema_url:
            logger.warning("[=] API Schema Analyzer called without schema_url parameter")
            return jsonify({
                "error": "Schema URL parameter is required"
            }), 400

        logger.info(f"[?] Starting API schema analysis: {schema_url}")

        # Fetch schema
        command = f"curl -s '{schema_url}'"
        result = execute_command(command, use_cache=True)

        if not result.get("success"):
            return jsonify({
                "error": "Failed to fetch API schema"
            }), 400

        schema_content = result.get("stdout", "")

        analysis_results = {
            "schema_url": schema_url,
            "schema_type": schema_type,
            "endpoints_found": [],
            "security_issues": [],
            "recommendations": []
        }

        # Parse schema based on type
        try:
            import json
            schema_data = json.loads(schema_content)

            if schema_type.lower() in ["openapi", "swagger"]:
                # OpenAPI/Swagger analysis
                paths = schema_data.get("paths", {})

                for path, methods in paths.items():
                    for method, details in methods.items():
                        if isinstance(details, dict):
                            endpoint_info = {
                                "path": path,
                                "method": method.upper(),
                                "summary": details.get("summary", ""),
                                "parameters": details.get("parameters", []),
                                "security": details.get("security", [])
                            }
                            analysis_results["endpoints_found"].append(endpoint_info)

                            # Check for security issues
                            if not endpoint_info["security"]:
                                analysis_results["security_issues"].append({
                                    "endpoint": f"{method.upper()} {path}",
                                    "issue": "no_authentication",
                                    "severity": "MEDIUM",
                                    "description": "Endpoint has no authentication requirements"
                                })

                            # Check for sensitive data in parameters
                            for param in endpoint_info["parameters"]:
                                param_name = param.get("name", "").lower()
                                if any(sensitive in param_name for sensitive in ["password", "token", "key", "secret"]):
                                    analysis_results["security_issues"].append({
                                        "endpoint": f"{method.upper()} {path}",
                                        "issue": "sensitive_parameter",
                                        "severity": "HIGH",
                                        "description": f"Sensitive parameter detected: {param_name}"
                                    })

            # Generate recommendations
            if analysis_results["security_issues"]:
                analysis_results["recommendations"] = [
                    "Implement authentication for all endpoints",
                    "Use HTTPS for all API communications",
                    "Validate and sanitize all input parameters",
                    "Implement rate limiting",
                    "Add proper error handling",
                    "Use secure headers (CORS, CSP, etc.)"
                ]

        except json.JSONDecodeError:
            analysis_results["security_issues"].append({
                "endpoint": "schema",
                "issue": "invalid_json",
                "severity": "HIGH",
                "description": "Schema is not valid JSON"
            })

        logger.info(f"[#] Schema analysis completed | Issues found: {len(analysis_results['security_issues'])}")

        return jsonify({
            "success": True,
            "schema_analysis_results": analysis_results
        })

    except Exception as e:
        logger.error(f"[!!] Error in API schema analyzer: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

# ============================================================================
# ADVANCED CTF TOOLS (v5.0 ENHANCEMENT)
# ============================================================================

@app.route("/api/tools/volatility3", methods=["POST"])
def volatility3():
    # Execute Volatility3 for advanced memory forensics with enhanced logging
    try:
        params = request.json
        memory_file = params.get("memory_file", "")
        plugin = params.get("plugin", "")
        output_file = params.get("output_file", "")
        additional_args = params.get("additional_args", "")

        if not memory_file:
            logger.warning("🧠 Volatility3 called without memory_file parameter")
            return jsonify({
                "error": "Memory file parameter is required"
            }), 400

        if not plugin:
            logger.warning("🧠 Volatility3 called without plugin parameter")
            return jsonify({
                "error": "Plugin parameter is required"
            }), 400

        command = f"vol.py -f {memory_file} {plugin}"

        if output_file:
            command += f" -o {output_file}"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🧠 Starting Volatility3 analysis: {plugin}")
        result = execute_command(command)
        logger.info(f"[#] Volatility3 analysis completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in volatility3 endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/foremost", methods=["POST"])
def foremost():
    # Execute Foremost for file carving with enhanced logging
    try:
        params = request.json
        input_file = params.get("input_file", "")
        output_dir = params.get("output_dir", "/tmp/foremost_output")
        file_types = params.get("file_types", "")
        additional_args = params.get("additional_args", "")

        if not input_file:
            logger.warning("📁 Foremost called without input_file parameter")
            return jsonify({
                "error": "Input file parameter is required"
            }), 400

        # Ensure output directory exists
        Path(output_dir).mkdir(parents=True, exist_ok=True)

        command = f"foremost -o {output_dir}"

        if file_types:
            command += f" -t {file_types}"

        if additional_args:
            command += f" {additional_args}"

        command += f" {input_file}"

        logger.info(f"📁 Starting Foremost file carving: {input_file}")
        result = execute_command(command)
        result["output_directory"] = output_dir
        logger.info(f"[#] Foremost carving completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in foremost endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/steghide", methods=["POST"])
def steghide():
    # Execute Steghide for steganography analysis with enhanced logging
    try:
        params = request.json
        action = params.get("action", "extract")  # extract, embed, info
        cover_file = params.get("cover_file", "")
        embed_file = params.get("embed_file", "")
        passphrase = params.get("passphrase", "")
        output_file = params.get("output_file", "")
        additional_args = params.get("additional_args", "")

        if not cover_file:
            logger.warning("🖼️ Steghide called without cover_file parameter")
            return jsonify({
                "error": "Cover file parameter is required"
            }), 400

        if action == "extract":
            command = f"steghide extract -sf {cover_file}"
            if output_file:
                command += f" -xf {output_file}"
        elif action == "embed":
            if not embed_file:
                return jsonify({"error": "Embed file required for embed action"}), 400
            command = f"steghide embed -cf {cover_file} -ef {embed_file}"
        elif action == "info":
            command = f"steghide info {cover_file}"
        else:
            return jsonify({"error": "Invalid action. Use: extract, embed, info"}), 400

        if passphrase:
            command += f" -p {passphrase}"
        else:
            command += " -p ''"  # Empty passphrase

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🖼️ Starting Steghide {action}: {cover_file}")
        result = execute_command(command)
        logger.info(f"[#] Steghide {action} completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in steghide endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/exiftool", methods=["POST"])
def exiftool():
    # Execute ExifTool for metadata extraction with enhanced logging
    try:
        params = request.json
        file_path = params.get("file_path", "")
        output_format = params.get("output_format", "")  # json, xml, csv
        tags = params.get("tags", "")
        additional_args = params.get("additional_args", "")

        if not file_path:
            logger.warning("📷 ExifTool called without file_path parameter")
            return jsonify({
                "error": "File path parameter is required"
            }), 400

        command = f"exiftool"

        if output_format:
            command += f" -{output_format}"

        if tags:
            command += f" -{tags}"

        if additional_args:
            command += f" {additional_args}"

        command += f" {file_path}"

        logger.info(f"📷 Starting ExifTool analysis: {file_path}")
        result = execute_command(command)
        logger.info(f"[#] ExifTool analysis completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in exiftool endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/tools/hashpump", methods=["POST"])
def hashpump():
    # Execute HashPump for hash length extension attacks with enhanced logging
    try:
        params = request.json
        signature = params.get("signature", "")
        data = params.get("data", "")
        key_length = params.get("key_length", "")
        append_data = params.get("append_data", "")
        additional_args = params.get("additional_args", "")

        if not all([signature, data, key_length, append_data]):
            logger.warning("🔐 HashPump called without required parameters")
            return jsonify({
                "error": "Signature, data, key_length, and append_data parameters are required"
            }), 400

        command = f"hashpump -s {signature} -d '{data}' -k {key_length} -a '{append_data}'"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🔐 Starting HashPump attack")
        result = execute_command(command)
        logger.info(f"[#] HashPump attack completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in hashpump endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

# ============================================================================
# BUG BOUNTY RECONNAISSANCE TOOLS (v5.0 ENHANCEMENT)
# ============================================================================

@app.route("/api/tools/hakrawler", methods=["POST"])
def hakrawler():
    # Execute Hakrawler for web endpoint discovery with enhanced logging
    #
    # Note: This implementation uses the standard Kali Linux hakrawler (hakluke/hakrawler)
    # command line arguments, NOT the Elsfa7-110 fork. The standard version uses:
    # - echo URL | hakrawler (stdin input)
    # - -d for depth (not -depth)
    # - -s for showing sources (not -forms)
    # - -u for unique URLs
    # - -subs for subdomain inclusion
    try:
        params = request.json
        url = params.get("url", "")
        depth = params.get("depth", 2)
        forms = params.get("forms", True)
        robots = params.get("robots", True)
        sitemap = params.get("sitemap", True)
        wayback = params.get("wayback", False)
        additional_args = params.get("additional_args", "")

        if not url:
            logger.warning("🕷️ Hakrawler called without URL parameter")
            return jsonify({
                "error": "URL parameter is required"
            }), 400

        # Build command for standard Kali Linux hakrawler (hakluke version)
        command = f"echo '{url}' | hakrawler -d {depth}"

        if forms:
            command += " -s"  # Show sources (includes forms)
        if robots or sitemap or wayback:
            command += " -subs"  # Include subdomains for better coverage

        # Add unique URLs flag for cleaner output
        command += " -u"

        if additional_args:
            command += f" {additional_args}"

        logger.info(f"🕷️ Starting Hakrawler crawling: {url}")
        result = execute_command(command)
        logger.info(f"[#] Hakrawler crawling completed")
        return jsonify(result)
    except Exception as e:
        logger.error(f"[!!] Error in hakrawler endpoint: {str(e)}")
        return jsonify({
            "error": f"Server error: {str(e)}"
        }), 500

# ============================================================================
# ADVANCED VULNERABILITY INTELLIGENCE API ENDPOINTS (v6.0 ENHANCEMENT)
# ============================================================================

@app.route("/api/vuln-intel/cve-monitor", methods=["POST"])
def cve_monitor():
    # Monitor CVE databases for new vulnerabilities with AI analysis
    try:
        params = request.json
        hours = params.get("hours", 24)
        severity_filter = params.get("severity_filter", "HIGH,CRITICAL")
        keywords = params.get("keywords", "")

        logger.info(f"[?] Monitoring CVE feeds for last {hours} hours with severity filter: {severity_filter}")

        # Fetch latest CVEs
        cve_results = cve_intelligence.fetch_latest_cves(hours, severity_filter)

        # Filter by keywords if provided
        if keywords and cve_results.get("success"):
            keyword_list = [k.strip().lower() for k in keywords.split(",")]
            filtered_cves = []

            for cve in cve_results.get("cves", []):
                description = cve.get("description", "").lower()
                if any(keyword in description for keyword in keyword_list):
                    filtered_cves.append(cve)

            cve_results["cves"] = filtered_cves
            cve_results["filtered_by_keywords"] = keywords
            cve_results["total_after_filter"] = len(filtered_cves)

        # Analyze exploitability for top CVEs
        exploitability_analysis = []
        for cve in cve_results.get("cves", [])[:5]:  # Analyze top 5 CVEs
            cve_id = cve.get("cve_id", "")
            if cve_id:
                analysis = cve_intelligence.analyze_cve_exploitability(cve_id)
                if analysis.get("success"):
                    exploitability_analysis.append(analysis)

        result = {
            "success": True,
            "cve_monitoring": cve_results,
            "exploitability_analysis": exploitability_analysis,
            "timestamp": datetime.now().isoformat()
        }

        logger.info(f"[#] CVE monitoring completed | Found: {len(cve_results.get('cves', []))} CVEs")
        return jsonify(result)

    except Exception as e:
        logger.error(f"[!!] Error in CVE monitoring: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/vuln-intel/exploit-generate", methods=["POST"])
def exploit_generate():
    # Generate exploits from vulnerability data using AI
    try:
        params = request.json
        cve_id = params.get("cve_id", "")
        target_os = params.get("target_os", "")
        target_arch = params.get("target_arch", "x64")
        exploit_type = params.get("exploit_type", "poc")
        evasion_level = params.get("evasion_level", "none")

        # Additional target context
        target_info = {
            "target_os": target_os,
            "target_arch": target_arch,
            "exploit_type": exploit_type,
            "evasion_level": evasion_level,
            "target_ip": params.get("target_ip", "192.168.1.100"),
            "target_port": params.get("target_port", 80),
            "description": params.get("target_description", f"Target for {cve_id}")
        }

        if not cve_id:
            logger.warning("🤖 Exploit generation called without CVE ID")
            return jsonify({
                "success": False,
                "error": "CVE ID parameter is required"
            }), 400

        logger.info(f"🤖 Generating exploit for {cve_id} | Target: {target_os} {target_arch}")

        # First analyze the CVE for context
        cve_analysis = cve_intelligence.analyze_cve_exploitability(cve_id)

        if not cve_analysis.get("success"):
            return jsonify({
                "success": False,
                "error": f"Failed to analyze CVE {cve_id}: {cve_analysis.get('error', 'Unknown error')}"
            }), 400

        # Prepare CVE data for exploit generation
        cve_data = {
            "cve_id": cve_id,
            "description": f"Vulnerability analysis for {cve_id}",
            "exploitability_level": cve_analysis.get("exploitability_level", "UNKNOWN"),
            "exploitability_score": cve_analysis.get("exploitability_score", 0)
        }

        # Generate exploit
        exploit_result = exploit_generator.generate_exploit_from_cve(cve_data, target_info)

        # Search for existing exploits for reference
        existing_exploits = cve_intelligence.search_existing_exploits(cve_id)

        result = {
            "success": True,
            "cve_analysis": cve_analysis,
            "exploit_generation": exploit_result,
            "existing_exploits": existing_exploits,
            "target_info": target_info,
            "timestamp": datetime.now().isoformat()
        }

        logger.info(f"[>] Exploit generation completed for {cve_id}")
        return jsonify(result)

    except Exception as e:
        logger.error(f"[!!] Error in exploit generation: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/vuln-intel/attack-chains", methods=["POST"])
def discover_attack_chains():
    # Discover multi-stage attack possibilities
    try:
        params = request.json
        target_software = params.get("target_software", "")
        attack_depth = params.get("attack_depth", 3)
        include_zero_days = params.get("include_zero_days", False)

        if not target_software:
            logger.warning("🔗 Attack chain discovery called without target software")
            return jsonify({
                "success": False,
                "error": "Target software parameter is required"
            }), 400

        logger.info(f"🔗 Discovering attack chains for {target_software} | Depth: {attack_depth}")

        # Discover attack chains
        chain_results = vulnerability_correlator.find_attack_chains(target_software, attack_depth)

        # Enhance with exploit generation for viable chains
        if chain_results.get("success") and chain_results.get("attack_chains"):
            enhanced_chains = []

            for chain in chain_results["attack_chains"][:2]:  # Enhance top 2 chains
                enhanced_chain = chain.copy()
                enhanced_stages = []

                for stage in chain["stages"]:
                    enhanced_stage = stage.copy()

                    # Try to generate exploit for this stage
                    vuln = stage.get("vulnerability", {})
                    cve_id = vuln.get("cve_id", "")

                    if cve_id:
                        try:
                            cve_data = {"cve_id": cve_id, "description": vuln.get("description", "")}
                            target_info = {"target_os": "linux", "target_arch": "x64", "evasion_level": "basic"}

                            exploit_result = exploit_generator.generate_exploit_from_cve(cve_data, target_info)
                            enhanced_stage["exploit_available"] = exploit_result.get("success", False)

                            if exploit_result.get("success"):
                                enhanced_stage["exploit_code"] = exploit_result.get("exploit_code", "")[:500] + "..."
                        except:
                            enhanced_stage["exploit_available"] = False

                    enhanced_stages.append(enhanced_stage)

                enhanced_chain["stages"] = enhanced_stages
                enhanced_chains.append(enhanced_chain)

            chain_results["enhanced_chains"] = enhanced_chains

        result = {
            "success": True,
            "attack_chain_discovery": chain_results,
            "parameters": {
                "target_software": target_software,
                "attack_depth": attack_depth,
                "include_zero_days": include_zero_days
            },
            "timestamp": datetime.now().isoformat()
        }

        logger.info(f"[>] Attack chain discovery completed | Found: {len(chain_results.get('attack_chains', []))} chains")
        return jsonify(result)

    except Exception as e:
        logger.error(f"[!!] Error in attack chain discovery: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/vuln-intel/threat-feeds", methods=["POST"])
def threat_intelligence_feeds():
    # Aggregate and correlate threat intelligence from multiple sources
    try:
        params = request.json
        indicators = params.get("indicators", [])
        timeframe = params.get("timeframe", "30d")
        sources = params.get("sources", "all")

        if isinstance(indicators, str):
            indicators = [i.strip() for i in indicators.split(",")]

        if not indicators:
            logger.warning("🧠 Threat intelligence called without indicators")
            return jsonify({
                "success": False,
                "error": "Indicators parameter is required"
            }), 400

        logger.info(f"🧠 Correlating threat intelligence for {len(indicators)} indicators")

        correlation_results = {
            "indicators_analyzed": indicators,
            "timeframe": timeframe,
            "sources": sources,
            "correlations": [],
            "threat_score": 0,
            "recommendations": []
        }

        # Analyze each indicator
        cve_indicators = [i for i in indicators if i.startswith("CVE-")]
        ip_indicators = [i for i in indicators if i.replace(".", "").isdigit()]
        hash_indicators = [i for i in indicators if len(i) in [32, 40, 64] and all(c in "0123456789abcdef" for c in i.lower())]

        # Process CVE indicators
        for cve_id in cve_indicators:
            try:
                cve_analysis = cve_intelligence.analyze_cve_exploitability(cve_id)
                if cve_analysis.get("success"):
                    correlation_results["correlations"].append({
                        "indicator": cve_id,
                        "type": "cve",
                        "analysis": cve_analysis,
                        "threat_level": cve_analysis.get("exploitability_level", "UNKNOWN")
                    })

                    # Add to threat score
                    exploit_score = cve_analysis.get("exploitability_score", 0)
                    correlation_results["threat_score"] += min(exploit_score, 100)

                # Search for existing exploits
                exploits = cve_intelligence.search_existing_exploits(cve_id)
                if exploits.get("success") and exploits.get("total_exploits", 0) > 0:
                    correlation_results["correlations"].append({
                        "indicator": cve_id,
                        "type": "exploit_availability",
                        "exploits_found": exploits.get("total_exploits", 0),
                        "threat_level": "HIGH"
                    })
                    correlation_results["threat_score"] += 25

            except Exception as e:
                logger.warning(f"Error analyzing CVE {cve_id}: {str(e)}")

        # Process IP indicators (basic reputation check simulation)
        for ip in ip_indicators:
            # Simulate threat intelligence lookup
            correlation_results["correlations"].append({
                "indicator": ip,
                "type": "ip_reputation",
                "analysis": {
                    "reputation": "unknown",
                    "geolocation": "unknown",
                    "associated_threats": []
                },
                "threat_level": "MEDIUM"  # Default for unknown IPs
            })

        # Process hash indicators
        for hash_val in hash_indicators:
            correlation_results["correlations"].append({
                "indicator": hash_val,
                "type": "file_hash",
                "analysis": {
                    "hash_type": f"hash{len(hash_val)}",
                    "malware_family": "unknown",
                    "detection_rate": "unknown"
                },
                "threat_level": "MEDIUM"
            })

        # Calculate overall threat score and generate recommendations
        total_indicators = len(indicators)
        if total_indicators > 0:
            correlation_results["threat_score"] = min(correlation_results["threat_score"] / total_indicators, 100)

            if correlation_results["threat_score"] >= 75:
                correlation_results["recommendations"] = [
                    "Immediate threat response required",
                    "Block identified indicators",
                    "Enhance monitoring for related IOCs",
                    "Implement emergency patches for identified CVEs"
                ]
            elif correlation_results["threat_score"] >= 50:
                correlation_results["recommendations"] = [
                    "Elevated threat level detected",
                    "Increase monitoring for identified indicators",
                    "Plan patching for identified vulnerabilities",
                    "Review security controls"
                ]
            else:
                correlation_results["recommendations"] = [
                    "Low to medium threat level",
                    "Continue standard monitoring",
                    "Plan routine patching",
                    "Consider additional threat intelligence sources"
                ]

        result = {
            "success": True,
            "threat_intelligence": correlation_results,
            "timestamp": datetime.now().isoformat()
        }

        logger.info(f"[>] Threat intelligence correlation completed | Threat Score: {correlation_results['threat_score']:.1f}")
        return jsonify(result)

    except Exception as e:
        logger.error(f"[!!] Error in threat intelligence: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/vuln-intel/zero-day-research", methods=["POST"])
def zero_day_research():
    # Automated zero-day vulnerability research using AI analysis
    try:
        params = request.json
        target_software = params.get("target_software", "")
        analysis_depth = params.get("analysis_depth", "standard")
        source_code_url = params.get("source_code_url", "")

        if not target_software:
            logger.warning("🔬 Zero-day research called without target software")
            return jsonify({
                "success": False,
                "error": "Target software parameter is required"
            }), 400

        logger.info(f"🔬 Starting zero-day research for {target_software} | Depth: {analysis_depth}")

        research_results = {
            "target_software": target_software,
            "analysis_depth": analysis_depth,
            "research_areas": [],
            "potential_vulnerabilities": [],
            "risk_assessment": {},
            "recommendations": []
        }

        # Define research areas based on software type
        common_research_areas = [
            "Input validation vulnerabilities",
            "Memory corruption issues",
            "Authentication bypasses",
            "Authorization flaws",
            "Cryptographic weaknesses",
            "Race conditions",
            "Logic flaws"
        ]

        # Software-specific research areas
        web_research_areas = [
            "Cross-site scripting (XSS)",
            "SQL injection",
            "Server-side request forgery (SSRF)",
            "Insecure deserialization",
            "Template injection"
        ]

        system_research_areas = [
            "Buffer overflows",
            "Privilege escalation",
            "Kernel vulnerabilities",
            "Service exploitation",
            "Configuration weaknesses"
        ]

        # Determine research areas based on target
        target_lower = target_software.lower()
        if any(web_tech in target_lower for web_tech in ["apache", "nginx", "tomcat", "php", "node", "django"]):
            research_results["research_areas"] = common_research_areas + web_research_areas
        elif any(sys_tech in target_lower for sys_tech in ["windows", "linux", "kernel", "driver"]):
            research_results["research_areas"] = common_research_areas + system_research_areas
        else:
            research_results["research_areas"] = common_research_areas

        # Simulate vulnerability discovery based on analysis depth
        vuln_count = {"quick": 2, "standard": 4, "comprehensive": 6}.get(analysis_depth, 4)

        for i in range(vuln_count):
            potential_vuln = {
                "id": f"RESEARCH-{target_software.upper()}-{i+1:03d}",
                "category": research_results["research_areas"][i % len(research_results["research_areas"])],
                "severity": ["LOW", "MEDIUM", "HIGH", "CRITICAL"][i % 4],
                "confidence": ["LOW", "MEDIUM", "HIGH"][i % 3],
                "description": f"Potential {research_results['research_areas'][i % len(research_results['research_areas'])].lower()} in {target_software}",
                "attack_vector": "To be determined through further analysis",
                "impact": "To be assessed",
                "proof_of_concept": "Research phase - PoC development needed"
            }
            research_results["potential_vulnerabilities"].append(potential_vuln)

        # Risk assessment
        high_risk_count = sum(1 for v in research_results["potential_vulnerabilities"] if v["severity"] in ["HIGH", "CRITICAL"])
        total_vulns = len(research_results["potential_vulnerabilities"])

        research_results["risk_assessment"] = {
            "total_areas_analyzed": len(research_results["research_areas"]),
            "potential_vulnerabilities_found": total_vulns,
            "high_risk_findings": high_risk_count,
            "risk_score": min((high_risk_count * 25 + (total_vulns - high_risk_count) * 10), 100),
            "research_confidence": analysis_depth
        }

        # Generate recommendations
        if high_risk_count > 0:
            research_results["recommendations"] = [
                "Prioritize security testing in identified high-risk areas",
                "Conduct focused penetration testing",
                "Implement additional security controls",
                "Consider bug bounty program for target software",
                "Perform code review in identified areas"
            ]
        else:
            research_results["recommendations"] = [
                "Continue standard security testing",
                "Monitor for new vulnerability research",
                "Implement defense-in-depth strategies",
                "Regular security assessments recommended"
            ]

        # Source code analysis simulation
        if source_code_url:
            research_results["source_code_analysis"] = {
                "repository_url": source_code_url,
                "analysis_status": "simulated",
                "findings": [
                    "Static analysis patterns identified",
                    "Potential code quality issues detected",
                    "Security-relevant functions located"
                ],
                "recommendation": "Manual code review recommended for identified areas"
            }

        result = {
            "success": True,
            "zero_day_research": research_results,
            "disclaimer": "This is simulated research for demonstration. Real zero-day research requires extensive manual analysis.",
            "timestamp": datetime.now().isoformat()
        }

        logger.info(f"[>] Zero-day research completed | Risk Score: {research_results['risk_assessment']['risk_score']}")
        return jsonify(result)

    except Exception as e:
        logger.error(f"[!!] Error in zero-day research: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"Server error: {str(e)}"
        }), 500

@app.route("/api/ai/advanced-payload-generation", methods=["POST"])
def advanced_payload_generation():
    # Generate advanced payloads with AI-powered evasion techniques
    try:
        params = request.json
        attack_type = params.get("attack_type", "rce")
        target_context = params.get("target_context", "")
        evasion_level = params.get("evasion_level", "standard")
        custom_constraints = params.get("custom_constraints", "")

        if not attack_type:
            logger.warning("[>] Advanced payload generation called without attack type")
            return jsonify({
                "success": False,
                "error": "Attack type parameter is required"
            }), 400

        logger.info(f"[>] Generating advanced {attack_type} payload with {evasion_level} evasion")

        # Enhanced payload generation with contextual AI
        target_info = {
            "attack_type": attack_type,
            "complexity": "advanced",
            "technology": target_context,
            "evasion_level": evasion_level,
            "constraints": custom_constraints
        }

        # Generate base payloads using existing AI system
        base_result = ai_payload_generator.generate_contextual_payload(target_info)

        # Enhance with advanced techniques
        advanced_payloads = []

        for payload_info in base_result.get("payloads", [])[:10]:  # Limit to 10 advanced payloads
            enhanced_payload = {
                "payload": payload_info["payload"],
                "original_context": payload_info["context"],
                "risk_level": payload_info["risk_level"],
                "evasion_techniques": [],
                "deployment_methods": []
            }

            # Apply evasion techniques based on level
            if evasion_level in ["advanced", "nation-state"]:
                # Advanced encoding techniques
                encoded_variants = [
                    {
                        "technique": "Double URL Encoding",
                        "payload": payload_info["payload"].replace("%", "%25").replace(" ", "%2520")
                    },
                    {
                        "technique": "Unicode Normalization",
                        "payload": payload_info["payload"].replace("script", "scr\u0131pt")
                    },
                    {
                        "technique": "Case Variation",
                        "payload": "".join(c.upper() if i % 2 else c.lower() for i, c in enumerate(payload_info["payload"]))
                    }
                ]
                enhanced_payload["evasion_techniques"].extend(encoded_variants)

            if evasion_level == "nation-state":
                # Nation-state level techniques
                advanced_techniques = [
                    {
                        "technique": "Polyglot Payload",
                        "payload": f"/*{payload_info['payload']}*/ OR {payload_info['payload']}"
                    },
                    {
                        "technique": "Time-delayed Execution",
                        "payload": f"setTimeout(function(){{{payload_info['payload']}}}, 1000)"
                    },
                    {
                        "technique": "Environmental Keying",
                        "payload": f"if(navigator.userAgent.includes('specific')){{ {payload_info['payload']} }}"
                    }
                ]
                enhanced_payload["evasion_techniques"].extend(advanced_techniques)

            # Deployment methods
            enhanced_payload["deployment_methods"] = [
                "Direct injection",
                "Parameter pollution",
                "Header injection",
                "Cookie manipulation",
                "Fragment-based delivery"
            ]

            advanced_payloads.append(enhanced_payload)

        # Generate deployment instructions
        deployment_guide = {
            "pre_deployment": [
                "Reconnaissance of target environment",
                "Identification of input validation mechanisms",
                "Analysis of security controls (WAF, IDS, etc.)",
                "Selection of appropriate evasion techniques"
            ],
            "deployment": [
                "Start with least detectable payloads",
                "Monitor for defensive responses",
                "Escalate evasion techniques as needed",
                "Document successful techniques for future use"
            ],
            "post_deployment": [
                "Monitor for payload execution",
                "Clean up traces if necessary",
                "Document findings",
                "Report vulnerabilities responsibly"
            ]
        }

        result = {
            "success": True,
            "advanced_payload_generation": {
                "attack_type": attack_type,
                "evasion_level": evasion_level,
                "target_context": target_context,
                "payload_count": len(advanced_payloads),
                "advanced_payloads": advanced_payloads,
                "deployment_guide": deployment_guide,
                "custom_constraints_applied": custom_constraints if custom_constraints else "none"
            },
            "disclaimer": "These payloads are for authorized security testing only. Ensure proper authorization before use.",
            "timestamp": datetime.now().isoformat()
        }

        logger.info(f"[>] Advanced payload generation completed | Generated: {len(advanced_payloads)} payloads")
        return jsonify(result)

    except Exception as e:
        logger.error(f"[!!] Error in advanced payload generation: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"Server error: {str(e)}"
        }), 500

# ============================================================================
# CTF COMPETITION EXCELLENCE FRAMEWORK API ENDPOINTS (v8.0 ENHANCEMENT)
# ============================================================================

@app.route("/api/ctf/create-challenge-workflow", methods=["POST"])
def create_ctf_challenge_workflow():
    # Create specialized workflow for CTF challenge
    try:
        params = request.json
        challenge_name = params.get("name", "")
        category = params.get("category", "misc")
        difficulty = params.get("difficulty", "unknown")
        points = params.get("points", 100)
        description = params.get("description", "")
        target = params.get("target", "")

        if not challenge_name:
            return jsonify({"error": "Challenge name is required"}), 400

        # Create CTF challenge object
        challenge = CTFChallenge(
            name=challenge_name,
            category=category,
            difficulty=difficulty,
            points=points,
            description=description,
            target=target
        )

        # Generate workflow
        workflow = ctf_manager.create_ctf_challenge_workflow(challenge)

        logger.info(f"[>] CTF workflow created for {challenge_name} | Category: {category} | Difficulty: {difficulty}")
        return jsonify({
            "success": True,
            "workflow": workflow,
            "challenge": challenge.to_dict(),
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error creating CTF workflow: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/ctf/auto-solve-challenge", methods=["POST"])
def auto_solve_ctf_challenge():
    # Attempt to automatically solve a CTF challenge
    try:
        params = request.json
        challenge_name = params.get("name", "")
        category = params.get("category", "misc")
        difficulty = params.get("difficulty", "unknown")
        points = params.get("points", 100)
        description = params.get("description", "")
        target = params.get("target", "")

        if not challenge_name:
            return jsonify({"error": "Challenge name is required"}), 400

        # Create CTF challenge object
        challenge = CTFChallenge(
            name=challenge_name,
            category=category,
            difficulty=difficulty,
            points=points,
            description=description,
            target=target
        )

        # Attempt automated solving
        result = ctf_automator.auto_solve_challenge(challenge)

        logger.info(f"🤖 CTF auto-solve attempted for {challenge_name} | Status: {result['status']}")
        return jsonify({
            "success": True,
            "solve_result": result,
            "challenge": challenge.to_dict(),
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error in CTF auto-solve: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/ctf/team-strategy", methods=["POST"])
def create_ctf_team_strategy():
    # Create optimal team strategy for CTF competition
    try:
        params = request.json
        challenges_data = params.get("challenges", [])
        team_skills = params.get("team_skills", {})

        if not challenges_data:
            return jsonify({"error": "Challenges data is required"}), 400

        # Convert challenge data to CTFChallenge objects
        challenges = []
        for challenge_data in challenges_data:
            challenge = CTFChallenge(
                name=challenge_data.get("name", ""),
                category=challenge_data.get("category", "misc"),
                difficulty=challenge_data.get("difficulty", "unknown"),
                points=challenge_data.get("points", 100),
                description=challenge_data.get("description", ""),
                target=challenge_data.get("target", "")
            )
            challenges.append(challenge)

        # Generate team strategy
        strategy = ctf_coordinator.optimize_team_strategy(challenges, team_skills)

        logger.info(f"👥 CTF team strategy created | Challenges: {len(challenges)} | Team members: {len(team_skills)}")
        return jsonify({
            "success": True,
            "strategy": strategy,
            "challenges_count": len(challenges),
            "team_size": len(team_skills),
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error creating CTF team strategy: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/ctf/suggest-tools", methods=["POST"])
def suggest_ctf_tools():
    # Suggest optimal tools for CTF challenge based on description and category
    try:
        params = request.json
        description = params.get("description", "")
        category = params.get("category", "misc")

        if not description:
            return jsonify({"error": "Challenge description is required"}), 400

        # Get tool suggestions
        suggested_tools = ctf_tools.suggest_tools_for_challenge(description, category)
        category_tools = ctf_tools.get_category_tools(f"{category}_recon")

        # Get tool commands
        tool_commands = {}
        for tool in suggested_tools:
            try:
                tool_commands[tool] = ctf_tools.get_tool_command(tool, "TARGET")
            except:
                tool_commands[tool] = f"{tool} TARGET"

        logger.info(f"[+] CTF tools suggested | Category: {category} | Tools: {len(suggested_tools)}")
        return jsonify({
            "success": True,
            "suggested_tools": suggested_tools,
            "category_tools": category_tools,
            "tool_commands": tool_commands,
            "category": category,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error suggesting CTF tools: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/ctf/cryptography-solver", methods=["POST"])
def ctf_cryptography_solver():
    # Advanced cryptography challenge solver with multiple attack methods
    try:
        params = request.json
        cipher_text = params.get("cipher_text", "")
        cipher_type = params.get("cipher_type", "unknown")
        key_hint = params.get("key_hint", "")
        known_plaintext = params.get("known_plaintext", "")
        additional_info = params.get("additional_info", "")

        if not cipher_text:
            return jsonify({"error": "Cipher text is required"}), 400

        results = {
            "cipher_text": cipher_text,
            "cipher_type": cipher_type,
            "analysis_results": [],
            "potential_solutions": [],
            "recommended_tools": [],
            "next_steps": []
        }

        # Cipher type identification
        if cipher_type == "unknown":
            # Basic cipher identification heuristics
            if re.match(r'^[0-9a-fA-F]+$', cipher_text.replace(' ', '')):
                results["analysis_results"].append("Possible hexadecimal encoding")
                results["recommended_tools"].extend(["hex", "xxd"])

            if re.match(r'^[A-Za-z0-9+/]+=*$', cipher_text.replace(' ', '')):
                results["analysis_results"].append("Possible Base64 encoding")
                results["recommended_tools"].append("base64")

            if len(set(cipher_text.upper().replace(' ', ''))) <= 26:
                results["analysis_results"].append("Possible substitution cipher")
                results["recommended_tools"].extend(["frequency-analysis", "substitution-solver"])

        # Hash identification
        hash_patterns = {
            32: "MD5",
            40: "SHA1",
            64: "SHA256",
            128: "SHA512"
        }

        clean_text = cipher_text.replace(' ', '').replace('\n', '')
        if len(clean_text) in hash_patterns and re.match(r'^[0-9a-fA-F]+$', clean_text):
            hash_type = hash_patterns[len(clean_text)]
            results["analysis_results"].append(f"Possible {hash_type} hash")
            results["recommended_tools"].extend(["hashcat", "john", "hash-identifier"])

        # Frequency analysis for substitution ciphers
        if cipher_type in ["substitution", "caesar", "vigenere"] or "substitution" in results["analysis_results"]:
            char_freq = {}
            for char in cipher_text.upper():
                if char.isalpha():
                    char_freq[char] = char_freq.get(char, 0) + 1

            if char_freq:
                most_common = max(char_freq, key=char_freq.get)
                results["analysis_results"].append(f"Most frequent character: {most_common} ({char_freq[most_common]} occurrences)")
                results["next_steps"].append("Try substituting most frequent character with 'E'")

        # ROT/Caesar cipher detection
        if cipher_type == "caesar" or len(set(cipher_text.upper().replace(' ', ''))) <= 26:
            results["recommended_tools"].append("rot13")
            results["next_steps"].append("Try all ROT values (1-25)")

        # RSA-specific analysis
        if cipher_type == "rsa" or "rsa" in additional_info.lower():
            results["recommended_tools"].extend(["rsatool", "factordb", "yafu"])
            results["next_steps"].extend([
                "Check if modulus can be factored",
                "Look for small public exponent attacks",
                "Check for common modulus attacks"
            ])

        # Vigenère cipher analysis
        if cipher_type == "vigenere" or "vigenere" in additional_info.lower():
            results["recommended_tools"].append("vigenere-solver")
            results["next_steps"].extend([
                "Perform Kasiski examination for key length",
                "Use index of coincidence analysis",
                "Try common key words"
            ])

        logger.info(f"🔐 CTF crypto analysis completed | Type: {cipher_type} | Tools: {len(results['recommended_tools'])}")
        return jsonify({
            "success": True,
            "analysis": results,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error in CTF crypto solver: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/ctf/forensics-analyzer", methods=["POST"])
def ctf_forensics_analyzer():
    # Advanced forensics challenge analyzer with multiple investigation techniques
    try:
        params = request.json
        file_path = params.get("file_path", "")
        analysis_type = params.get("analysis_type", "comprehensive")
        extract_hidden = params.get("extract_hidden", True)
        check_steganography = params.get("check_steganography", True)

        if not file_path:
            return jsonify({"error": "File path is required"}), 400

        results = {
            "file_path": file_path,
            "analysis_type": analysis_type,
            "file_info": {},
            "metadata": {},
            "hidden_data": [],
            "steganography_results": [],
            "recommended_tools": [],
            "next_steps": []
        }

        # Basic file analysis
        try:
            # File command
            file_result = subprocess.run(['file', file_path], capture_output=True, text=True, timeout=30)
            if file_result.returncode == 0:
                results["file_info"]["type"] = file_result.stdout.strip()

                # Determine file category and suggest tools
                file_type = file_result.stdout.lower()
                if "image" in file_type:
                    results["recommended_tools"].extend(["exiftool", "steghide", "stegsolve", "zsteg"])
                    results["next_steps"].extend([
                        "Extract EXIF metadata",
                        "Check for steganographic content",
                        "Analyze color channels separately"
                    ])
                elif "audio" in file_type:
                    results["recommended_tools"].extend(["audacity", "sonic-visualizer", "spectrum-analyzer"])
                    results["next_steps"].extend([
                        "Analyze audio spectrum",
                        "Check for hidden data in audio channels",
                        "Look for DTMF tones or morse code"
                    ])
                elif "pdf" in file_type:
                    results["recommended_tools"].extend(["pdfinfo", "pdftotext", "binwalk"])
                    results["next_steps"].extend([
                        "Extract text and metadata",
                        "Check for embedded files",
                        "Analyze PDF structure"
                    ])
                elif "zip" in file_type or "archive" in file_type:
                    results["recommended_tools"].extend(["unzip", "7zip", "binwalk"])
                    results["next_steps"].extend([
                        "Extract archive contents",
                        "Check for password protection",
                        "Look for hidden files"
                    ])
        except Exception as e:
            results["file_info"]["error"] = str(e)

        # Metadata extraction
        try:
            exif_result = subprocess.run(['exiftool', file_path], capture_output=True, text=True, timeout=30)
            if exif_result.returncode == 0:
                results["metadata"]["exif"] = exif_result.stdout
        except Exception as e:
            results["metadata"]["exif_error"] = str(e)

        # Binwalk analysis for hidden files
        if extract_hidden:
            try:
                binwalk_result = subprocess.run(['binwalk', '-e', file_path], capture_output=True, text=True, timeout=60)
                if binwalk_result.returncode == 0:
                    results["hidden_data"].append({
                        "tool": "binwalk",
                        "output": binwalk_result.stdout
                    })
            except Exception as e:
                results["hidden_data"].append({
                    "tool": "binwalk",
                    "error": str(e)
                })

        # Steganography checks
        if check_steganography:
            # Check for common steganography tools
            steg_tools = ["steghide", "zsteg", "outguess"]
            for tool in steg_tools:
                try:
                    if tool == "steghide":
                        steg_result = subprocess.run([tool, 'info', file_path], capture_output=True, text=True, timeout=30)
                    elif tool == "zsteg":
                        steg_result = subprocess.run([tool, '-a', file_path], capture_output=True, text=True, timeout=30)
                    elif tool == "outguess":
                        steg_result = subprocess.run([tool, '-r', file_path, '/tmp/outguess_output'], capture_output=True, text=True, timeout=30)

                    if steg_result.returncode == 0 and steg_result.stdout.strip():
                        results["steganography_results"].append({
                            "tool": tool,
                            "output": steg_result.stdout
                        })
                except Exception as e:
                    results["steganography_results"].append({
                        "tool": tool,
                        "error": str(e)
                    })

        # Strings analysis
        try:
            strings_result = subprocess.run(['strings', file_path], capture_output=True, text=True, timeout=30)
            if strings_result.returncode == 0:
                # Look for interesting strings (flags, URLs, etc.)
                interesting_strings = []
                for line in strings_result.stdout.split('\n'):
                    if any(keyword in line.lower() for keyword in ['flag', 'password', 'key', 'secret', 'http', 'ftp']):
                        interesting_strings.append(line.strip())

                if interesting_strings:
                    results["hidden_data"].append({
                        "tool": "strings",
                        "interesting_strings": interesting_strings[:20]  # Limit to first 20
                    })
        except Exception as e:
            results["hidden_data"].append({
                "tool": "strings",
                "error": str(e)
            })

        logger.info(f"[?] CTF forensics analysis completed | File: {file_path} | Tools used: {len(results['recommended_tools'])}")
        return jsonify({
            "success": True,
            "analysis": results,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error in CTF forensics analyzer: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/ctf/binary-analyzer", methods=["POST"])
def ctf_binary_analyzer():
    # Advanced binary analysis for reverse engineering and pwn challenges
    try:
        params = request.json
        binary_path = params.get("binary_path", "")
        analysis_depth = params.get("analysis_depth", "comprehensive")  # basic, comprehensive, deep
        check_protections = params.get("check_protections", True)
        find_gadgets = params.get("find_gadgets", True)

        if not binary_path:
            return jsonify({"error": "Binary path is required"}), 400

        results = {
            "binary_path": binary_path,
            "analysis_depth": analysis_depth,
            "file_info": {},
            "security_protections": {},
            "interesting_functions": [],
            "strings_analysis": {},
            "gadgets": [],
            "recommended_tools": [],
            "exploitation_hints": []
        }

        # Basic file information
        try:
            file_result = subprocess.run(['file', binary_path], capture_output=True, text=True, timeout=30)
            if file_result.returncode == 0:
                results["file_info"]["type"] = file_result.stdout.strip()

                # Determine architecture and suggest tools
                file_output = file_result.stdout.lower()
                if "x86-64" in file_output or "x86_64" in file_output:
                    results["file_info"]["architecture"] = "x86_64"
                elif "i386" in file_output or "80386" in file_output:
                    results["file_info"]["architecture"] = "i386"
                elif "arm" in file_output:
                    results["file_info"]["architecture"] = "ARM"

                results["recommended_tools"].extend(["gdb-peda", "radare2", "ghidra"])
        except Exception as e:
            results["file_info"]["error"] = str(e)

        # Security protections check
        if check_protections:
            try:
                checksec_result = subprocess.run(['checksec', '--file', binary_path], capture_output=True, text=True, timeout=30)
                if checksec_result.returncode == 0:
                    results["security_protections"]["checksec"] = checksec_result.stdout

                    # Parse protections and provide exploitation hints
                    output = checksec_result.stdout.lower()
                    if "no canary found" in output:
                        results["exploitation_hints"].append("Stack canary disabled - buffer overflow exploitation possible")
                    if "nx disabled" in output:
                        results["exploitation_hints"].append("NX disabled - shellcode execution on stack possible")
                    if "no pie" in output:
                        results["exploitation_hints"].append("PIE disabled - fixed addresses, ROP/ret2libc easier")
                    if "no relro" in output:
                        results["exploitation_hints"].append("RELRO disabled - GOT overwrite attacks possible")
            except Exception as e:
                results["security_protections"]["error"] = str(e)

        # Strings analysis
        try:
            strings_result = subprocess.run(['strings', binary_path], capture_output=True, text=True, timeout=30)
            if strings_result.returncode == 0:
                strings_output = strings_result.stdout.split('\n')

                # Categorize interesting strings
                interesting_categories = {
                    "functions": [],
                    "format_strings": [],
                    "file_paths": [],
                    "potential_flags": [],
                    "system_calls": []
                }

                for string in strings_output:
                    string = string.strip()
                    if not string:
                        continue

                    # Look for function names
                    if any(func in string for func in ['printf', 'scanf', 'gets', 'strcpy', 'system', 'execve']):
                        interesting_categories["functions"].append(string)

                    # Look for format strings
                    if '%' in string and any(fmt in string for fmt in ['%s', '%d', '%x', '%n']):
                        interesting_categories["format_strings"].append(string)

                    # Look for file paths
                    if string.startswith('/') or '\\' in string:
                        interesting_categories["file_paths"].append(string)

                    # Look for potential flags
                    if any(keyword in string.lower() for keyword in ['flag', 'ctf', 'key', 'password']):
                        interesting_categories["potential_flags"].append(string)

                    # Look for system calls
                    if string in ['sh', 'bash', '/bin/sh', '/bin/bash', 'cmd.exe']:
                        interesting_categories["system_calls"].append(string)

                results["strings_analysis"] = interesting_categories

                # Add exploitation hints based on strings
                if interesting_categories["functions"]:
                    dangerous_funcs = ['gets', 'strcpy', 'sprintf', 'scanf']
                    found_dangerous = [f for f in dangerous_funcs if any(f in s for s in interesting_categories["functions"])]
                    if found_dangerous:
                        results["exploitation_hints"].append(f"Dangerous functions found: {', '.join(found_dangerous)} - potential buffer overflow")

                if interesting_categories["format_strings"]:
                    if any('%n' in s for s in interesting_categories["format_strings"]):
                        results["exploitation_hints"].append("Format string with %n found - potential format string vulnerability")

        except Exception as e:
            results["strings_analysis"]["error"] = str(e)

        # ROP gadgets search
        if find_gadgets and analysis_depth in ["comprehensive", "deep"]:
            try:
                ropgadget_result = subprocess.run(['ROPgadget', '--binary', binary_path, '--only', 'pop|ret'], capture_output=True, text=True, timeout=60)
                if ropgadget_result.returncode == 0:
                    gadget_lines = ropgadget_result.stdout.split('\n')
                    useful_gadgets = []

                    for line in gadget_lines:
                        if 'pop' in line and 'ret' in line:
                            useful_gadgets.append(line.strip())

                    results["gadgets"] = useful_gadgets[:20]  # Limit to first 20 gadgets

                    if useful_gadgets:
                        results["exploitation_hints"].append(f"Found {len(useful_gadgets)} ROP gadgets - ROP chain exploitation possible")
                        results["recommended_tools"].append("ropper")

            except Exception as e:
                results["gadgets"] = [f"Error finding gadgets: {str(e)}"]

        # Function analysis with objdump
        if analysis_depth in ["comprehensive", "deep"]:
            try:
                objdump_result = subprocess.run(['objdump', '-t', binary_path], capture_output=True, text=True, timeout=30)
                if objdump_result.returncode == 0:
                    functions = []
                    for line in objdump_result.stdout.split('\n'):
                        if 'F .text' in line:  # Function in text section
                            parts = line.split()
                            if len(parts) >= 6:
                                func_name = parts[-1]
                                functions.append(func_name)

                    results["interesting_functions"] = functions[:50]  # Limit to first 50 functions
            except Exception as e:
                results["interesting_functions"] = [f"Error analyzing functions: {str(e)}"]

        # Add tool recommendations based on findings
        if results["exploitation_hints"]:
            results["recommended_tools"].extend(["pwntools", "gdb-peda", "one-gadget"])

        if "format string" in str(results["exploitation_hints"]).lower():
            results["recommended_tools"].append("format-string-exploiter")

        logger.info(f"🔬 CTF binary analysis completed | Binary: {binary_path} | Hints: {len(results['exploitation_hints'])}")
        return jsonify({
            "success": True,
            "analysis": results,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error in CTF binary analyzer: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

# ============================================================================
# ADVANCED PROCESS MANAGEMENT API ENDPOINTS (v10.0 ENHANCEMENT)
# ============================================================================

@app.route("/api/process/execute-async", methods=["POST"])
def execute_command_async():
    # Execute command asynchronously using enhanced process management
    try:
        params = request.json
        command = params.get("command", "")
        context = params.get("context", {})

        if not command:
            return jsonify({"error": "Command parameter is required"}), 400

        # Execute command asynchronously
        task_id = enhanced_process_manager.execute_command_async(command, context)

        logger.info(f"[>] Async command execution started | Task ID: {task_id}")
        return jsonify({
            "success": True,
            "task_id": task_id,
            "command": command,
            "status": "submitted",
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error in async command execution: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/process/get-task-result/<task_id>", methods=["GET"])
def get_async_task_result(task_id):
    # Get result of asynchronous task
    try:
        result = enhanced_process_manager.get_task_result(task_id)

        if result["status"] == "not_found":
            return jsonify({"error": "Task not found"}), 404

        logger.info(f"[=] Task result retrieved | Task ID: {task_id} | Status: {result['status']}")
        return jsonify({
            "success": True,
            "task_id": task_id,
            "result": result,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error getting task result: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/process/pool-stats", methods=["GET"])
def get_process_pool_stats():
    # Get process pool statistics and performance metrics
    try:
        stats = enhanced_process_manager.get_comprehensive_stats()

        logger.info(f"[#] Process pool stats retrieved | Active workers: {stats['process_pool']['active_workers']}")
        return jsonify({
            "success": True,
            "stats": stats,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error getting pool stats: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/process/cache-stats", methods=["GET"])
def get_cache_stats():
    # Get advanced cache statistics
    try:
        cache_stats = enhanced_process_manager.cache.get_stats()

        logger.info(f"[D] Cache stats retrieved | Hit rate: {cache_stats['hit_rate']:.1f}%")
        return jsonify({
            "success": True,
            "cache_stats": cache_stats,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error getting cache stats: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/process/clear-cache", methods=["POST"])
def clear_process_cache():
    # Clear the advanced cache
    try:
        enhanced_process_manager.cache.clear()

        logger.info("[~] Process cache cleared")
        return jsonify({
            "success": True,
            "message": "Cache cleared successfully",
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error clearing cache: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/process/resource-usage", methods=["GET"])
def get_resource_usage():
    # Get current system resource usage and trends
    try:
        current_usage = enhanced_process_manager.resource_monitor.get_current_usage()
        usage_trends = enhanced_process_manager.resource_monitor.get_usage_trends()

        logger.info(f"[^] Resource usage retrieved | CPU: {current_usage['cpu_percent']:.1f}% | Memory: {current_usage['memory_percent']:.1f}%")
        return jsonify({
            "success": True,
            "current_usage": current_usage,
            "usage_trends": usage_trends,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error getting resource usage: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/process/performance-dashboard", methods=["GET"])
def get_performance_dashboard():
    # Get performance dashboard data
    try:
        dashboard_data = enhanced_process_manager.performance_dashboard.get_summary()
        pool_stats = enhanced_process_manager.process_pool.get_pool_stats()
        resource_usage = enhanced_process_manager.resource_monitor.get_current_usage()

        # Create comprehensive dashboard
        dashboard = {
            "performance_summary": dashboard_data,
            "process_pool": pool_stats,
            "resource_usage": resource_usage,
            "cache_stats": enhanced_process_manager.cache.get_stats(),
            "auto_scaling_status": enhanced_process_manager.auto_scaling_enabled,
            "system_health": {
                "cpu_status": "healthy" if resource_usage["cpu_percent"] < 80 else "warning" if resource_usage["cpu_percent"] < 95 else "critical",
                "memory_status": "healthy" if resource_usage["memory_percent"] < 85 else "warning" if resource_usage["memory_percent"] < 95 else "critical",
                "disk_status": "healthy" if resource_usage["disk_percent"] < 90 else "warning" if resource_usage["disk_percent"] < 98 else "critical"
            }
        }

        logger.info(f"[#] Performance dashboard retrieved | Success rate: {dashboard_data.get('success_rate', 0):.1f}%")
        return jsonify({
            "success": True,
            "dashboard": dashboard,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error getting performance dashboard: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/process/terminate-gracefully/<int:pid>", methods=["POST"])
def terminate_process_gracefully(pid):
    # Terminate process with graceful degradation
    try:
        params = request.json or {}
        timeout = params.get("timeout", 30)

        success = enhanced_process_manager.terminate_process_gracefully(pid, timeout)

        if success:
            logger.info(f"[OK] Process {pid} terminated gracefully")
            return jsonify({
                "success": True,
                "message": f"Process {pid} terminated successfully",
                "pid": pid,
                "timestamp": datetime.now().isoformat()
            })
        else:
            return jsonify({
                "success": False,
                "error": f"Failed to terminate process {pid}",
                "pid": pid,
                "timestamp": datetime.now().isoformat()
            }), 400

    except Exception as e:
        logger.error(f"[!!] Error terminating process {pid}: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/process/auto-scaling", methods=["POST"])
def configure_auto_scaling():
    # Configure auto-scaling settings
    try:
        params = request.json
        enabled = params.get("enabled", True)
        thresholds = params.get("thresholds", {})

        # Update auto-scaling configuration
        enhanced_process_manager.auto_scaling_enabled = enabled

        if thresholds:
            enhanced_process_manager.resource_thresholds.update(thresholds)

        logger.info(f"⚙️ Auto-scaling configured | Enabled: {enabled}")
        return jsonify({
            "success": True,
            "auto_scaling_enabled": enabled,
            "resource_thresholds": enhanced_process_manager.resource_thresholds,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error configuring auto-scaling: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/process/scale-pool", methods=["POST"])
def manual_scale_pool():
    # Manually scale the process pool
    try:
        params = request.json
        action = params.get("action", "")  # "up" or "down"
        count = params.get("count", 1)

        if action not in ["up", "down"]:
            return jsonify({"error": "Action must be 'up' or 'down'"}), 400

        current_stats = enhanced_process_manager.process_pool.get_pool_stats()
        current_workers = current_stats["active_workers"]

        if action == "up":
            max_workers = enhanced_process_manager.process_pool.max_workers
            if current_workers + count <= max_workers:
                enhanced_process_manager.process_pool._scale_up(count)
                new_workers = current_workers + count
                message = f"Scaled up by {count} workers"
            else:
                return jsonify({"error": f"Cannot scale up: would exceed max workers ({max_workers})"}), 400
        else:  # down
            min_workers = enhanced_process_manager.process_pool.min_workers
            if current_workers - count >= min_workers:
                enhanced_process_manager.process_pool._scale_down(count)
                new_workers = current_workers - count
                message = f"Scaled down by {count} workers"
            else:
                return jsonify({"error": f"Cannot scale down: would go below min workers ({min_workers})"}), 400

        logger.info(f"📏 Manual scaling | {message} | Workers: {current_workers} → {new_workers}")
        return jsonify({
            "success": True,
            "message": message,
            "previous_workers": current_workers,
            "current_workers": new_workers,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error scaling pool: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/process/health-check", methods=["GET"])
def process_health_check():
    # Comprehensive health check of the process management system
    try:
        # Get all system stats
        comprehensive_stats = enhanced_process_manager.get_comprehensive_stats()

        # Determine overall health
        resource_usage = comprehensive_stats["resource_usage"]
        pool_stats = comprehensive_stats["process_pool"]
        cache_stats = comprehensive_stats["cache"]

        health_score = 100
        issues = []

        # CPU health
        if resource_usage["cpu_percent"] > 95:
            health_score -= 30
            issues.append("Critical CPU usage")
        elif resource_usage["cpu_percent"] > 80:
            health_score -= 15
            issues.append("High CPU usage")

        # Memory health
        if resource_usage["memory_percent"] > 95:
            health_score -= 25
            issues.append("Critical memory usage")
        elif resource_usage["memory_percent"] > 85:
            health_score -= 10
            issues.append("High memory usage")

        # Disk health
        if resource_usage["disk_percent"] > 98:
            health_score -= 20
            issues.append("Critical disk usage")
        elif resource_usage["disk_percent"] > 90:
            health_score -= 5
            issues.append("High disk usage")

        # Process pool health
        if pool_stats["queue_size"] > 50:
            health_score -= 15
            issues.append("High task queue backlog")

        # Cache health
        if cache_stats["hit_rate"] < 30:
            health_score -= 10
            issues.append("Low cache hit rate")

        health_score = max(0, health_score)

        # Determine status
        if health_score >= 90:
            status = "excellent"
        elif health_score >= 75:
            status = "good"
        elif health_score >= 50:
            status = "fair"
        elif health_score >= 25:
            status = "poor"
        else:
            status = "critical"

        health_report = {
            "overall_status": status,
            "health_score": health_score,
            "issues": issues,
            "system_stats": comprehensive_stats,
            "recommendations": []
        }

        # Add recommendations based on issues
        if "High CPU usage" in issues:
            health_report["recommendations"].append("Consider reducing concurrent processes or upgrading CPU")
        if "High memory usage" in issues:
            health_report["recommendations"].append("Clear caches or increase available memory")
        if "High task queue backlog" in issues:
            health_report["recommendations"].append("Scale up process pool or optimize task processing")
        if "Low cache hit rate" in issues:
            health_report["recommendations"].append("Review cache TTL settings or increase cache size")

        logger.info(f"🏥 Health check completed | Status: {status} | Score: {health_score}/100")
        return jsonify({
            "success": True,
            "health_report": health_report,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"[!!] Error in health check: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

# ============================================================================
# BANNER AND STARTUP CONFIGURATION
# ============================================================================

# ============================================================================
# INTELLIGENT ERROR HANDLING API ENDPOINTS
# ============================================================================

@app.route("/api/error-handling/statistics", methods=["GET"])
def get_error_statistics():
    # Get error handling statistics
    try:
        stats = error_handler.get_error_statistics()
        return jsonify({
            "success": True,
            "statistics": stats,
            "timestamp": datetime.now().isoformat()
        })
    except Exception as e:
        logger.error(f"Error getting error statistics: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/error-handling/test-recovery", methods=["POST"])
def test_error_recovery():
    # Test error recovery system with simulated failures
    try:
        data = request.get_json()
        tool_name = data.get("tool_name", "nmap")
        error_type = data.get("error_type", "timeout")
        target = data.get("target", "example.com")

        # Simulate an error for testing
        if error_type == "timeout":
            exception = TimeoutError("Simulated timeout error")
        elif error_type == "permission_denied":
            exception = PermissionError("Simulated permission error")
        elif error_type == "network_unreachable":
            exception = ConnectionError("Simulated network error")
        else:
            exception = Exception(f"Simulated {error_type} error")

        context = {
            "target": target,
            "parameters": data.get("parameters", {}),
            "attempt_count": 1
        }

        # Get recovery strategy
        recovery_strategy = error_handler.handle_tool_failure(tool_name, exception, context)

        return jsonify({
            "success": True,
            "recovery_strategy": {
                "action": recovery_strategy.action.value,
                "parameters": recovery_strategy.parameters,
                "max_attempts": recovery_strategy.max_attempts,
                "success_probability": recovery_strategy.success_probability,
                "estimated_time": recovery_strategy.estimated_time
            },
            "error_classification": error_handler.classify_error(str(exception), exception).value,
            "alternative_tools": error_handler.tool_alternatives.get(tool_name, []),
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"Error testing recovery system: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/error-handling/fallback-chains", methods=["GET"])
def get_fallback_chains():
    # Get available fallback tool chains
    try:
        operation = request.args.get("operation", "")
        failed_tools = request.args.getlist("failed_tools")

        if operation:
            fallback_chain = degradation_manager.create_fallback_chain(operation, failed_tools)
            return jsonify({
                "success": True,
                "operation": operation,
                "fallback_chain": fallback_chain,
                "is_critical": degradation_manager.is_critical_operation(operation),
                "timestamp": datetime.now().isoformat()
            })
        else:
            return jsonify({
                "success": True,
                "available_operations": list(degradation_manager.fallback_chains.keys()),
                "critical_operations": list(degradation_manager.critical_operations),
                "timestamp": datetime.now().isoformat()
            })

    except Exception as e:
        logger.error(f"Error getting fallback chains: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/error-handling/execute-with-recovery", methods=["POST"])
def execute_with_recovery_endpoint():
    # Execute a command with intelligent error handling and recovery
    try:
        data = request.get_json()
        tool_name = data.get("tool_name", "")
        command = data.get("command", "")
        parameters = data.get("parameters", {})
        max_attempts = data.get("max_attempts", 3)
        use_cache = data.get("use_cache", True)

        if not tool_name or not command:
            return jsonify({"error": "tool_name and command are required"}), 400

        # Execute command with recovery
        result = execute_command_with_recovery(
            tool_name=tool_name,
            command=command,
            parameters=parameters,
            use_cache=use_cache,
            max_attempts=max_attempts
        )

        return jsonify({
            "success": result.get("success", False),
            "result": result,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"Error executing command with recovery: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/error-handling/classify-error", methods=["POST"])
def classify_error_endpoint():
    # Classify an error message
    try:
        data = request.get_json()
        error_message = data.get("error_message", "")

        if not error_message:
            return jsonify({"error": "error_message is required"}), 400

        error_type = error_handler.classify_error(error_message)
        recovery_strategies = error_handler.recovery_strategies.get(error_type, [])

        return jsonify({
            "success": True,
            "error_type": error_type.value,
            "recovery_strategies": [
                {
                    "action": strategy.action.value,
                    "parameters": strategy.parameters,
                    "success_probability": strategy.success_probability,
                    "estimated_time": strategy.estimated_time
                }
                for strategy in recovery_strategies
            ],
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"Error classifying error: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/error-handling/parameter-adjustments", methods=["POST"])
def get_parameter_adjustments():
    # Get parameter adjustments for a tool and error type
    try:
        data = request.get_json()
        tool_name = data.get("tool_name", "")
        error_type_str = data.get("error_type", "")
        original_params = data.get("original_params", {})

        if not tool_name or not error_type_str:
            return jsonify({"error": "tool_name and error_type are required"}), 400

        # Convert string to ErrorType enum
        try:
            error_type = ErrorType(error_type_str)
        except ValueError:
            return jsonify({"error": f"Invalid error_type: {error_type_str}"}), 400

        adjusted_params = error_handler.auto_adjust_parameters(tool_name, error_type, original_params)

        return jsonify({
            "success": True,
            "tool_name": tool_name,
            "error_type": error_type.value,
            "original_params": original_params,
            "adjusted_params": adjusted_params,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"Error getting parameter adjustments: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

@app.route("/api/error-handling/alternative-tools", methods=["GET"])
def get_alternative_tools():
    # Get alternative tools for a given tool
    try:
        tool_name = request.args.get("tool_name", "")

        if not tool_name:
            return jsonify({"error": "tool_name parameter is required"}), 400

        alternatives = error_handler.tool_alternatives.get(tool_name, [])

        return jsonify({
            "success": True,
            "tool_name": tool_name,
            "alternatives": alternatives,
            "has_alternatives": len(alternatives) > 0,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"Error getting alternative tools: {str(e)}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

# Create the banner after all classes are defined
BANNER = ModernVisualEngine.create_banner()

if __name__ == "__main__":
    # Display the beautiful new banner
    print(BANNER)

    parser = argparse.ArgumentParser(description="Run the VectorAI AI API Server")
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")
    parser.add_argument("--port", type=int, default=API_PORT, help=f"Port for the API server (default: {API_PORT})")
    args = parser.parse_args()

    if args.debug:
        DEBUG_MODE = True
        logger.setLevel(logging.DEBUG)

    if args.port != API_PORT:
        API_PORT = args.port

    # Enhanced startup messages with beautiful formatting
    startup_info = (
        f"\n{ModernVisualEngine.COLORS['MATRIX_GREEN']}{ModernVisualEngine.COLORS['BOLD']}+-----------------------------------------------------------------------------+{ModernVisualEngine.COLORS['RESET']}\n"
        f"{ModernVisualEngine.COLORS['BOLD']}|{ModernVisualEngine.COLORS['RESET']} {ModernVisualEngine.COLORS['NEON_BLUE']}[>] Starting VectorAI AI Tools API Server{ModernVisualEngine.COLORS['RESET']}\n"
        f"{ModernVisualEngine.COLORS['BOLD']}+-----------------------------------------------------------------------------+{ModernVisualEngine.COLORS['RESET']}\n"
        f"{ModernVisualEngine.COLORS['BOLD']}|{ModernVisualEngine.COLORS['RESET']} {ModernVisualEngine.COLORS['CYBER_ORANGE']}[W] Port:{ModernVisualEngine.COLORS['RESET']} {API_PORT}\n"
        f"{ModernVisualEngine.COLORS['BOLD']}|{ModernVisualEngine.COLORS['RESET']} {ModernVisualEngine.COLORS['WARNING']}[+] Debug Mode:{ModernVisualEngine.COLORS['RESET']} {DEBUG_MODE}\n"
        f"{ModernVisualEngine.COLORS['BOLD']}|{ModernVisualEngine.COLORS['RESET']} {ModernVisualEngine.COLORS['ELECTRIC_PURPLE']}[D] Cache Size:{ModernVisualEngine.COLORS['RESET']} {CACHE_SIZE} | TTL: {CACHE_TTL}s\n"
        f"{ModernVisualEngine.COLORS['BOLD']}|{ModernVisualEngine.COLORS['RESET']} {ModernVisualEngine.COLORS['TERMINAL_GRAY']}[T]  Command Timeout:{ModernVisualEngine.COLORS['RESET']} {COMMAND_TIMEOUT}s\n"
        f"{ModernVisualEngine.COLORS['BOLD']}|{ModernVisualEngine.COLORS['RESET']} {ModernVisualEngine.COLORS['MATRIX_GREEN']}[*] Enhanced Visual Engine:{ModernVisualEngine.COLORS['RESET']} Active\n"
        f"{ModernVisualEngine.COLORS['MATRIX_GREEN']}{ModernVisualEngine.COLORS['BOLD']}+-----------------------------------------------------------------------------+{ModernVisualEngine.COLORS['RESET']}\n"
    )

    for line in startup_info.strip().split('\n'):
        if line.strip():
            logger.info(line)

    app.run(host="0.0.0.0", port=API_PORT, debug=DEBUG_MODE)
